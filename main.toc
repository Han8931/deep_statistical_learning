\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Probability}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Transformations of Random Variable}{1}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Formal Definition}{1}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Intuition}{2}{subsection.1.2.2}%
\contentsline {section}{\numberline {1.3}Gaussian Distribution}{2}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Conditional Gaussian Distribution}{2}{subsection.1.3.1}%
\contentsline {section}{\numberline {1.4}Naive Bayes}{3}{section.1.4}%
\contentsline {section}{\numberline {1.5}Logistic Regression}{5}{section.1.5}%
\contentsline {chapter}{\numberline {2}Bayesian Regression}{7}{chapter.2}%
\contentsline {section}{\numberline {2.1}Curve Fitting}{7}{section.2.1}%
\contentsline {section}{\numberline {2.2}Bayesian Curve Fitting}{8}{section.2.2}%
\contentsline {chapter}{\numberline {3}Training, Testing, and Regularization}{9}{chapter.3}%
\contentsline {section}{\numberline {3.1}Sources of Error in ML}{9}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Alternative Derivation}{10}{subsection.3.1.1}%
\contentsline {part}{I\hspace {1em}Kernel Methods}{12}{part.1}%
\contentsline {chapter}{\numberline {4}Introduction to Kernel Methods}{13}{chapter.4}%
\contentsline {chapter}{\numberline {5}Gaussian Process}{15}{chapter.5}%
\contentsline {section}{\numberline {5.1}Introduction}{15}{section.5.1}%
\contentsline {chapter}{\numberline {6}Support Vector Machine}{16}{chapter.6}%
\contentsline {section}{\numberline {6.1}Introduction}{16}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Orthogonal Projection}{16}{subsection.6.1.1}%
\contentsline {section}{\numberline {6.2}Decision Boundary with Margin}{16}{section.6.2}%
\contentsline {section}{\numberline {6.3}Error Handling in SVM}{18}{section.6.3}%
\contentsline {section}{\numberline {6.4}Kernel Trick}{18}{section.6.4}%
\contentsline {section}{\numberline {6.5}SVM Optimization: Lagrange Multipliers}{19}{section.6.5}%
\contentsline {section}{\numberline {6.6}The Wolfe Dual Problem}{19}{section.6.6}%
\contentsline {section}{\numberline {6.7}Karush-Kuhn-Tucker conditions }{21}{section.6.7}%
\contentsline {part}{II\hspace {1em}Generative Modeling}{22}{part.2}%
\contentsline {chapter}{\numberline {7}Sampling Based Inference}{23}{chapter.7}%
\contentsline {section}{\numberline {7.1}Basic Sampling Methods}{23}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}Inverse Transform Sampling}{23}{subsection.7.1.1}%
\contentsline {subsection}{\numberline {7.1.2}Ancestral Sampling}{23}{subsection.7.1.2}%
\contentsline {subsection}{\numberline {7.1.3}Rejection Sampling}{24}{subsection.7.1.3}%
\contentsline {subsection}{\numberline {7.1.4}Importance Sampling}{25}{subsection.7.1.4}%
\contentsline {section}{\numberline {7.2}Gibbs Sampling}{27}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Markov Chain}{27}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Stationary Distribution}{28}{subsection.7.2.2}%
\contentsline {paragraph}{Limit theorem of Markov chain}{28}{section*.2}%
\contentsline {paragraph}{Reversible MC}{28}{section*.3}%
\contentsline {section}{\numberline {7.3}Markov Chain Monte Carlo}{28}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}Metropolis-Hasting Algorithm}{29}{subsection.7.3.1}%
\contentsline {chapter}{\numberline {8}Topic Modeling}{30}{chapter.8}%
\contentsline {section}{\numberline {8.1}Latent Dirichlet Allocation}{30}{section.8.1}%
\contentsline {subsection}{\numberline {8.1.1}LDA Inference}{31}{subsection.8.1.1}%
\contentsline {subsection}{\numberline {8.1.2}Dirichlet Distribution}{31}{subsection.8.1.2}%
\contentsline {chapter}{\numberline {9}Latent Variable Models}{32}{chapter.9}%
\contentsline {section}{\numberline {9.1}Introduction}{32}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Motivation of Latent Variable Models}{32}{subsection.9.1.1}%
\contentsline {chapter}{\numberline {10}Clustering}{33}{chapter.10}%
\contentsline {section}{\numberline {10.1}K-Means Clustering}{33}{section.10.1}%
\contentsline {section}{\numberline {10.2}Gaussian Mixture Models}{35}{section.10.2}%
\contentsline {subsection}{\numberline {10.2.1}Multinomial Distribution}{35}{subsection.10.2.1}%
\contentsline {subsection}{\numberline {10.2.2}Multivariate Gaussian Distribution}{35}{subsection.10.2.2}%
\contentsline {subsection}{\numberline {10.2.3}Gaussian Mixture Models}{36}{subsection.10.2.3}%
\contentsline {subsection}{\numberline {10.2.4}Maximum Likelihood}{37}{subsection.10.2.4}%
\contentsline {paragraph}{Singularity}{38}{section*.4}%
\contentsline {paragraph}{Identifiability}{38}{section*.5}%
\contentsline {subsection}{\numberline {10.2.5}Expectation Maximization for GMM}{38}{subsection.10.2.5}%
\contentsline {section}{\numberline {10.3}Alternative View of EM}{40}{section.10.3}%
\contentsline {section}{\numberline {10.4}Latent Variable Modeling}{41}{section.10.4}%
\contentsline {subsection}{\numberline {10.4.1}Evidence Lower Bound (ELBO)}{41}{subsection.10.4.1}%
\contentsline {subsection}{\numberline {10.4.2}Expectation Maximization}{42}{subsection.10.4.2}%
\contentsline {subsection}{\numberline {10.4.3}Categorical Latent Variables}{43}{subsection.10.4.3}%
\contentsline {chapter}{\numberline {11}Hidden Markov Models}{44}{chapter.11}%
\contentsline {section}{\numberline {11.1}Introduction}{44}{section.11.1}%
\contentsline {subsection}{\numberline {11.1.1}Conditional Independence}{44}{subsection.11.1.1}%
\contentsline {subsection}{\numberline {11.1.2}Notation}{44}{subsection.11.1.2}%
\contentsline {section}{\numberline {11.2}Bayesian Network}{45}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Bayes Ball}{45}{subsection.11.2.1}%
\contentsline {subsection}{\numberline {11.2.2}Potential Function}{45}{subsection.11.2.2}%
\contentsline {section}{\numberline {11.3}Hidden Markov Models}{47}{section.11.3}%
\contentsline {section}{\numberline {11.4}Evaluation: Forward-Backward Probability}{48}{section.11.4}%
\contentsline {subsection}{\numberline {11.4.1}Joint Probability}{48}{subsection.11.4.1}%
\contentsline {subsection}{\numberline {11.4.2}Marginal Probability}{48}{subsection.11.4.2}%
\contentsline {subsection}{\numberline {11.4.3}Forward Algorithm}{49}{subsection.11.4.3}%
\contentsline {subsection}{\numberline {11.4.4}Backward Probability}{50}{subsection.11.4.4}%
\contentsline {section}{\numberline {11.5}Decoding: Viterbi Algorithm}{51}{section.11.5}%
\contentsline {section}{\numberline {11.6}Learning: Baum-Welch Algorithm}{53}{section.11.6}%
\contentsline {subsection}{\numberline {11.6.1}EM Algorithm}{53}{subsection.11.6.1}%
\contentsline {section}{\numberline {11.7}Summary}{55}{section.11.7}%
\contentsline {chapter}{\numberline {12}Explicit Generative Models}{56}{chapter.12}%
\contentsline {section}{\numberline {12.1}Variational Autoencoder}{56}{section.12.1}%
\contentsline {subsection}{\numberline {12.1.1}VAE Optimization}{58}{subsection.12.1.1}%
\contentsline {subsection}{\numberline {12.1.2}Conditional VAE}{58}{subsection.12.1.2}%
\contentsline {subsection}{\numberline {12.1.3}Variational Deep Embedding (VaDE)}{59}{subsection.12.1.3}%
\contentsline {subsection}{\numberline {12.1.4}Importance Weighted VAE}{59}{subsection.12.1.4}%
\contentsline {chapter}{\numberline {13}Implicit Generative Models}{60}{chapter.13}%
\contentsline {section}{\numberline {13.1}Generative Adversarial Networks}{60}{section.13.1}%
\contentsline {subsection}{\numberline {13.1.1}Discriminator}{60}{subsection.13.1.1}%
\contentsline {subsection}{\numberline {13.1.2}Generator}{62}{subsection.13.1.2}%
\contentsline {section}{\numberline {13.2}Some notes}{62}{section.13.2}%
\contentsline {section}{\numberline {13.3}Wasserstein Generative Adversarial Networks}{64}{section.13.3}%
\contentsline {subsection}{\numberline {13.3.1}KL Divergence}{64}{subsection.13.3.1}%
\contentsline {subsection}{\numberline {13.3.2}Jensen-Shannon Divergence}{64}{subsection.13.3.2}%
\contentsline {subsection}{\numberline {13.3.3}Wasserstein Distance}{65}{subsection.13.3.3}%
\contentsline {section}{\numberline {13.4}WGAN}{66}{section.13.4}%
\contentsline {subsection}{\numberline {13.4.1}Lipschitz continuity}{66}{subsection.13.4.1}%
\contentsline {section}{\numberline {13.5}InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}{68}{section.13.5}%
\contentsline {subsection}{\numberline {13.5.1}Joint Entropy}{68}{subsection.13.5.1}%
\contentsline {subsection}{\numberline {13.5.2}Conditional Entropy}{68}{subsection.13.5.2}%
\contentsline {subsection}{\numberline {13.5.3}Variational Mutual Information Maximization}{68}{subsection.13.5.3}%
\contentsline {chapter}{\numberline {14}Diffusion Model}{70}{chapter.14}%
\contentsline {section}{\numberline {14.1}Introduction}{70}{section.14.1}%
\contentsline {section}{\numberline {14.2}Forward Diffusion}{71}{section.14.2}%
\contentsline {section}{\numberline {14.3}Backward Process}{72}{section.14.3}%
\contentsline {section}{\numberline {14.4}Distribution Modeling}{74}{section.14.4}%
\contentsline {section}{\numberline {14.5}Summary}{80}{section.14.5}%
\contentsline {section}{\numberline {14.6}Score Matching}{82}{section.14.6}%
\contentsline {subsection}{\numberline {14.6.1}Fisher Divergence}{83}{subsection.14.6.1}%
\contentsline {subsection}{\numberline {14.6.2}Langevin Dynamics}{84}{subsection.14.6.2}%
\contentsline {part}{III\hspace {1em}Natural Language Processing}{86}{part.3}%
\contentsline {chapter}{\numberline {15}Introduction}{87}{chapter.15}%
\contentsline {section}{\numberline {15.1}Evaluation Metrics}{87}{section.15.1}%
\contentsline {subsection}{\numberline {15.1.1}Perplexity}{87}{subsection.15.1.1}%
\contentsline {subsection}{\numberline {15.1.2}Cross-Entropy and Perplexity}{88}{subsection.15.1.2}%
\contentsline {chapter}{\numberline {16}Transformer}{89}{chapter.16}%
\contentsline {section}{\numberline {16.1}Attention Mechanism}{89}{section.16.1}%
\contentsline {section}{\numberline {16.2}Transformer}{90}{section.16.2}%
