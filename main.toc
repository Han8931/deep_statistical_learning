\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Probability}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Naive Bayes}{1}{section.1.2}%
\contentsline {section}{\numberline {1.3}Logistic Regression}{3}{section.1.3}%
\contentsline {chapter}{\numberline {2}Training, Testing, and Regularization}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}Sources of Error in ML}{5}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Alternative Derivation}{6}{subsection.2.1.1}%
\contentsline {chapter}{\numberline {3}Support Vector Machine}{8}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduction}{8}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Orthogonal Projection}{8}{subsection.3.1.1}%
\contentsline {section}{\numberline {3.2}Decision Boundary with Margin}{8}{section.3.2}%
\contentsline {section}{\numberline {3.3}Error Handling in SVM}{10}{section.3.3}%
\contentsline {section}{\numberline {3.4}Kernel Trick}{10}{section.3.4}%
\contentsline {section}{\numberline {3.5}SVM Optimization: Lagrange Multipliers}{11}{section.3.5}%
\contentsline {section}{\numberline {3.6}The Wolfe Dual Problem}{11}{section.3.6}%
\contentsline {section}{\numberline {3.7}Karush-Kuhn-Tucker conditions }{13}{section.3.7}%
\contentsline {chapter}{\numberline {4}Sampling Based Inference}{14}{chapter.4}%
\contentsline {section}{\numberline {4.1}Basic Sampling Methods}{14}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Inverse Transform Sampling}{14}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Ancestral Sampling}{14}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Rejection Sampling}{15}{subsection.4.1.3}%
\contentsline {subsection}{\numberline {4.1.4}Importance Sampling}{16}{subsection.4.1.4}%
\contentsline {section}{\numberline {4.2}Gibbs Sampling}{18}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Markov Chain}{18}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Stationary Distribution}{19}{subsection.4.2.2}%
\contentsline {paragraph}{Limit theorem of Markov chain}{19}{section*.2}%
\contentsline {paragraph}{Reversible MC}{19}{section*.3}%
\contentsline {section}{\numberline {4.3}Markov Chain Monte Carlo}{19}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Metropolis-Hasting Algorithm}{20}{subsection.4.3.1}%
\contentsline {chapter}{\numberline {5}Topic Modeling}{21}{chapter.5}%
\contentsline {section}{\numberline {5.1}Latent Dirichlet Allocation}{21}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}LDA Inference}{22}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Dirichlet Distribution}{22}{subsection.5.1.2}%
\contentsline {chapter}{\numberline {6}Latent Variable Models}{23}{chapter.6}%
\contentsline {section}{\numberline {6.1}Introduction}{23}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Motivation of Latent Variable Models}{23}{subsection.6.1.1}%
\contentsline {chapter}{\numberline {7}Clustering}{24}{chapter.7}%
\contentsline {section}{\numberline {7.1}K-Means Clustering}{24}{section.7.1}%
\contentsline {section}{\numberline {7.2}Gaussian Mixture Models}{26}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Multinomial Distribution}{26}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Multivariate Gaussian Distribution}{26}{subsection.7.2.2}%
\contentsline {subsection}{\numberline {7.2.3}Gaussian Mixture Models}{27}{subsection.7.2.3}%
\contentsline {subsection}{\numberline {7.2.4}Maximum Likelihood}{28}{subsection.7.2.4}%
\contentsline {paragraph}{Singularity}{29}{section*.4}%
\contentsline {paragraph}{Identifiability}{29}{section*.5}%
\contentsline {subsection}{\numberline {7.2.5}Expectation Maximization for GMM}{29}{subsection.7.2.5}%
\contentsline {section}{\numberline {7.3}Alternative View of EM}{31}{section.7.3}%
\contentsline {section}{\numberline {7.4}Latent Variable Modeling}{32}{section.7.4}%
\contentsline {subsection}{\numberline {7.4.1}Evidence Lower Bound (ELBO)}{32}{subsection.7.4.1}%
\contentsline {subsection}{\numberline {7.4.2}Expectation Maximization}{33}{subsection.7.4.2}%
\contentsline {subsection}{\numberline {7.4.3}Categorical Latent Variables}{34}{subsection.7.4.3}%
\contentsline {chapter}{\numberline {11}Hidden Markov Models}{61}{chapter.11}%
\contentsline {section}{\numberline {11.1}Introduction}{61}{section.11.1}%
\contentsline {subsection}{\numberline {11.1.1}Conditional Independence}{61}{subsection.11.1.1}%
\contentsline {subsection}{\numberline {11.1.2}Notation}{61}{subsection.11.1.2}%
\contentsline {section}{\numberline {11.2}Bayesian Network}{62}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Bayes Ball}{62}{subsection.11.2.1}%
\contentsline {subsection}{\numberline {11.2.2}Potential Function}{62}{subsection.11.2.2}%
\contentsline {section}{\numberline {11.3}Hidden Markov Models}{64}{section.11.3}%
\contentsline {section}{\numberline {11.4}Evaluation: Forward-Backward Probability}{65}{section.11.4}%
\contentsline {subsection}{\numberline {11.4.1}Joint Probability}{65}{subsection.11.4.1}%
\contentsline {subsection}{\numberline {11.4.2}Marginal Probability}{65}{subsection.11.4.2}%
\contentsline {subsection}{\numberline {11.4.3}Forward Algorithm}{66}{subsection.11.4.3}%
\contentsline {subsection}{\numberline {11.4.4}Backward Probability}{67}{subsection.11.4.4}%
\contentsline {section}{\numberline {11.5}Decoding: Viterbi Algorithm}{68}{section.11.5}%
\contentsline {section}{\numberline {11.6}Learning: Baum-Welch Algorithm}{70}{section.11.6}%
\contentsline {subsection}{\numberline {11.6.1}EM Algorithm}{70}{subsection.11.6.1}%
\contentsline {section}{\numberline {11.7}Summary}{72}{section.11.7}%
\contentsline {chapter}{\numberline {9}Explicit Generative Models}{47}{chapter.9}%
\contentsline {section}{\numberline {9.1}Variational Autoencoder}{47}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}VAE Optimization}{49}{subsection.9.1.1}%
\contentsline {subsection}{\numberline {9.1.2}Conditional VAE}{49}{subsection.9.1.2}%
\contentsline {subsection}{\numberline {9.1.3}Variational Deep Embedding (VaDE)}{50}{subsection.9.1.3}%
\contentsline {subsection}{\numberline {9.1.4}Importance Weighted VAE}{50}{subsection.9.1.4}%
\contentsline {chapter}{\numberline {10}Implicit Generative Models}{51}{chapter.10}%
\contentsline {section}{\numberline {10.1}Generative Adversarial Networks}{51}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}Discriminator}{51}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}Generator}{53}{subsection.10.1.2}%
\contentsline {section}{\numberline {10.2}Some notes}{53}{section.10.2}%
\contentsline {section}{\numberline {10.3}Wasserstein Generative Adversarial Networks}{55}{section.10.3}%
\contentsline {subsection}{\numberline {10.3.1}KL Divergence}{55}{subsection.10.3.1}%
\contentsline {subsection}{\numberline {10.3.2}Jensen-Shannon Divergence}{55}{subsection.10.3.2}%
\contentsline {subsection}{\numberline {10.3.3}Wasserstein Distance}{56}{subsection.10.3.3}%
\contentsline {section}{\numberline {10.4}WGAN}{57}{section.10.4}%
\contentsline {subsection}{\numberline {10.4.1}Lipschitz continuity}{57}{subsection.10.4.1}%
\contentsline {section}{\numberline {10.5}InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}{59}{section.10.5}%
\contentsline {subsection}{\numberline {10.5.1}Joint Entropy}{59}{subsection.10.5.1}%
\contentsline {subsection}{\numberline {10.5.2}Conditional Entropy}{59}{subsection.10.5.2}%
\contentsline {subsection}{\numberline {10.5.3}Variational Mutual Information Maximization}{59}{subsection.10.5.3}%
\contentsline {chapter}{\numberline {11}Hidden Markov Models}{61}{chapter.11}%
\contentsline {section}{\numberline {11.1}Introduction}{61}{section.11.1}%
\contentsline {subsection}{\numberline {11.1.1}Conditional Independence}{61}{subsection.11.1.1}%
\contentsline {subsection}{\numberline {11.1.2}Notation}{61}{subsection.11.1.2}%
\contentsline {section}{\numberline {11.2}Bayesian Network}{62}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Bayes Ball}{62}{subsection.11.2.1}%
\contentsline {subsection}{\numberline {11.2.2}Potential Function}{62}{subsection.11.2.2}%
\contentsline {section}{\numberline {11.3}Hidden Markov Models}{64}{section.11.3}%
\contentsline {section}{\numberline {11.4}Evaluation: Forward-Backward Probability}{65}{section.11.4}%
\contentsline {subsection}{\numberline {11.4.1}Joint Probability}{65}{subsection.11.4.1}%
\contentsline {subsection}{\numberline {11.4.2}Marginal Probability}{65}{subsection.11.4.2}%
\contentsline {subsection}{\numberline {11.4.3}Forward Algorithm}{66}{subsection.11.4.3}%
\contentsline {subsection}{\numberline {11.4.4}Backward Probability}{67}{subsection.11.4.4}%
\contentsline {section}{\numberline {11.5}Decoding: Viterbi Algorithm}{68}{section.11.5}%
\contentsline {section}{\numberline {11.6}Learning: Baum-Welch Algorithm}{70}{section.11.6}%
\contentsline {subsection}{\numberline {11.6.1}EM Algorithm}{70}{subsection.11.6.1}%
\contentsline {section}{\numberline {11.7}Summary}{72}{section.11.7}%
\contentsline {chapter}{\numberline {12}Diffusion Model}{73}{chapter.12}%
\contentsline {section}{\numberline {12.1}Introduction}{73}{section.12.1}%
\contentsline {section}{\numberline {12.2}Forward Diffusion}{74}{section.12.2}%
\contentsline {section}{\numberline {12.3}Backward Process}{75}{section.12.3}%
\contentsline {section}{\numberline {12.4}Distribution Modeling}{77}{section.12.4}%
\contentsline {section}{\numberline {12.5}Summary}{83}{section.12.5}%
\contentsline {section}{\numberline {12.6}Score Matching}{85}{section.12.6}%
\contentsline {subsection}{\numberline {12.6.1}Fisher Divergence}{86}{subsection.12.6.1}%
\contentsline {subsection}{\numberline {12.6.2}Langevin Dynamics}{87}{subsection.12.6.2}%
