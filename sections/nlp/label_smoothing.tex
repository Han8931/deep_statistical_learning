\section{Label Smoothing}
For each training example $x$, our model computes the probability of each label $k\in \{1,...,K\}$, $p(k|x) = \frac{\exp(z_k)}{\sum_i \exp(z_i)}$. Here $z_k$ are the logits.

Label smoothing is a mechanism to regularize the classifier layer by estimating the marginalized effect of label-dropout during training.

Vanila corss-entropy can cause two problem: 
\begin{itemize}
    \item First, it may result in over-fitting: if the model learns to assign full probability to the ground-truth label for each training example, it is not guaranteed to generalize. 
    \item Second, it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient $\frac{\partial \ell}{\partial z_k}$, reduces the ability of the model to adapt. Intuitively, this happens because the model becomes too confident about its predictions. 
\end{itemize}
We propose a mechanism for encouraging the model to be less confident. While this may not be desired if the goal is to maximize the log-likelihood of training labels, it does regularize the model and makes it more adaptable. The method is very simple. Consider a distribution over labels $u(k)$, independent of the training example $x$, and a smoothing parameter $\epsilon$. For a training example with ground-truth label $y$, we replace the label distribution $q(k|x) = \delta_{k,y}$ with
$$q'(k|x) = (1-\epsilon)\delta_{k,y}+\epsilon u(k)$$
which is a mixture of the original ground-truth distribution $q(k|x)$ and the fixed distribution $u(k)$, with weights $1-\epsilon$ and $\epsilon$, respectively. This can be seen as the distribution of the label $k$ obtained as follows: 
$$q'(k|x) = (1-\epsilon)\delta_{k,y}+\frac{\epsilon}{K} $$
We refer to this change in ground-truth label distribution as label-smoothing regularization, or LSR.

\subsection{\href{https://leimao.github.io/blog/Label-Smoothing/}{Another Interpretation}}

Instead of using one-hot encoded vector, we introduce noise distribution $u(y|x)$. Our new ground truth label for data $(x_i,y_i)$ would be

\begin{align*}
p^{\prime}(y|x_i) &= (1-\varepsilon) p(y|x_i) + \varepsilon u(y|x_i) \\
&=
\begin{cases}
    1 - \varepsilon + \varepsilon u(y|x_i) & \text{if } y = y_i \\
    \varepsilon u(y|x_i) & \text{otherwise}
\end{cases}
\end{align*}
Where $\varepsilon$ is a weight factor, $\varepsilon\in [0,1]$, and note that $\sum_{y=1}^{K}p'(y|x_i)=1$.
