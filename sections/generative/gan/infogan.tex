\section{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}
\label{sec:q}

\subsection{Joint Entropy}
\begin{align*}
H(X,Y) = \mathbbm{E}_{X,Y}[-\log p(x,y)] = -\sum_{x,y}p(x,y)\log p(x,y)
\end{align*}
\subsection{Conditional Entropy}
\begin{align*}
H(X|Y) &= \mathbbm{E}_{Y}[H(X,Y)] \\
&= -\sum_{y\sim p_Y(y)}p(y)\sum_{x\sim p_X(x)}p(x|y)\log p(x|y)\\
& = -\sum_{y\sim p_Y(y)}\sum_{x\sim p_X(x)}p(y)p(x|y)\log p(x|y)\\
& = -\sum_{y\sim p_Y(y)}\sum_{x\sim p_X(x)}p(x,y)\log p(x|y) = -\mathbbm{E}_{x,y}[\log p(x|y)]\\
& = -\sum_{y\sim p_Y(y)}\sum_{x\sim p_X(x)}p(x,y)\log \frac{p(x,y)}{p(y)}\\
& = -\sum_{y\sim p_Y(y)}\sum_{x\sim p_X(x)}p(x,y)\log p(x,y) + \sum_{y\sim p_Y(y)}\sum_{x\sim p_X(x)}p(x,y)\log p(y)\\
& = H(X,Y) - H(Y)
\end{align*}
\subsection{Variational Mutual Information Maximization}
\begin{align*}
I(c;G(z,c)) &= H(c) - H(c|G(z,c))\\
& = H(c) + \int\int p(c=c',x=G(z,c))\log p(c=c'|x=G(z,c)) dc' dz\\
& = H(c) + \mathbbm{E}_{x\sim G(z,c),c'\sim p(c|x)}[\log p(c'|x)]\\
& = H(c) + \mathbbm{E}_{x\sim G(z,c)}\mathbbm{E}_{c'\sim p(c|x)}[\log p(c'|x)]\\
& = H(c) + \mathbbm{E}_{x\sim G(z,c)}\mathbbm{E}_{c'\sim p(c|x)}\Bigg[\log \frac{p(c'|x)Q(c'|x)}{Q(c'|x)}\Bigg]\\
& = H(c) + \mathbbm{E}_{x\sim G(z,c)}\mathbbm{E}_{c'\sim p(c|x)}\Bigg[\log \frac{p(c'|x)}{Q(c'|x)}\Bigg] + \mathbbm{E}_{x\sim G(z,c)}\mathbbm{E}_{c'\sim p(c|x)}\Big[\log Q(c'|x)\Big]\\
& = H(c) + \mathbbm{E}_{x\sim G(z,c)}\Bigg[D_{KL}(p(c'|x)||Q(c'|x))\Bigg] + \mathbbm{E}_{x\sim G(z,c)}\mathbbm{E}_{c'\sim p(c|x)}\Big[\log Q(c'|x)\Big]\\
& \geq H(c) + \mathbbm{E}_{x\sim G(z,c)}\mathbbm{E}_{c'\sim p(c|x)}\Big[\log Q(c'|x)\Big] \footnotemark
\end{align*}

Thus we get a lower bound for the mutual information as follows:

$$I(c;G(z,c)) \geq H(c) + \mathbbm{E}_{x\sim G(z,c)}\mathbbm{E}_{c'\sim p(c|x)}\Big[\log Q(c'|x)\Big]$$

However, we still have a problem. We need to sample $c$ from $p(c|x)$. Thus, we need to replace it with a known distribution. Firstly, with the reasoning that $x\sim G(z,c)$ means sample $c$ from $p(c)$ then sample $x$ from $G(z,c)$. So we can express $\mathbbm{E}_{x\sim G(z,c)}$ with $\mathbbm{E}_{c\sim p(c)}\mathbbm{E}_{x\sim G(z,c)}$. and by the Lemma \ref{lemma:1}, 
\begin{align*}
I(c;G(z,c)) &\geq H(c) + \mathbbm{E}_{x\sim G(z,c)}\mathbbm{E}_{c'\sim p(c|x)}\Big[\log Q(c'|x)\Big]\\
&= H(c) + \mathbbm{E}_{c\sim p(c)}\mathbbm{E}_{x\sim G(z,c)}\mathbbm{E}_{c'\sim p(c|x)}\Big[\log Q(c'|x)\Big]\\
& = H(c) + \mathbbm{E}_{c\sim p(c)}\mathbbm{E}_{x\sim G(z,c)}\Big[\log Q(c'|x)\Big] \footnotemark
\end{align*}

Thus, we can directly sample $c$ from the known distribution instead of $p(c|x)$.

\begin{lemma}
	For random variables $X, Y$ and function $f(x, y)$ under suitable regularity conditions:
	$$\mathbbm{E}_{x\sim X, y\sim Y|x}[f(x,y)] = \mathbbm{E}_{x\sim X, y\sim Y|x, x'\sim X|y}[f(x',y)]$$
	\begin{proof}
		\begin{align*}
		\mathbbm{E}_{x\sim X, y\sim Y|x}[f(x,y)] &=\int_x P(x)\int_y P(y|x)f(x,y)dydx\\
		& = \int_x\int_yP(x,y)f(x,y)dydx\\
		& = \int_{x'}\int_yP(x',y)f(x',y)dydx'\\
		& = \int_{x'}\int_y P(y)P(x'|y)f(x',y)dydx'\\
		& = \int_{x'}\int_y\int_{x} P(x,y)P(x'|y)f(x',y)dxdydx'\\
		& = \int_{x}P(x)\int_y P(x|y) \int_{x'} P(x'|y)f(x',y)dxdydx'\\
		& = \mathbbm{E}_{x\sim X, y\sim Y|x, x'\sim X|y}[f(x',y)]
		\end{align*}
	\end{proof}
	\label{lemma:1}
\end{lemma} 
