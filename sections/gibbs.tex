\section{Gibbs Sampling}
% \label{sec:}

The phrase ``Markov chain Monte Carlo'' encompasses a broad array of techniques that have in common a few key ideas. The setup for all the techniques that we will discuss in this book is as follows:

\begin{enumerate}
	\item We want to sample from a some complicated density or probability mass function $\pi$. Often, this density is the result of a Bayesian computation so it can be interpreted as a posterior density. The presumption here is that we can evaluate $\pi$ but we cannot sample from it.
	\item We know that certain stochastic processes called Markov chains will converge to a stationary distribution (if it exists and if specific conditions are satisfied). Simulating from such a Markov chain for a long enough time will eventually give us a sample from the chainâ€™s stationary distribution.
	\item Given the functional form of the density $\pi$, we want to construct a Markov chain that has $\pi$ as its stationary distribution.
	\item We want to sample values from the Markov chain such that the sequence of values $\{x_n\}$ generated by the chain converges in distribution to the density $\pi$.
\end{enumerate}

In order for all these ideas to make sense, we need to first go through some background on Markov chains. The rest of this chapter will be spent defining all these terms, the conditions under which they make sense, and giving examples of how they can be implemented in practice.

\subsection{Markov Chain}

A Markov chain is a stochastic process that evolves over time by transitioning into different states. The sequence of states is denoted by the collection $\{X_i\}$ and the transition between states is random, following the rule 
$$P(X_t|X_{t-1},\dots,X_0)=P(X_t|X_{t-1})$$

\begin{itemize}
	\item Each node has a probability distribution of states.
	\item Each link represents a probability state transition.  
		% \begin{align*}
		% 	P(X_1=i) = \sum_{i=1}^N P(X_i=j|X_0=i)P(X_0=i)\\
		% \end{align*}
\end{itemize}

\begin{itemize}
	\item $i \to j$: Accessible if a state $j$ is accessible from $i$. 
		\begin{itemize}
			\item $i \leftrightarrow j$: Communicate between the two states.
		\end{itemize}
	\item Reducibility: A Markov chain is \textbf{\textit{irreducible}} if $i\leftrightarrow j, \forall i,j\in S$. Simply, if all states can visit other states, then it is irreducible. 
	\item Periodic: State $i$ has a period $d$ (\ie periodically visit the state $i$) $\leftrightarrow$ aperiodic.
	\item Transience: A state is transient if, when we leave this state, there is a non-zero probability that we will never return to it. Conversely, a state is recurrent if we know that we will return to that state, in the future, with probability 1 after leaving it (if it is not transient). 
		\begin{itemize}
			\item Stationary Distribution: A probability of being in a state s at time-step t is equal to a probability of being in the state s at the next time-step. Then, it is a stationary probability distribution.
		\end{itemize}
	\item Ergodicity: A state is ergodic if the state is recurrent, aperiodic. Markov chain is ergodic if all states are ergodic. 
\end{itemize}

\subsection{Stationary Distribution}
% The return time $RT_i = \min\{n>0: X_n=i|X_0=i\}$ is the minimum time when we observe the state $X_n$ is at $i$ after the first visit ($X_0$) at $n$. 

% Limit theorem of Markov chain:
% \begin{itemize}
% 	\item If a MC is irreducible and ergodic.
% \end{itemize}

\paragraph{Limit theorem of Markov chain}

For a Markov chain with a discrete state space and transition matrix $P$, let $\pi_*$ be such that $\pi_*P=\pi_*$. Then $\pi_*$ is a stationary distribution of the Markov chain and the chain is said to be stationary if it reaches this distribution.

The basic limit theorem for Markov chains says that, under a specific set of assumptions that we will detail below, we have 
$$||\pi_*-\pi_n|| \to 0$$
as $n\to\infty$, where $||\cdot||$ is the total variation distance between the two densities. Therefore, no matter where we start the Markov chain ($\pi_0$), $\pi_n$ will eventually approach the stationary distribution. Another way to think of this is that 
$$\lim_{n\to\infty}\pi_n(i)=\pi_*(i).$$
for all states $i$ in the state space. Note that $\pi_0$ is the probability distribution of the Markov chain at time 0. Also, $\pi_n$ denote the distribution of the chain at time $n$.

\url{https://bookdown.org/rdpeng/advstatcomp/background.html}


\paragraph{Reversible MC}
Consider a stationary ergodic Markov chain with transition probability$p(i, j)$ and stationary distribution $\pi(i)$, if we reverse the process, we will get a reversed Markov chain with transition probability$q(i, j)$: 
\begin{align*}
	q(j,i) &= P(X_m=i|X_{m+1}=j)\\
		   &= \frac{P(X_m=i,X_{m+1}=j)}{P(X_{m+1}=j)}\\
		   &= \frac{P(X_m=i|X_{m+1}=j)P(X_{m+1}=j)}{P(X_{m+1}=j)}\\
		   &= \frac{\pi(i)p(i,j)}{\pi(j)}\\
	\pi(i)p(i,j) &= \pi(j)q(j,i)
\end{align*}
If $p(i,j) = q(j,i)$, it is called time-reversible Markov chain. 

% \section{Markov Chain for Sampling}

\section{Markov Chain Monte Carlo}
The basic sampling methods we have learnt so far do not leverage past information, which assumes all samples are independent. In Markov chain based sampling, we will treat random variables as a sequence of sampling process. 

In Markov Chain Monte Carlo(MCMC), we assume that a stationary distribution is already known. We are more interested in estimating a transition rule that describing the stationary distribution. 

\subsection{Metropolis-Hasting Algorithm}
\begin{itemize}
	\item Current value $z^t$
	\item Propose a candidate $z^*\sim q(z^*|z^t)$ where $q_t$ is a proposal distribution (\ie Normal distribution). 
	\item Same as importance and rejections samplings, yet the difference is the Markov property idea in the proposal distribution. 
	\item With an acceptance probability (or moving criteria), $\alpha$.
\end{itemize}

\href{https://youtu.be/oX2wIGSn4jY}{Ref: YouTube Lecture}

% The product of MCMC is a posterior distribution.
