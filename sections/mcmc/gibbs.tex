\section{Gibbs Sampling}
% \label{sec:}

The phrase ``Markov chain Monte Carlo'' encompasses a broad array of techniques that have in common a few key ideas. The setup for all the techniques that we will discuss in this book is as follows:

\begin{enumerate}
	\item We want to sample from a some complicated density or probability mass function $\pi$. Often, this density is the result of a Bayesian computation so it can be interpreted as a posterior density. The presumption here is that we can evaluate $\pi$ but we cannot sample from it.
	\item We know that certain stochastic processes called Markov chains will converge to a stationary distribution (if it exists and if specific conditions are satisfied). Simulating from such a Markov chain for a long enough time will eventually give us a sample from the chain’s stationary distribution.
	\item Given the functional form of the density $\pi$, we want to construct a Markov chain that has $\pi$ as its stationary distribution.
	\item We want to sample values from the Markov chain such that the sequence of values $\{x_n\}$ generated by the chain converges in distribution to the density $\pi$.
\end{enumerate}

In order for all these ideas to make sense, we need to first go through some background on Markov chains. The rest of this chapter will be spent defining all these terms, the conditions under which they make sense, and giving examples of how they can be implemented in practice.

\section{Markov Chain}
Reference: \href{https://gregorygundersen.com/blog/2019/10/28/ergodic-markov-chains/}{Link}

A Markov chain is a stochastic process that evolves over time by transitioning into different states. The sequence of states is denoted by the collection $\{X_i\}$ and the transition between states is random, following the rule 

\begin{definition}
	Let $D$ be a finite set. A random process $X_1, X2,\dots$  with values in $D$ is called a Markov chain if
$$P(X_t=x_{t+1}|X_{t}=x_t,\dots,X_0=x_0)=P(X_{t+1}=x_{t+1}|X_{t}=x_t)$$
\label{def:markov_chain}
\end{definition}
We can think of $X_t$ as a random state at time $t$, and the Markovian assumption is that the probability of transitioning from $x_t$ to $x_{t+1}$ only depends on $x_t$. In words, the future depends only on the present. Let $p_{ij}$ be the probability of transitioning from state $i$ to state $j$. A Markov chain can be defined by a transition probability matrix:
\begin{definition}
	The matrix $\mathbf{P}=(p_{ij})_{i,j}\in D$ is called the transition probability matrix.
\label{def:markov_chain_transition_matrix}
\end{definition}
Thus, $P$ is a $∣D∣\times∣D∣$ matrix, where $∣D∣$ denotes the cardinality of $D$, and the cell value $p_{ij}$ is the probability of transitioning from state $i$ to state j$,$ and the rows of $P$ must sum to one. In this post, we will restrict ourselves to time \textit{homogeneous Markov chains}:

\begin{definition}
A Markov chain is called time homogeneous if 
$$\mathbb{P}{X_{t+1}=j∣X_n=i}=p_{ij},\, \forall n.$$
\end{definition}
In words, the transition probabilities are not changing as a function of time. Finally, let's introduce some useful notation for the initial state of the Markov chain. Let

\begin{itemize}
	\item Each node has a probability distribution of states.
	\item Each link represents a probability state transition.  
		% \begin{align*}
		% 	P(X_1=i) = \sum_{i=1}^N P(X_i=j|X_0=i)P(X_0=i)\\
		% \end{align*}
	\item $i \to j$: Accessible if a state $j$ is accessible from $i$. 
		\begin{itemize}
			\item $i \leftrightarrow j$: Communicate between the two states.
		\end{itemize}
	\item Reducibility: A Markov chain is \textbf{\textit{irreducible}} if $i\leftrightarrow j, \forall i,j\in S$. Simply, if all states are able to visit other states, it is irreducible. 
	\item Periodic: State $i$ has a period $d$ (\ie periodically visit the state $i$) $\leftrightarrow$ aperiodic.
	\item Transience: A state is \textit{transient} if, when we leave this state, there is a non-zero probability that we will never return to it. Conversely, a state is \textit{recurrent} if we know that we will return to that state, in the future, with probability 1 after leaving it (if it is not transient). 
		\begin{itemize}
			\item Stationary Distribution: A probability of being in a state s at time-step t is equal to a probability of being in the state s at the next time-step. Then, it is a stationary probability distribution.
		\end{itemize}
	\item Ergodicity: A state is ergodic if the state is recurrent, aperiodic. Markov chain is ergodic if all states are ergodic. 
\end{itemize}

\subsection{Stationary Distribution}
% The return time $RT_i = \min\{n>0: X_n=i|X_0=i\}$ is the minimum time when we observe the state $X_n$ is at $i$ after the first visit ($X_0$) at $n$. 

% Limit theorem of Markov chain:
% \begin{itemize}
% 	\item If a MC is irreducible and ergodic.
% \end{itemize}

\paragraph{Limit theorem of Markov chain}

For a Markov chain with a discrete state space and transition matrix $P$, let $\pi_*$ be such that $\pi_*P=\pi_*$. Then $\pi_*$ is a stationary distribution of the Markov chain and the chain is said to be stationary if it reaches this distribution.

The basic limit theorem for Markov chains says that, under a specific set of assumptions that we will detail below, we have 
$$||\pi_*-\pi_n|| \to 0$$
as $n\to\infty$, where $||\cdot||$ is the total variation distance between the two densities. Therefore, no matter where we start the Markov chain ($\pi_0$), $\pi_n$ will eventually approach the stationary distribution. Another way to think of this is that 
$$\lim_{n\to\infty}\pi_n(i)=\pi_*(i).$$
for all states $i$ in the state space. Note that $\pi_0$ is the probability distribution of the Markov chain at time 0. Also, $\pi_n$ denote the distribution of the chain at time $n$.

\url{https://bookdown.org/rdpeng/advstatcomp/background.html}


\paragraph{Reversible MC}
Consider a stationary ergodic Markov chain with transition probability$p(i, j)$ and stationary distribution $\pi(i)$, if we reverse the process, we will get a reversed Markov chain with transition probability$q(i, j)$: 
\begin{align*}
	q(j,i) &= P(X_m=i|X_{m+1}=j)\\
		   &= \frac{P(X_m=i,X_{m+1}=j)}{P(X_{m+1}=j)}\\
		   &= \frac{P(X_m=i|X_{m+1}=j)P(X_{m+1}=j)}{P(X_{m+1}=j)}\\
		   &= \frac{\pi(i)p(i,j)}{\pi(j)}\\
	\pi(i)p(i,j) &= \pi(j)q(j,i)
\end{align*}
If $p(i,j) = q(j,i)$, it is called time-reversible Markov chain. 

% \section{Markov Chain for Sampling}

