\chapter{Gaussian Process}

\section{Introduction}
\label{sec:gaussian_process}
A \textit{Gaussian process} (GP) is a probability distribution over possible functions that fit a set of points fully specified by a mean and covariance function. It is a generalization of the Gaussian probability distribution. Whereas a probability distribution describes random variables which are scalars or vectors (for multivariate distributions), a stochastic process governs the properties of functions.


\textbf{It means that GPs don't give you a single function to fit data. They will give you an entire distribution of possible functions.} More formally, GPs define \textbf{distributions over functions} $f(x)$ of which the distribution is defined by a mean function $m(x)=\mathbb{E}[f(x)]$ and positive definite covariance function $k(x,x')=\mathbb{E}[(f(x)-m(x))(f(x')-m(x'))]$, which tells us how points are related to each other (\ie \textit{kernel}). Thus, it is a distribution over functions whose shape is defined by the kernel:
% A GP is a collection of R.Vs., where any finite subset follows a multivariate Gaussian distribution.
\begin{align*}
	f(x) \sim \mathcal{GP}(m(x),k(x,x')).
\end{align*}

% Since we have the probability distribution over all possible functions, we can calculate the mean and the variance of the function to determine how confident in our predictions.
Let's say we have points then GPs assume that the function values of the points follow a multivariate Gaussian distribution. For example, we have observations $\mX=\{x_1, \dots, x_5\}$, then we have

\begin{align*}
	\begin{bmatrix}
		f(x_1)\\
		f(x_2)\\
		\vdots \\
		f(x_5)
	\end{bmatrix}\sim \mathcal{N}
	\begin{pmatrix}
	\begin{bmatrix}
		m(x_1)\\
		m(x_2)\\
		\vdots\\
		m(x_5)
	\end{bmatrix},
	\begin{bmatrix}
		k(x_1,x_1)& \cdots& k(x_1,x_5)\\
		\vdots& \ddots& \vdots\\
		k(x_5,x_1)& \cdots& k(x_5,x_5)\\
	\end{bmatrix}
	\end{pmatrix},
\end{align*}

More formally, let' say we have estimated functions $\rvf = [f(x_1), \dots, f(x_n)]$ with these observations. Now say we have some new points $\mathbf{X}_*$ where we want to predict $f(\mathbf{X}_*)$.

The joint distribution of $\rvf$ and $\rvf_*$ can be modeled as:
\begin{align*}
	\begin{pmatrix}
		\rvf\\
		\rvf_*
		\end{pmatrix}\sim \mathcal{N} 
		\begin{pmatrix}
		\begin{pmatrix}
			m(\mathbf{X})\\
			m(\mathbf{X}_*)
		\end{pmatrix},
		\begin{pmatrix}
			\mathbf{K} & \mathbf{K}_*\\
			\mathbf{K}_*^T & \mathbf{K}_{**}
		\end{pmatrix}
		\end{pmatrix},
\end{align*}
% where 
% However, what we want is $\rvf_*$, predictions for new observations, which is a conditional distribution over $\rvf_*$. 
% Let $f(x) = \phi(x)^Tw$, with $w\sim \mathcal{N}(0, \alpha^{-1}\mathbf{I})$. Then, $m(x) = \mathbb{E}[f(x)] = \mathbb{E}[w]^T\phi(x) = 0$
% $k(x, x') = \mathbb{E}[(f(x)-m(x))(f(x')-m(x'))] =\mathbb{E}[f(x)(f(x')] $
where $m(X) = [m(x_1),\dots, m(x_n)]$ and
\begin{align*}
	\mathbf{K }&= \kappa(\mathbf{X,X})\\
	\mathbf{K}_* &= \kappa(\mathbf{X},\mathbf{X}_*)\\
	\mathbf{K}_{**} &= \kappa(\mathbf{X}_*,\mathbf{X}_*).
\end{align*}
The mean is assumed to be $(m(\mathbf{X}), m(\mathbf{X}_*))=0$. 

While this equation describes the joint probability distribution $P(\rvf, \rvf_*|\mathbf{X}, \mathbf{X}_*)$ over $\rvf$ and $\rvf_*$ in regressions, we need the conditional distribution $P(\rvf_*|\rvf, \mathbf{X}, \mathbf{X}_*)$ for a prediction. The conditional distribution can be achieved by the Marginal and conditional distributions of MVN theorem:

\begin{theorem} Marginals and conditionals of an MVN:
A random vector $\mathbf{X}$ follows a multivariate normal distribution:
\begin{align*}
	\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Sigma})
\end{align*}
where $\mathbf{X}$ can be partitioned as:
\begin{align*}
	\mathbf{X} = \begin{bmatrix}
	\mathbf{X}_1 \\
	\mathbf{X}_2
	\end{bmatrix}
\end{align*}
with corresponding partitions of the mean vector $\boldsymbol{\mu}$ and the covariance matrix \(\mathbf{\Sigma}\):
\begin{align*}
\boldsymbol{\mu} = \begin{bmatrix}
\boldsymbol{\mu}_1 \\
\boldsymbol{\mu}_2
\end{bmatrix}, \quad
\mathbf{\Sigma} = \begin{bmatrix}
\mathbf{\Sigma}_{11} & \mathbf{\Sigma}_{12} \\
\mathbf{\Sigma}_{21} & \mathbf{\Sigma}_{22}
\end{bmatrix},\quad
\mathbf{\Lambda} = \mathbf{\Sigma}^{-1} = \begin{bmatrix}
\mathbf{\Lambda}_{11} & \mathbf{\Lambda}_{12} \\
\mathbf{\Lambda}_{21} & \mathbf{\Lambda}_{22}
\end{bmatrix},
\end{align*}
Then, the marginal distribution of $\mathbf{X}_1$ and $\mathbf{X}_2$ are given by
\begin{align*}
	&\mathbf{X}_1 \sim \mathcal{N}(\boldsymbol{\mu}_1, \mathbf{\Sigma}_{11})\\
	&\mathbf{X}_2 \sim \mathcal{N}(\boldsymbol{\mu}_2, \mathbf{\Sigma}_{22})
\end{align*}
Subsequently, the (posteriror) conditional distribution of $\mathbf{X}_1$ given $\mathbf{X}_2$ is:
\begin{align*}
	\mathbf{X}_1 | \mathbf{X}_2 = \rvx_2\sim \mathcal{N}(\boldsymbol{\mu}', \mathbf{\Sigma}'),
\end{align*}
where the conditional mean \(\boldsymbol{\mu}_{1|2}\) and the conditional covariance \(\mathbf{\Sigma}_{1|2}\) are given by:
\begin{align*}
	&\boldsymbol{\mu}' = \boldsymbol{\mu}_1 + \mathbf{\Sigma}_{12}\mathbf{\Sigma}_{22}^{-1} (\mathbf{x}_2 - \boldsymbol{\mu}_2)\\
	&\mathbf{\Sigma}' = \mathbf{\Sigma}_{11} - \mathbf{\Sigma}_{12}\mathbf{\Sigma}_{22}^{-1}\mathbf{\Sigma}_{21}
\end{align*}
\end{theorem}
Note that $\mathbf{\Sigma}'$ does no depend on $\rvx_2$. It doesn't hold for general (non-Gaussian) random variables.
By using the theorem, we get
$$\rvf_*|\rvf, \mathbf{X}, \mathbf{X}_*\sim \mathcal{N}(\mathbf{K}_*^T\mathbf{K}^{-1}\rvf, \mathbf{K}_{**}-\mathbf{K}_*^T\mathbf{K}^{-1}\mathbf{K}_{*})$$

In practice, we typically have noisy estimations of target functions, $y = f(x)+\epsilon$. By assuming the i.i.d., Gaussian noise with variance $\sigma_n^2$, the prior on these noisy observations then becomes $cov(y) = \mathbf{K}+\sigma_n^2\mathbf{I}$. The joint distribution of the observed target values and the function values at the test locations under the prior as 
\begin{align*}
	\begin{bmatrix}
		\rvy\\
		\rvf_*
	\end{bmatrix} \sim \mathcal{N}
	\begin{bmatrix}
		\mathbf{0}, \begin{bmatrix}
			\mathbf{K}+\sigma_n^2\mathbf{I} & \mathbf{K}_*\\
			\mathbf{K}_*^T & \mathbf{K}_{**}
		\end{bmatrix}
	\end{bmatrix}
\end{align*}

\section{Regression using Gaussian Process}
\begin{itemize}
	\item A set of input points $X = \{x_1, x_2, \dots, x_n\}$ (\ie training data).
	\item A set of corresponding output values $y = \{y_1, y_2, \dots, y_n\}$.
\end{itemize}
We assume that the output values are generated by some unknown function $f(x)$ with some Gaussian noise $\epsilon$:
\begin{align*}
	y = f(X) + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma_n^2)
\end{align*}
The goal is to predict the value of the function $f(x_*)$ at a new input point $x_*$.

We assume that the function $f(x)$ is drawn from a \textbf{Gaussian Process}:
\begin{align*}
	f(x) \sim \mathcal{GP}(m(x), k(x, x')),
\end{align*}
where:
\begin{itemize}
	\item $m(x)$ is the mean function (usually assumed to be 0 for simplicity).
	\item $k(x, x')$ is the covariance function (or kernel), which encodes assumptions about the smoothness of the function.
\end{itemize}
The actual observations $y = f(x) + \epsilon$ include Gaussian noise $\epsilon$, which is typically assumed to have zero mean and variance \( \sigma_n^2 \).

To predict the function value \( f_* = f(x_*) \) at a new point \( x_* \), we use the **posterior distribution** over the function, conditioned on the observed data \( X \) and \( y \). This leads to two main equations:
\begin{itemize}
	\item Posterior Mean (Prediction):
		\begin{align*}
		  \mathbb{E}[f_*] = K(X, x_*)^\top [K(X, X) + \sigma_n^2 I]^{-1} y
		\end{align*}
	\item Posterior Variance (Uncertainty):
		\begin{align*}
			\text{Var}[f_*] = K(x_*, x_*) - K(X, x_*)^\top [K(X, X) + \sigma_n^2 I]^{-1} K(X, x_*)
		\end{align*}
\end{itemize}
These equations give us the mean and variance of the predictive distribution for the function value at the new point $x_*$.

\paragraph{Example:} First, choose a covariance function (\ie Kernel). The kernel function $k(x, x')$ is crucial in defining the relationship between different input points. Some commonly used kernels are:

Squared Exponential (RBF) Kernel:
\begin{align*}
	k(x, x') = \sigma_f^2 \exp\left(-\frac{(x - x')^2}{2l^2}\right),
\end{align*}
where $l$ is the length scale and $\sigma_f^2$ is the signal variance. The kernel function governs the smoothness and behavior of the GP, so selecting an appropriate kernel is important.

Subsequently, compute the covariance matrices:
\begin{itemize}
	\item Covariance Matrix for Training Points: $K(X, X)$
		\begin{align*}
			K(X, X) = \begin{bmatrix}
				k(x_1, x_1) & k(x_1, x_2) & \dots & k(x_1, x_n) \\
				k(x_2, x_1) & k(x_2, x_2) & \dots & k(x_2, x_n) \\
				\vdots & \vdots & \ddots & \vdots \\
				k(x_n, x_1) & k(x_n, x_2) & \dots & k(x_n, x_n)
			\end{bmatrix}
		\end{align*}
	\item Covariance Between Training Points and Test Point: $K(X, x_*)$
		\begin{align*}
			K(X, x_*) = \begin{bmatrix}
				k(x_1, x_*) \\
				k(x_2, x_*) \\
				\vdots \\
				k(x_n, x_*)
			\end{bmatrix}
		\end{align*}
	\item Covariance at the Test Point: $K(x_*, x_*) = k(x_*, x_*)$
\end{itemize}
Then, compute the posterior mean and variance by Using the posterior mean and posterior variance equations from earlier, compute:
\begin{itemize}
	\item The mean prediction $\mathbb{E}[f_*]$ at the test point $x_*$.
	\item The uncertainty $\text{Var}[f_*]$ at $x_*$.
\end{itemize}
This gives the complete predictive distribution for the function value at $x_*$.

\section{Time Complexity}

One drawback of Gaussian processes is that it scales very badly with the number of observations $N$. It takes $O(N^3)$ time.
