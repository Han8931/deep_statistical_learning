\chapter{Regression}
\label{sec:basic_regression}
Suppose a noisy measurement $\rvy = [y_1, \dots, y_m]^T$ with noise $\boldsymbol{\eta} = [\eta_1, \dots, \eta_d]^T$ and we want to estimate parameter $\boldsymbol{\theta} = [\theta_1,\dots,\theta_d]^T$ by using our input $\rvx = [x_1, \dots, x_d]$. 

The measurement $\rvy$ can be modeled as follows:
$$\rvy = \mathbf{X}\boldsymbol{\theta}+\boldsymbol{\eta},$$
where $\mathbf{X}$ is a $m\times d$ input matrix (or our observations). Given a parameter $\boldsymbol{\theta}$, we consider the difference between the noisy measurements and estimated value as follows:
$$\boldsymbol{\epsilon} = \rvy - \mathbf{X}\boldsymbol{\theta}$$
Then, we can establish an objective function as follows:
$$J(\boldsymbol{\theta}) = \boldsymbol{\epsilon}^T\boldsymbol{\epsilon}$$
Note that this is equivalent to minimizing the mean squared error:
$$MSE = \frac{1}{n}\sum_{i=1}^n (y_i-\rvx_i\boldsymbol{\theta})^2.$$
% The OLEs' solution can be optimized by a closed form as follows:
% $$f(\rvx) = \mathbf{X}\boldsymbol{\theta},$$
% where $\rvx = [x_1, \dots, x_d]$ and $\boldsymbol{\theta} = [\theta_1,\dots,\theta_d]^T$. The ridge regression for $\mathbf{X}\in \mathbb{R}^{n\times d}$ matrix can be modeled as follows:
We can optimize this in a closed-form as follows:
\begin{align*}
	J(\boldsymbol{\theta}) &= \|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2 \\
			&= (\mathbf{y}-\mathbf{X}\boldsymbol{\theta})^T(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})\\
			&= (\mathbf{y}^T-\boldsymbol{\theta}^T\mathbf{X}^T)(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})\\
			&= \rvy^T\rvy-\boldsymbol{\theta}^T\mathbf{X}^T\rvy-\rvy^T\mathbf{X}\boldsymbol{\theta}+\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}
\end{align*}
To find the $\boldsymbol{\theta}$ that minimizes the objective function, we will compute a derivative of the function while setting it equal to zero:
\begin{align*}
	\frac{\partial J}{\partial \boldsymbol{\theta}}&= -\mathbf{X}^T\rvy-\mathbf{X}^T\rvy+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta} = 0\\
	\boldsymbol{\theta}	&= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\rvy
\end{align*}
\begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt

N = 50
x = np.random.randn(N)
w_1 = 3.4 # True Parameter
w_0 = 0.9 # True Parameter
y = w_1*x + w_0 + 0.3*np.random.randn(N) # Synthesize training data

X = np.column_stack((x, np.ones(N)))
W = np.array([w_1, w_0])

# From Scratch
XtX    = np.dot(X.T, X)
XtXinvX = np.dot(np.linalg.inv(XtX), X.T) # d x m
W_best = np.dot(XtXinvX, y.T)
print(f"W_best: {W_best}") 

# Pythonic Approach
theta = np.linalg.lstsq(X, y, rcond=None)[0]
print(f"Theta: {theta}") 

t = np.linspace(0, 1, 200)
y_pred = W_best[0]*t+W_best[1]
yhat = theta[0]*t+theta[1]
plt.plot(x, y, 'o')
plt.plot(t, y_pred, 'r', linewidth=4)
plt.show()
\end{lstlisting}

\section{Overdetermined and Underdetermined Systems}
Recall that the linear regression problem is an optimization problem of finding the optimal parameter as follows:
$$\boldsymbol{\theta}_{opt} = \argmin_{\boldsymbol{\theta}\in \mathbb{R}^d}||y-\mathbf{X}\boldsymbol{\theta}||^2.$$
We say the optimization problem is \textit{overdetermined} if $\mathbf{X}\in \mathbb{R}^{m\times d}$ is tall and skinny, \ie $m>d$. This problem has a unique solution $\boldsymbol{\theta}	&= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\rvy$ if and only if $\mathbf{X}^T\mathbf{X}$ is invertible. Equivalently, $\mathbf{X}$ should be linearly independent (\ie full rank). 

The problem is called \textit{underdetermined} if $\mathbf{X}$ is fat and short, \ie $N<d$. This problem will have infinitely many solutions.  This problem will have infinitely many solutions. Among all the feasible solutions, we pick the one that minimizes the squared norm. The solution is called the \textit{minimum-norm} least squares. Consider a underdetermined linear regression problem:  
\begin{align*}
	\boldsymbol{\theta} = \argmin_{\boldsymbol{\theta}\in \mathbb{R}^d} \|\boldsymbol{\theta}\|^2, \,\textrm{subject to}\ \rvy = \mathbf{X}\boldsymbol{\theta},
\end{align*}
where $\mathbf{X}\in \mathbb{R}^{m\times d}, \boldsymbol{\theta}\in \mathbb{R}^d,$ and $\rvy\in \mathbb{R}^m$. If rank$(\mathbf{X})=N$, then the linear regression problem will have a unique global minimum 
\begin{align*}
	\boldsymbol{\theta} = \mathbf{X}^T(\mathbf{X}\mathbf{X}^T)^{-1}\rvy.
\end{align*}
This solution is called the minimum-norm least-squares solution. The proof of this solution is given by:
\begin{align*}
	\mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\lambda}) = \|\boldsymbol{\theta}\|^2 + \boldsymbol{\lambda}^T(\mathbf{X}\boldsymbol{\theta}-\rvy),
\end{align*}
where $\boldsymbol{\lambda}$ is a Lagrange multiplier. The solution of the constrained optimization is the stationary point of the Lagrangian. To find it, we take the derivatives w.r.t., $\boldsymbol{\lambda}$ and $\boldsymbol{\theta}$ as follows: 
\begin{align*}
	\nabla_{\boldsymbol{\theta}} &= 2 \boldsymbol{\theta} + \mathbf{X}^T\boldsymbol{\lambda} = 0\\ 
	\nabla_{\boldsymbol{\lambda}} &= \mathbf{X}\boldsymbol{\theta} - \rvy = 0
\end{align*}
The first equation gives us $\boldsymbol{\theta} = -\mathbf{X}^T\boldsymbol{\lambda}/2$. Substituting it into the second equation, and assuming that rank$(\mathbf{X})=N$ so that $\mathbf{X}^T\mathbf{X}$ is invertible, we have $\boldsymbol{\lambda} = -2 (\mathbf{X}\mathbf{X}^T)^{-1}\rvy.$ Thus, we have
\begin{align*}
	\boldsymbol{\theta} = \mathbf{X}^T(\mathbf{X}\mathbf{X}^T)^{-1}\rvy.
\end{align*}


\section{Overfitting}
We examine the relationship between the number of training samples and the complexity of the model.


\section{Ridge Regression}
Regularization means that instead of seeking the model parameters by minimizing the training loss alone, we add a penalty term to force the parameters to ``behave better''. 


With the ridge regression principle, we can optimize it as follows:
\begin{align}
	J(\boldsymbol{\theta}) &= \|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2 + \lambda \|\boldsymbol{\theta}\|^2_2 \\
			&= (\mathbf{y}-\mathbf{X}\boldsymbol{\theta})^T(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})+\lambda\boldsymbol{\theta}^T\boldsymbol{\theta}\\
			&= (\mathbf{y}^T-\boldsymbol{\theta}^T\mathbf{X}^T)(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})+\lambda\boldsymbol{\theta}^T\boldsymbol{\theta}\\
			&= \rvy^T\rvy-\boldsymbol{\theta}^T\mathbf{X}^T\rvy-\rvy^T\mathbf{X}\boldsymbol{\theta}+\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+\boldsymbol{\theta}^T\lambda\mathbf{I}\boldsymbol{\theta}\\
	\frac{\partial J}{\partial \boldsymbol{\theta}}&= -\mathbf{X}^T\rvy-\mathbf{X}^T\rvy+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+2\lambda\mathbf{I}\boldsymbol{\theta} = 0\\
	\boldsymbol{\theta}	&= (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\rvy
	\label{eq:ridge_regression}
\end{align}

\begin{itemize}
	\item If $\lambda\to 0$, then $\|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2 + \underbrace{\lambda \|\boldsymbol{\theta}\|^2_2}_{=0}$ 
	\item $\lambda\to \infty$, then $\underbrace{\frac{1}{\lambda}\|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2}_{=0} + \|\boldsymbol{\theta}\|^2_2$, since what we want to do is to minimize the objective function, we can divide it by $\lambda$. Therefore, the solution will be $\boldsymbol{\theta}=0$, because it is the smallest value the squared function can achieve. 
\end{itemize}
Note that $\mathbf{X}^T\mathbf{X}$ is always symmetric \footnote{$(\mathbf{X}^T\mathbf{X})^T = \mathbf{X}^T\mathbf{X}$.}. Thus, it can be decomposed as $Q\Lambda Q^T$ by the Spectral theorem, where $Q and \Lambda$ are eigenvector and eigenvalue matrices. Then, the inverse operation in the ridge regression can be expressed as follows:
\begin{align*}
	\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I} &= \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T+\lambda\mathbf{I}\\
											 &= \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T+\lambda\mathbf{Q}\mathbf{Q}^T\\
											 &= \mathbf{Q}(\mathbf{\Lambda}+\lambda\mathbf{I})\mathbf{Q}^T.
\end{align*}
Even if the symmetric matrix is not invertible or close to not invertible, the regularization constant $\lambda$ makes it invertible (by making it to be a full-rank). 

We can change this into a dual form:
\begin{align}
	(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\boldsymbol{\theta}	&= (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\rvy\\
	(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\boldsymbol{\theta} &= \mathbf{X}^T\rvy\\ 
	\boldsymbol{\theta} &= \lambda^{-1}\mathbf{I}(\mathbf{X}^T\rvy-\mathbf{X}^T\mathbf{X}\boldsymbol{\theta})\\
	&= \mathbf{X}^T\lambda^{-1}(\rvy-\mathbf{X}\boldsymbol{\theta})\\
	&= \mathbf{X}^T\alpha\\
	\lambda\alpha &= (\rvy-\mathbf{X}\boldsymbol{\theta})\\
	&= (\rvy-\mathbf{X}\mathbf{X}^T\alpha)\\
	\rvy &= (\mathbf{X}\mathbf{X}^T\alpha+\lambda\alpha)\\
	\alpha &= (\mathbf{X}\mathbf{X}^T+\lambda)^{-1}\rvy\\
	\alpha &= (\mathbf{G}+\lambda)^{-1}\rvy,
	\label{eq:dual_regression}
\end{align}
where $\mathbf{G} = \mathbf{X}\mathbf{X}^T$ is a \textit{Gram matrix}. Thus, we just have to solve $m\times m$ matrix. 




\section{Weighted LSE}
The OLEs assume an equal confidence on all the measurements. Now we look at varying confidence in the measurements. We assume that the noise for each measurement has zero mean and is independent, then the covariance matrix for all measurement noise is given by
\begin{align*}
	R &= \mathbb{E}(\eta\eta^T)\\
	  &= \begin{bmatrix}
		  \sigma_1^2 & \dots & 0\\
		  \vdots & \ddots & \vdots\\
		  0 & \ddots & \sigma_l^2\\
	  \end{bmatrix}
\end{align*}
By denoting the error vector $\rvy-\mathbf{X}\boldsymbol{\theta}$ as $\boldsymbol{\epsilon} = (\epsilon_1, \dots, \epsilon_l)^T$, we will minimize the sum of squared differences weighted over the variations of the measurements:
$$J(\tilda{\rvx}) = \boldsymbol{\epsilon}^TR^{-1}\boldsymbol{\epsilon}=\frac{\boldsymbol{\epsilon}_1^2}{\sigma_1^2}+\dots+\frac{\boldsymbol{\epsilon}_l^2}{\sigma_l^2}$$
% This is equivalent to
% $$\frac{1}{n}\sum_{i=1}^{n}\sum_{i=1}^n \alpha_i (y_i-\rvx_i\boldsymbol{\theta}).$$

The best estimate of the parameter is given by
$$\boldsymbol{\theta} = (\mathbf{X}^TR^{-1}\mathbf{X})^{-1}\mathbf{X}^TR^{-1}\rvy.$$
Note that the measurement noise matrix $R$ must be non-singular for a solution to exist.

\section{Recursive Least Squares}
\label{sec:recursive_least_square}

\begin{itemize}
	\item $H==X$
	\item $x==\theta$
\end{itemize}

The ordinary least-squares assumes that all measurements are available at a certain time. However, this often might not be the case in practice. More often, we obtain measurements sequentially and want to update our estimate with each new measurement. In this case, the matrix $H$ needs to be augmented. This update can be very expensive. When then number of measurements is extremely large, the solutions of the least squares problem are difficult to compute. 

These motivate the RLS. Suppose we have an estimate $\tilda{\rvx}_{k-1}$ after $k-1$ measurements, and obtain a new measurement $\rvy_k$. To be general, every measurements is now an $m$-vector with values yielded by, say, several measuring instruments. 

A linear recursive estimator can be written in the following form:
\begin{align*}
	\rvy_k &= H_k\rvx+\eta_k\\
	\tilda{\rvx}_k &= \tilda{\rvx}_{k-1}+K_k (\rvy_k-H_k\tilda{\rvx}_{k-1})
\end{align*}
Here $H_k$ is an $m\times n$ matrix, and $K_k$ is $n\times m$ and referred to as the \textit{estimator gain matrix}. We refer to $\rvy_k-H_k\tilda{\rvx}_{k-1}$ as the \textit{correction term}. Namely, the new estimate is modified from the previous estimate $\tilda{\rvx}_{k-1}$ with a correction via the gain vector. 

The current estimation error is

\begin{align*}
	\boldsymbol{\epsilon}_k	&= \rvx-\tilda{\rvx}_k \\
							&= \rvx-\tilda{\rvx}_{k-1} - K_k (\rvy_k-H_k\tilda{\rvx}_{k-1})\\
							&= \boldsymbol{\epsilon}_{k-1}-K_k (H_k\rvx+\eta_k-H_k\tilda{\rvx}_{k-1})\\
							&= \boldsymbol{\epsilon}_{k-1}-K_k H_k(\rvx-\tilda{\rvx}_{k-1})-K_k\eta_k\\
							&= (I-K_k H_k)\boldsymbol{\epsilon}_{k-1}-K_k\eta_k,
\end{align*}
where $I$ is the $n\times n$ identity matrix. The mean of this error is then
$$\mathbb{E}[\boldsymbol{\epsilon}_{k}] = (I-K_k H_k)\mathbb{E}[\boldsymbol{\epsilon}_{k-1}]-K_k\mathbb{E}[\boldsymbol{\eta}_{k}]$$
If $\mathbb{E}[\boldsymbol{\eta}_{k}]=0$ and $\mathbb{E}[\boldsymbol{\epsilon}_{k-1}]=0$, then $\mathbb{E}[\boldsymbol{\epsilon}_{k}]=0$. So if the measurement noise has zero mean for all $k$, and the initial estimate of $\rvx$ is set equal to its expected value, then $\tilda{\rvx}_k=\rvx_k, \forall k$. With this property, the estimator $\tilda{\rvx}_k &= \tilda{\rvx}_{k-1}+K_k (\rvy_k-H_k\tilda{\rvx}_{k-1})$ is \textit{unbiased}. The property holds regardless of the value of the gain vector $K_k$. This means the estimate will be equal to the true value $\rvx$ on average. 

The key is to determine the optimal value of the gain vector $K_k$. The optimality criterion used by us is to minimize the aggregated variance of the estimation errors at time $k$: 
\begin{align*}
	J_k &= \mathbb{E}[||\rvx-\tilda{\rvx}_k||^2]\\
		&= \mathbb{E}[\boldsymbol{\epsilon}_{k}^T\boldsymbol{\epsilon}_{k}]\\
		&= \mathbb{E}[tr(\boldsymbol{\epsilon}_{k}\boldsymbol{\epsilon}_{k}^T)]\\
		&= tr(P_k),
\end{align*}
where $P_k=\mathbb{E}[\boldsymbol{\epsilon}_{k}\boldsymbol{\epsilon}_{k}^T]$ is the estimation-error covariance and the third line is done by the trace of a product (or cyclic property). The expectation in the third line can go into the trace operator by its linearity. Next, we can obtain $P_k$ by
\begin{align*}
	P_k &= \mathbb{E}\bigg[\big((I-K_k H_k)\boldsymbol{\epsilon}_{k-1}-K_k\eta_k\big)\big((I-K_k H_k)\boldsymbol{\epsilon}_{k-1}-K_k\eta_k\big)^T\bigg]
\end{align*}
By rearranging the above equation with the property that the mean of noise is zero, we can get
$P_k = (I-K_k H_k)P_{k-1}(I-K_k H_k)^T+K_kR_kK_k^T.$

This equation is the recurrence for the covariance of the least squares estimation error. It is consistent with the intuition that as the measurement noise $R_k$ increases, the uncertainty $P_k$ increases. 

Next, we have to compute $K_k$ that minimizes the cost function given by error equation. 







