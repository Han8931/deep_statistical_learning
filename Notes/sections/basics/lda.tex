\section{Linear Discriminant Analysis}

We seek to obtain a transformation of $X$ to $Y$ through projecting the samples in X onto a hyperplane.
\begin{itemize}
  \item Data matrix $\mathbf X=\bigl[\mathbf x_1,\dots,\mathbf x_N\bigr]^\top\in\mathbb R^{N\times m}$ with rowâ€‘vectors $\mathbf x_i$.
  \item Class labels $i\in\{1,\dots,C\}$, and each class has $N_i$ samples. 
  \item Each sample is belong to class $\omega_i$.
\end{itemize}
Of all the possible lines we would like to select the one that maximizes the separability. In order to find a good projection vector, we need to define a measure of separation between the projections.

The mean vector of each class in $\rvx$ and $\rvy$ is given by
\begin{align*}
	\mu_i &= \frac{1}{N}\sum_{\rvx\in \omega_i} \rvx\\
	\tilde{\mu}_i &= \frac{1}{N}\sum_{\rvy\in \omega_i} \rvy = \frac{1}{N}\sum_{\rvy\in \omega_i} \rvw^T\rvx = \rvw^T\frac{1}{N}\sum_{\rvy\in \omega_i} \rvx = \rvw^T\mu_i
\end{align*}
This indicates that projecting leads to projecting the mean of $\rvx$ to the mean of $\rvy$. Then, we could set the distance between the projected means as our objective function as follows:

\begin{align*}
	J(\rvw) = |\tilde{\mu}_1 - \tilde{\mu}_2| = |\rvw^T(\mu_1 - \mu_2)|.
\end{align*}
However, maximizing the distance between the projected means is not enough to yield better class separability. Thus, we will introduce the within-class variability, so-called \textit{scatter} matrix:
\begin{align*}
	S_i &= \sum_{\rvx\in \omega_i} (\rvx-\mu_i)(\rvx-\mu_i)^T\\
	S_W &= S_1+S_2
\end{align*}
Where $S_i$ is the covariance matrix of class $\omega_i$, and $S_\omega$ is called the \textit{within-class scatter matrix}.

Now the scatter of the projection $\rvy$ can then be expressed as a function of the scatter matrix in feature space $\rvx$.
\begin{align*}
	\tilde{s}_i^2 = \sum_{y\in \omega_i}(y-\mu_i)^2 &= \sum_{\rvx\in\omega_i}(w^T\rvx - w^T\mu_i)^2\\
													&= \sum_{\rvx\in\omega_i}\left(w^T(\rvx - \mu_i)\right)^2\\
													&= \sum_{\rvx\in\omega_i}\underbrace{\left(w^T(\rvx - \mu_i)\right)}_{\text scalar}\underbrace{\left(w^T(\rvx - \mu_i)\right)}_{\text scalar^T=scalar}\\
													&= \sum_{\rvx\in\omega_i}w^T(\rvx - \mu_i)(\rvx - \mu_i)^Tw\\
													&= w^TS_iw.
\end{align*}
Thus, 
\begin{align*}
	\tilde{s}_1^2+\tilde{s}_2^2 = w^TS_1w+w^TS_2w = w^T(S_1+S_2)w = w^TS_Ww = \tilde{S}_W.
\end{align*}
Similarly, the difference between the projected means can be expressed as follows:
\begin{align*}
	(\tilde{\mu}_1-\tilde{\mu}_2)^2 &= (w^T\mu_1-w^T\mu_2)\\
									&= w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw\\
									&= w^TS_Bw = \tilde{S}_B
\end{align*}
This matrix is called the \textit{between-class scatter}. Since $S_B$ is the outer product of two vectors, its rank is at most one.

For a projection vector $\mathbf w\in\mathbb R^{d}$ (with $\mathbf w\neq\mathbf 0$) define the
\emph{Fisher ratio}
\[
	J(\mathbf w) = \frac{|\tilde{\mu}_1-\tilde{\mu}_2|^2}{\tilde{s}_1^2+\tilde{s}_2^2} = \frac{\mathbf w^{\top}S_B\mathbf w}{\mathbf w^{\top}S_W\mathbf w}.
\]
The optimal $w$ satisfies that $|\tilde{\mu}_1-\tilde{\mu}_2|$ (inter-class spread) is big and $\tilde{s}_1^2+\tilde{s}_2^2$ (intra-class spread) is small. Note that this objective function is called \textit{Rayleigh quotient}.

Maximising the objective $J$ with respect to $\rvw$ is equivalent to maximising the numerator while holding the denominator constant as both the numerator and denominator have the same order of $\|\rvw\|$ and thus $J$ is invariant to scale changes in $\|\rvw\|$. Further, as we are only interested in the direction of $\rvw$, we can hold the denominator constant to 1. This results in a Lagrangian:
\begin{align*}
	\mathcal L(\mathbf w,\lambda)=\mathbf w^{\top}S_B\mathbf w-\lambda(\mathbf w^{\top}S_W\mathbf w-1).
\end{align*}
\begin{itemize}
	\item If you scale $\mathbf w$ by any nonzero scalar $c$, then numerator: $(c\mathbf w)^\top S_B (c\mathbf w)=c^2\,\mathbf w^\top S_B \mathbf w$ denominator: $(c\mathbf w)^\top S_W (c\mathbf w)=c^2\,\mathbf w^\top S_W \mathbf w$. The factor $c^2$ cancels in the ratio, so $J(c\mathbf w)=J(\mathbf w)$. Therefore, only the direction of $\mathbf w$ matters, not its length. That's what invariant to scale changes in $\|\mathbf w\|$ means.
	\item Because the ratio is scale-invariant, we can arbitrarily fix the scale of $\rvw$. The common choice is: $\mathbf w^{\top}S_W\mathbf w-1$.
\end{itemize}

Then, $\partial\mathcal L/\partial\mathbf w=\mathbf 0$ gives
\begin{align*}
	S_B\mathbf w&=\lambda S_W\mathbf w.\\
	S_W^{-1}S_B\mathbf w&=\lambda \mathbf w.
\end{align*}
By using the definition of $S_B$, 
\begin{align*}
	S_W^{-1}S_B\rvw = S_W^{-1}(\tilde{\mu}_1-\tilde{\mu}_2)(\tilde{\mu}_1-\tilde{\mu}_2)^T\rvw = \lambda \rvw.
\end{align*}
Let's set $(\tilde{\mu}_1-\tilde{\mu}_2)^T\rvw = \alpha$ and $\alpha$ is a scalar. Then, we get
\begin{align*}
	S_W^{-1}(\tilde{\mu}_1-\tilde{\mu}_2) = \frac{\lambda \rvw}{\alpha}
\end{align*}
Since we are only interested in the direction of $\rvw$, we can discard the scalar term, then
\begin{align*}
	\rvw^* = S_W^{-1}(\tilde{\mu}_1-\tilde{\mu}_2).
\end{align*}


