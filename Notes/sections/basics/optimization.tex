\chapter{Optimization}
\label{ch:optimization}

\section{Intuition of Gradient}

Gradient is a vector function that represents a direction of steepest increase of a function to be differentiated at a certain point. For example, a convex function $z = ax^2+by^2$ has a gradient $[2ax, 2by]$. Its steepest descent direction is $[-2ax, -2by]$. At the point $x_0,y_0$, the gradient direction for the function is $2ax_0(y-y_0) = 2by_0(x-x_0)$. But then the lowest point $(x,y) = (0,0)$ does not lie on the line. Thus, we cannot find that minimum point in one step of ``gradient descent''. The steepeset direction does not lead to the bottom of the bowl. 

\subsection{Direction of Gradient Descent}

% Most deep learning algorithms involve \textbf{optimization}.

\begin{itemize}
	\item The derivative of the objective function $f(\mathbf{x})$ provides the slope of $f(\mathbf{x})$ at the point $f(\mathbf{x})$.
	\item It tells us how to change $\mathbf{x}$ in order to make a small improvement in our goal.
\end{itemize}

	A function $f(\mathbf{x})$ can be approximated by its first-order Taylor expansion at $\bar{\mathbf{x}}$:
	$$f(\mathbf{x})\approx f(\bar{\mathbf{x}})+\nabla f(\bar{\mathbf{x}})^T(x-\bar{\mathbf{x}})$$
	Now let $\mathbf{d}\neq0, \|\mathbf{d}\|=1$ be a direction, and in consideration of a new point $\mathbf{x}:=\bar{\mathbf{x}}+\mathbf{d}$, we define:
	$$f(\bar{\mathbf{x}}+\mathbf{d})\approx f(\bar{\mathbf{x}})+\nabla f(\bar{\mathbf{x}})^T\mathbf{d}$$

We would like to choose $\mathbf{d}$ that minimizes the function $f$. From the Cauchy-Schwarz inequality \footnote{Cauchy-Schwarz Inequaility: $|\mathbf{a}\cdot \mathbf{b}|\leq \|\mathbf{a}\|\textrm{ } \|\mathbf{b}\|$. Equality holds if and only if either $\mathbf{a}$ or $\mathbf{b}$ is a multiple of the other.}, we know that
$$|\nabla f(\bar{\mathbf{x}})^T\mathbf{d}|\leq \|\nabla f(\bar{\mathbf{x}})\|\textrm{ }\|\mathbf{d}\|.$$
The equality holds if and only if $\mathbf{d}=\lambda \nabla f(\bar{\mathbf{x}})$, where $\lambda\in \mathbb{R}$. Since we want to minimize the function $f$, we negate the steepest direction $\mathbf{d}^{*}$ so the iterations of steepest descent becomes
	$$\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \eta \nabla f(\mathbf{x}^{(k)})$$
	, where $k$ is the index of iteration step and $\eta$ is the learning rate parameter.



%	$\nabla f(\bar{\mathbf{x}})^T\mathbf{d}/\|\mathbf{d}\|\geq \|\nabla f(\bar{\mathbf{x}})\|=\nabla f(\bar{\mathbf{x}})^T\Bigg(\frac{\nabla f(\bar{\mathbf{x}})}{\|\nabla f(\bar{\mathbf{x}})\|}\Bigg)$

\section{Normalized Gradient Descent}

The underlying issue of the vanila gradient descent is the presence of saddle points in nonconvex functions; the gradient $\nabla f(x)$ vanishes near saddle points, which causes GD to ``stall'' in neighboring regions. This both slows the overall convergence rate and makes detection of local minima difficult. The detrimental effects of this issue become particularly severe in high-dimensional problems where the number of saddle points may proliferate.


However, in the normalized gradient descent
$$ \frac{\nabla f(x)}{\|\nabla f(x)\|}$$
The normalized gradient preserves the direction of the gradient but ignores magnitude, because the normalization does not vanish near saddle points, the intuitive expectation is that NGD should not slow down in the neighborhood of saddle points and should therefore escape quickly. 

\section{Projected Gradient Descent}
Gradient Descent (GD) is a standard way to solve unconstrained optimization problem. Starting from an initial point $x\in \mathbb{R}^n$, GD itereates until a stopping criterion is met. Projected Gradient Descent (PGD) is a way to solve constrained optimization problem. Consider a constraint set $\mathcal{Q}$, starting from a initial point $x_0 \in \mathcal{Q}$, PGD iterates the following equation until a stopping condition is met:
$$x_{k+1} = P_\mathcal{Q} \Big(x_k - t_k \nabla f(x_k)\Big),$$
where $P_\mathcal{Q}$ is the projection operator
$$P_\mathcal{Q}(x_0) = \argmin_{x\in \mathcal{Q}} \frac{1}{2}\|x-x_0\|^2_2$$
In other words, given a point $x_0$, $P_\mathcal{Q}$ tries to to find a point $x\in \mathcal{Q}$ which is ``closest''to $x_0$.


Note that a vector projection can be expressed as follows:
$$a_1 = \|\mathbf{a}\|\cos \theta = \mathbf{a}\cdot \hat{\mathbf{b}} = \mathbf{a}\cdot \frac{\mathbf{b}}{\|\mathbf{b}\|}$$
Thus, a projection for unit $L_2$ ball is given by the solution of the equation as follows:
$$\mathbf{x} = \mathcal{P}_{\|x\|_2\leq 1}(\mathbf{y})$$
The solution is
$$\mathbf{x} = \frac{\mathbf{y}}{\max \{1,\|\mathbf{y}\|_2\}}$$
The ``geometric'' proof is given as follows: Let $\mathcal{S} = \{\mathbf{x}\in \mathbb{R}^n: \|\mathbf{x}\|_2 \leq 1\}.$
\begin{itemize}
		\item If $\mathbf{y}\in \mathcal{S}$, then $\|y\|_2\leq 1$ and $\mathbf{y}$ itself is the closest point to $\mathbf{y}$.
		\item If $\mathbf{y}\notin \mathcal{S}$, then $\|y\|_2> 1$ and the closest point $\mathbf{x}\in \mathcal{S}$ to $\mathbf{y}$ will be simply $\frac{\mathbf{y}}{\|\mathbf{y}\|_2}$ as the norm of $\frac{\mathbf{y}}{\|\mathbf{y}\|_2}=1$.
\end{itemize}
By combining the bost cases, we have
$$\mathbf{x} = \frac{\mathbf{y}}{\max \{1,\|\mathbf{y}\|_2\}}$$


\section{Exponentially Weighted Average}
\begin{align*}
    v_t = \beta v_{t-1} + (1-\beta) \theta_t
\end{align*}
Larger $\beta$ value covers more longer history. EMA is exponentially weighted average the previous result. 

\section{Bias Correction}
The initial values of $v_t$ will be very low which need to be compensated, since the curve starts from 0, there are not many values to average on in the initial points. Thus, the curve is lower than the correct value initially and then moves in line with expected values. The $\beta$ is the same as the averaging coefficient. As $t$ becomes large, the impact of the bias correction will be decreased. 
\begin{align*}
    v_t = \frac{v_t}{1-\beta^t}
\end{align*}


\section{Momentum}
Momentum can reduce the oscillation in the gradients. Let's say $w$ has a small value and $b$ is in charge of oscillation. Then momentum can cancel out $db$ by averaging them. 

\begin{align*}
    &v_{dw} = \beta v_{dw} + (1-\beta)dw \\
    &v_{db} = \beta v_{db} + (1-\beta)db \\
    & w = w-\alpha v_{dw}\\
    & b = b-\alpha v_{db}\\
\end{align*}

\section{Adagrad: Adaptive Gradient}
\begin{align*}
    &v_{dw} = v_{dw} + dw \cdot dw \\
	& w = w-\frac{\alpha}{\sqrt{v_{dw}}+\epsilon} v_{dw}\\
\end{align*}
A con of Adagrad is learning rate will become very small


\section{RMS Prop}

\begin{align*}
    &s_{dw} = \beta s_{dw} + (1-\beta)dw^2 \\
    &s_{db} = \beta s_{db} + (1-\beta)db^2 \\
	& w = w-\alpha  \frac{dw}{\sqrt{s_{dw}}}\\
	& b = b-\alpha \frac{db}{\sqrt{s_{db}}} \\
\end{align*}

\section{ADAM}
Its name is derived from adaptive moment estimation, and the reason it's called that is because Adam uses estimations of first and second moments of gradient to adapt the learning rate for each weight of the neural network. $N$-th moment of a random variable is defined as the expected value of that variable to the power of $n$. More formally:
$$m_n = \mathbb{E}[X^n]$$

To estimates the moments, Adam utilizes exponentially moving averages, computed on the gradient evaluated on a current mini-batch:

Since $m$ and $v$ are estimates of first and second moments, we want to have the following property:
\begin{align}
	\mathbb{E}[m_t] &= \mathbb{E}[g_t]\\
	\mathbb{E}[v_t] &= \mathbb{E}[g_t^2]
\end{align}
Unbiased estimators

ADAM uses both momentum style and RMS prop style averaging. 
\begin{itemize}
	\item $v_{dw} = \beta v_{dw} + (1-\beta)dw$
	\item $v_{db} = \beta v_{db} + (1-\beta)db$
	\item $s_{dw} = \beta s_{dw} + (1-\beta)dw^2 $
	\item $s_{db} = \beta s_{db} + (1-\beta)db^2 $
\end{itemize}

Using them,
\begin{itemize}
	\item $ v^{\textrm{corr}}_{dw} = \frac{v_{dw}}{1-\beta_1^t}$
	\item $ v^{\textrm{corr}}_{db} = \frac{v_{db}}{1-\beta_1^t}$
	\item $ s^{\textrm{corr}}_{dw} = \frac{s_{dw}}{1-\beta_2^t}$
	\item $ s^{\textrm{corr}}_{db} = \frac{s_{db}}{1-\beta_2^t}$
\end{itemize}

Finally, 
\begin{align*}
	& w = w-\alpha  \frac{v^{\textrm{corr}}_{dw}}{\sqrt{s^{\textrm{corr}}_{dw}}+\varepsilon}\\
	& b = b-\alpha  \frac{v^{\textrm{corr}}_{db}}{\sqrt{s^{\textrm{corr}}_{db}}+\varepsilon}\\
\end{align*}




