% \chapter{Regressions}
\section{Introduction}
\label{sec:regression}

Regression is a process for finding the relationship between the inputs and the outputs. In a regression problem, we consider a set of noisy measurement (or noisy output data)  $\rvy = [y_1, \dots, y_d]^T$ with measurement noise $\boldsymbol{\eta} = [\eta_1, \dots, \eta_d]^T$. We also consider a set of input data $\rvx = [x_1, \dots, x_d]$. We call the set of these input-output pairs $\mathcal{D} = \{(\rvx_1, \rvy_1), \dots, (\rvx_m, \rvy_m)\}$ the training data. The true relationship between the input and the output data is unknown. We denote this unknown relationship as a mapping $f(\cdot)$ that takes $\rvx_n$ and maps it to $y_n$, 
\begin{align*}
	\rvy = f(\rvx).
\end{align*}
Finding the true $f(\cdot)$ from a finite number of data points $\mathbcal{D}$ is infeasible. There are infinitely many ways to design $f(\cdot)$ for every $\rvx_i$. The idea of regression is to add a structure to the problem. Instead of looking for the true $f(\cdot)$, we find a proxy $g_\theta(\cdot)$ that takes a certain parameters $\boldsymbol{\theta} = [\theta_1,\dots,\theta_d]^T$. For instance, we can postulate that $(\rvx_n,\rvy_n)$ has a linear relationship:
$$g_{\boldsymbol{\theta}}(\rvy) = \mathbf{X}\boldsymbol{\theta}+\boldsymbol{\eta},$$
where $\mathbf{X}$ is a $m\times d$ input matrix (or our observations). Since we do not know the true relationship, any choice of model is our conjecture. However, we can model the error of our choice. Given a parameter $\boldsymbol{\theta}$, we consider the difference between the noisy measurements and estimated value as follows:
$$\boldsymbol{\epsilon} = \rvy - \mathbf{X}\boldsymbol{\theta}$$
The purpose of regression is to find the best $\boldsymbol{\theta}$ such that the error is minimized. Therefore, we can consider a following objective function:
$$J(\boldsymbol{\theta}) = \boldsymbol{\epsilon}^T\boldsymbol{\epsilon}$$
Note that this is equivalent to minimizing the mean squared error:
$$MSE = \frac{1}{n}\sum_{i=1}^n (y_i-\rvx_i\boldsymbol{\theta})^2.$$
% The OLEs' solution can be optimized by a closed form as follows:
% $$f(\rvx) = \mathbf{X}\boldsymbol{\theta},$$
% where $\rvx = [x_1, \dots, x_d]$ and $\boldsymbol{\theta} = [\theta_1,\dots,\theta_d]^T$. The ridge regression for $\mathbf{X}\in \mathbb{R}^{n\times d}$ matrix can be modeled as follows:
We can optimize this in a closed-form as follows:
\begin{align*}
	J(\boldsymbol{\theta}) &= \|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2 \\
			&= (\mathbf{y}-\mathbf{X}\boldsymbol{\theta})^T(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})\\
			&= (\mathbf{y}^T-\boldsymbol{\theta}^T\mathbf{X}^T)(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})\\
			&= \rvy^T\rvy-\boldsymbol{\theta}^T\mathbf{X}^T\rvy-\rvy^T\mathbf{X}\boldsymbol{\theta}+\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}
\end{align*}
To find the $\boldsymbol{\theta}$ that minimizes the objective function, we will compute a derivative of the function while setting it equal to zero:
\begin{align*}
	\frac{\partial J}{\partial \boldsymbol{\theta}}&= -\mathbf{X}^T\rvy-\mathbf{X}^T\rvy+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta} = 0\\
	&\Rightarrow \mathbf{X}^T(\mathbf{X}\boldsymbol{\theta}-\rvy)= 0\\
	\boldsymbol{\theta}	&= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\rvy
\end{align*}
Note that the $\mathbf{X}^T(\mathbf{X}\boldsymbol{\theta}-\rvy)$ is called the \textit{normal equation}.

\begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt

N = 50
x = np.random.randn(N)
w_1 = 3.4 # True Parameter
w_0 = 0.9 # True Parameter
y = w_1*x + w_0 + 0.3*np.random.randn(N) # Synthesize training data

X = np.column_stack((x, np.ones(N)))
W = np.array([w_1, w_0])

# From Scratch
XtX    = np.dot(X.T, X)
XtXinvX = np.dot(np.linalg.inv(XtX), X.T) # d x m
W_best = np.dot(XtXinvX, y.T)
print(f"W_best: {W_best}") 

# Pythonic Approach
theta = np.linalg.lstsq(X, y, rcond=None)[0]
print(f"Theta: {theta}") 

t = np.linspace(0, 1, 200)
y_pred = W_best[0]*t+W_best[1]
yhat = theta[0]*t+theta[1]
plt.plot(x, y, 'o')
plt.plot(t, y_pred, 'r', linewidth=4)
plt.show()
\end{lstlisting}

\subsection{MLE Interpretation}
\begin{align*}
	&\mathcal{L}(\boldsymbol{\theta}; \mathcal{D}) = \prod_{t=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left(-\frac{(y_t-\boldsymbol{\theta}\rvx_t)^2}{2\sigma^2}\right)\\
	&\log \mathcal{L}(\boldsymbol{\theta}; \mathcal{D}) = \text{const} - \frac{1}{2\sigma^2}\sum_{t=1}^n(y_t-\boldsymbol{\theta}\rvx_t)^2
\end{align*}

\subsection{Time Complexity}
The time complexity of the training process involves the following steps:
\begin{itemize}
	\item Computing \( X^T X \): The product \( X^T X \) involves multiplying a \( d \times m \) matrix with an \( m \times d \) matrix. The time complexity of this matrix multiplication is \( O(md^2) \).
	\item Computing \( X^T \mathbf{y} \): \( X^T \mathbf{y} \) involves multiplying a \( d \times m \) matrix with an \( m \times 1 \) vector.
	 - The time complexity is \( O(md) \).
	\item Computing \( \left( X^T X \right)^{-1} \):
 - The inversion of a \( d \times d \) matrix \( X^T X \) has a time complexity of \( O(d^3) \).
	\item Multiplying \( \left( X^T X \right)^{-1} \) with \( X^T \mathbf{y} \): This is a matrix-vector multiplication involving a \( d \times d \) matrix and a \( d \times 1 \) vector. Thus, the time complexity is \( O(d^2) \).
	\item Total Training Time Complexity: The dominant terms in the training process are \( O(md^2) \) (for computing \( X^\top X \)) and \( O(d^3) \) (for inverting \( X^T X \)).
- Therefore, the total time complexity for training a linear regression model is \( O(md^2 + d^3) \).
\end{itemize}

Inference time complexity:
\begin{itemize}
	\item The inference step requires a matrix-vector multiplication between an \( m' \times d \) inference data matrix and a \( d \times 1 \) vector.
- The time complexity of this operation is \( O(m'd) \).
\end{itemize}

\section{Overdetermined and Underdetermined Systems}
Recall that the linear regression problem is an optimization problem of finding the optimal parameter as follows:
$$\boldsymbol{\theta}_{opt} = \argmin_{\boldsymbol{\theta}\in \mathbb{R}^d}||y-\mathbf{X}\boldsymbol{\theta}||^2.$$
We say the optimization problem is \textit{overdetermined} if $\mathbf{X}\in \mathbb{R}^{m\times d}$ is tall and skinny, \ie $m>d$. This problem has a unique solution $\boldsymbol{\theta}	= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\rvy$ if and only if $\mathbf{X}^T\mathbf{X}$ is invertible. Equivalently, $\mathbf{X}$ should be linearly independent (\ie full rank). 

If $\mathbf{X}$ is fat and short (\ie $m<d$), a problem is called \textit{underdetermined}. \textbf{This problem will have infinitely many solutions}. Among all the feasible solutions, we will pick the one that minimizes the squared norm. The solution is called the \textit{minimum-norm} least squares. Consider an underdetermined linear regression problem:  
\begin{align*}
	\boldsymbol{\theta} = \argmin_{\boldsymbol{\theta}\in \mathbb{R}^d} \|\boldsymbol{\theta}\|^2, \,\textrm{subject to}\ \rvy = \mathbf{X}\boldsymbol{\theta},
\end{align*}
where $\mathbf{X}\in \mathbb{R}^{m\times d}, \boldsymbol{\theta}\in \mathbb{R}^d,$ and $\rvy\in \mathbb{R}^m$. If the matrix has rank$(\mathbf{X})=m$, then the linear regression problem will have a unique global minimum 
\begin{align*}
	\boldsymbol{\theta} = \mathbf{X}^T(\mathbf{X}\mathbf{X}^T)^{-1}\rvy.
\end{align*}
This solution is called the minimum-norm least-squares solution. The proof of this solution is given by:
\begin{align*}
	\mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\lambda}) = \|\boldsymbol{\theta}\|^2 + \boldsymbol{\lambda}^T(\mathbf{X}\boldsymbol{\theta}-\rvy),
\end{align*}
where $\boldsymbol{\lambda}$ is a Lagrange multiplier. The solution of the constrained optimization is the stationary point of the Lagrangian. To find it, we take the derivatives w.r.t., $\boldsymbol{\lambda}$ and $\boldsymbol{\theta}$ as follows: 
\begin{align*}
	\nabla_{\boldsymbol{\theta}} &= 2 \boldsymbol{\theta} + \mathbf{X}^T\boldsymbol{\lambda} = 0\\ 
	\nabla_{\boldsymbol{\lambda}} &= \mathbf{X}\boldsymbol{\theta} - \rvy = 0
\end{align*}
The first equation gives us $\boldsymbol{\theta} = -\mathbf{X}^T\boldsymbol{\lambda}/2$. Substituting it into the second equation, and assuming that rank$(\mathbf{X})=m$ so that $\mathbf{X}^T\mathbf{X}$ is invertible, we have $\boldsymbol{\lambda} = -2 (\mathbf{X}\mathbf{X}^T)^{-1}\rvy.$ Thus, we have
\begin{align*}
	\boldsymbol{\theta} = \mathbf{X}^T(\mathbf{X}\mathbf{X}^T)^{-1}\rvy.
\end{align*}
Note that $\mathbf{X}\mathbf{X}^T$ is often called a \textit{Gram matrix}, $\mathbf{G}$ (\cf \Cref{sec:kernel:kernel_trick}). 

\section{Overfitting}
We examine the relationship between the number of training samples and the complexity of the model.


\section{Ridge Regression}
Regularization means that instead of seeking the model parameters by minimizing the training loss alone, we add a penalty term to force the parameters to ``behave better''. 


With the ridge regression principle, we can optimize it as follows:
\begin{align}
	J(\boldsymbol{\theta}) &= \|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2 + \lambda \|\boldsymbol{\theta}\|^2_2 \\
			&= (\mathbf{y}-\mathbf{X}\boldsymbol{\theta})^T(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})+\lambda\boldsymbol{\theta}^T\boldsymbol{\theta}\\
			&= (\mathbf{y}^T-\boldsymbol{\theta}^T\mathbf{X}^T)(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})+\lambda\boldsymbol{\theta}^T\boldsymbol{\theta}\\
			&= \rvy^T\rvy-\boldsymbol{\theta}^T\mathbf{X}^T\rvy-\rvy^T\mathbf{X}\boldsymbol{\theta}+\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+\boldsymbol{\theta}^T\lambda\mathbf{I}\boldsymbol{\theta}\\
	\frac{\partial J}{\partial \boldsymbol{\theta}}&= -\mathbf{X}^T\rvy-\mathbf{X}^T\rvy+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+2\lambda\mathbf{I}\boldsymbol{\theta} = 0\\
	\boldsymbol{\theta}	&= (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\rvy
	\label{eq:ridge_regression}
\end{align}

\begin{itemize}
	\item If $\lambda\to 0$, then $\|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2 + \underbrace{\lambda \|\boldsymbol{\theta}\|^2_2}_{=0}$ 
	\item $\lambda\to \infty$, then $\underbrace{\frac{1}{\lambda}\|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2}_{=0} + \|\boldsymbol{\theta}\|^2_2$, since what we want to do is to minimize the objective function, we can divide it by $\lambda$. Therefore, the solution will be $\boldsymbol{\theta}=0$, because it is the smallest value the squared function can achieve. 
\end{itemize}
Note that $\mathbf{X}^T\mathbf{X}$ is always symmetric \footnote{$(\mathbf{X}^T\mathbf{X})^T = \mathbf{X}^T\mathbf{X}$.}. Thus, it can be decomposed as $Q\Lambda Q^T$ by the Spectral theorem. The $Q$ and $\Lambda$ are eigenvector and eigenvalue matrices, respectively. Then, the inverse operation in the ridge regression can be expressed as follows:
\begin{align*}
	\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I} &= \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T+\lambda\mathbf{I}\\
											 &= \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T+\lambda\mathbf{Q}\mathbf{Q}^T\\
											 &= \mathbf{Q}(\mathbf{\Lambda}+\lambda\mathbf{I})\mathbf{Q}^T.
\end{align*}
Even if the symmetric matrix is not invertible or close to not invertible, the regularization constant $\lambda$ makes it invertible (by making it to be a full-rank). 

Note that we can change the ridge regression into a dual form:
\begin{align}
	(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\boldsymbol{\theta}	&= (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\rvy\\
	(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\boldsymbol{\theta} &= \mathbf{X}^T\rvy\\ 
	\boldsymbol{\theta} &= \lambda^{-1}\mathbf{I}(\mathbf{X}^T\rvy-\mathbf{X}^T\mathbf{X}\boldsymbol{\theta})\\
						&= \mathbf{X}^T\underbrace{\lambda^{-1}(\rvy-\mathbf{X}\boldsymbol{\theta})}_{=\alpha}\\
	&= \mathbf{X}^T\alpha\\
	\lambda\alpha &= (\rvy-\mathbf{X}\boldsymbol{\theta})\\
	&= (\rvy-\mathbf{X}\mathbf{X}^T\alpha)\\
	\rvy &= (\mathbf{X}\mathbf{X}^T\alpha+\lambda\alpha)\\
	\alpha &= (\mathbf{X}\mathbf{X}^T+\lambda)^{-1}\rvy\\
	\alpha &= (\mathbf{G}+\lambda)^{-1}\rvy.
	\label{eq:dual_regression}
\end{align}
This gives us the solution of the underdetermined problems. For a new data point $\rvx_{new}$, we can make an inference by computing dot products between the new data point and each training data point:
\begin{align*}
	y &= \boldsymbol{\theta}^T\rvx_{new}\\
	  &= \alpha^T\mathbf{X}\rvx_{new}
\end{align*}

\subsection{Time Complexity}
\begin{itemize}
	\item Training time: $O(md^2+m^3)$
	\item Inference time: $O(md)$
\end{itemize}


\section{Weighted LSE}
The OLEs assume an equal confidence on all the measurements. Now we look at varying confidence in the measurements. We assume that the noise for each measurement has zero mean and is independent, then the covariance matrix for all measurement noise is given by
\begin{align*}
	R &= \mathbb{E}(\eta\eta^T)\\
	  &= \begin{bmatrix}
		  \sigma_1^2 & \dots & 0\\
		  \vdots & \ddots & \vdots\\
		  0 & \dots & \sigma_l^2
	  \end{bmatrix}
\end{align*}
By denoting the error vector $\rvy-\mathbf{X}\boldsymbol{\theta}$ as $\boldsymbol{\epsilon} = (\epsilon_1, \dots, \epsilon_l)^T$, we will minimize the sum of squared differences weighted over the variations of the measurements:
\begin{align*}
	J(\tilde{\rvx}) &= \boldsymbol{\epsilon}^TR^{-1}\boldsymbol{\epsilon}=\frac{\boldsymbol{\epsilon}_1^2}{\sigma_1^2}+\dots+\frac{\boldsymbol{\epsilon}_l^2}{\sigma_l^2}\\
					&= (\rvy-\mathbf{X}\boldsymbol{\theta})^TR^{-1}(\rvy-\mathbf{X}\boldsymbol{\theta})
\end{align*}
Note that by dividing each residual by its variance, we effectively equalize the influence of each data point on the overall fitting process. Subsequently, by taking the partial derivative of $J$ with respect to $\boldsymbol{\theta}$, we get the best estimate of the parameter, which is given by

$$\boldsymbol{\theta} = (\mathbf{X}^TR^{-1}\mathbf{X})^{-1}\mathbf{X}^TR^{-1}\rvy.$$
Note that the measurement noise matrix $R$ must be non-singular for a solution to exist.

\section{Robust Linear Regression}
\label{sec:robust_linear_regression}

The linear regression is based on the squared error criterion. This criterion often suffers from a serious drawback caused by outliers. By the definition of a squared error, our training loss is given by

\begin{align*}
	\mathcal{L}(\boldsymbol{\theta}) = \sum_{n=1}^N\bigg(y_n-g_{\boldsymbol{\theta}}(x_n)\bigg)^2.
\end{align*}
Let's assume that one of these error terms is large due to an outlier. 
\begin{align*}
	\mathcal{L}(\boldsymbol{\theta}) = \bigg(y_1-g_{\boldsymbol{\theta}}(x_1)\bigg)^2+\underbrace{\bigg(y_2-g_{\boldsymbol{\theta}}(x_2)\bigg)^2}_{\textrm{Large}}+\cdots
\end{align*}
If one or a few of these individual error terms are large, the square operation will amplify them.  Consequently, the outliers suddenly have a very large contribution to the error. Since the goal of linear regression is to minimize the total loss, the presence of the outliers will drive the optimization solution to compensate for the large error.



