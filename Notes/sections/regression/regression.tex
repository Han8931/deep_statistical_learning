\section{Introduction}
\label{sec:regression}
Suppose a noisy measurement $\rvy = [y_1, \dots, y_m]^T$ with noise $\boldsymbol{\eta} = [\eta_1, \dots, \eta_d]^T$ and we want to estimate parameter $\boldsymbol{\theta} = [\theta_1,\dots,\theta_d]^T$ by using our input $\rvx = [x_1, \dots, x_d]$. 

The measurement $\rvy$ can be modeled as follows:
$$\rvy = \mathbf{X}\boldsymbol{\theta}+\boldsymbol{\eta},$$
where $\mathbf{X}$ is a $m\times d$ input matrix (or our observations). Given a parameter $\boldsymbol{\theta}$, we consider the difference between the noisy measurements and estimated value as follows:
$$\boldsymbol{\epsilon} = \rvy - \mathbf{X}\boldsymbol{\theta}$$
Then, we can establish an objective function as follows:
$$J(\boldsymbol{\theta}) = \boldsymbol{\epsilon}^T\boldsymbol{\epsilon}$$
Note that this is equivalent to minimizing the mean squared error:
$$MSE = \frac{1}{n}\sum_{i=1}^n (y_i-\rvx_i\boldsymbol{\theta})^2.$$
% The OLEs' solution can be optimized by a closed form as follows:
% $$f(\rvx) = \mathbf{X}\boldsymbol{\theta},$$
% where $\rvx = [x_1, \dots, x_d]$ and $\boldsymbol{\theta} = [\theta_1,\dots,\theta_d]^T$. The ridge regression for $\mathbf{X}\in \mathbb{R}^{n\times d}$ matrix can be modeled as follows:
We can optimize this in a closed-form as follows:
\begin{align*}
	J(\boldsymbol{\theta}) &= \|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2 \\
			&= (\mathbf{y}-\mathbf{X}\boldsymbol{\theta})^T(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})\\
			&= (\mathbf{y}^T-\boldsymbol{\theta}^T\mathbf{X}^T)(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})\\
			&= \rvy^T\rvy-\boldsymbol{\theta}^T\mathbf{X}^T\rvy-\rvy^T\mathbf{X}\boldsymbol{\theta}+\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}
\end{align*}
To find the $\boldsymbol{\theta}$ that minimizes the objective function, we will compute a derivative of the function while setting it equal to zero:
\begin{align*}
	\frac{\partial J}{\partial \boldsymbol{\theta}}&= -\mathbf{X}^T\rvy-\mathbf{X}^T\rvy+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta} = 0\\
	&\Rightarrow \mathbf{X}^T(\mathbf{X}\boldsymbol{\theta}-\rvy)= 0\\
	\boldsymbol{\theta}	&= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\rvy
\end{align*}
Note that the $\mathbf{X}^T(\mathbf{X}\boldsymbol{\theta}-\rvy)$ is called the \textit{normal equation}.

\begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt

N = 50
x = np.random.randn(N)
w_1 = 3.4 # True Parameter
w_0 = 0.9 # True Parameter
y = w_1*x + w_0 + 0.3*np.random.randn(N) # Synthesize training data

X = np.column_stack((x, np.ones(N)))
W = np.array([w_1, w_0])

# From Scratch
XtX    = np.dot(X.T, X)
XtXinvX = np.dot(np.linalg.inv(XtX), X.T) # d x m
W_best = np.dot(XtXinvX, y.T)
print(f"W_best: {W_best}") 

# Pythonic Approach
theta = np.linalg.lstsq(X, y, rcond=None)[0]
print(f"Theta: {theta}") 

t = np.linspace(0, 1, 200)
y_pred = W_best[0]*t+W_best[1]
yhat = theta[0]*t+theta[1]
plt.plot(x, y, 'o')
plt.plot(t, y_pred, 'r', linewidth=4)
plt.show()
\end{lstlisting}

\section{Overdetermined and Underdetermined Systems}
Recall that the linear regression problem is an optimization problem of finding the optimal parameter as follows:
$$\boldsymbol{\theta}_{opt} = \argmin_{\boldsymbol{\theta}\in \mathbb{R}^d}||y-\mathbf{X}\boldsymbol{\theta}||^2.$$
We say the optimization problem is \textit{overdetermined} if $\mathbf{X}\in \mathbb{R}^{m\times d}$ is tall and skinny, \ie $m>d$. This problem has a unique solution $\boldsymbol{\theta}	= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\rvy$ if and only if $\mathbf{X}^T\mathbf{X}$ is invertible. Equivalently, $\mathbf{X}$ should be linearly independent (\ie full rank). 

If $\mathbf{X}$ is fat and short (\ie $N<d$), a problem is called \textit{underdetermined}. \textbf{This problem will have infinitely many solutions}. Among all the feasible solutions, we will pick the one that minimizes the squared norm. The solution is called the \textit{minimum-norm} least squares. Consider an underdetermined linear regression problem:  
\begin{align*}
	\boldsymbol{\theta} = \argmin_{\boldsymbol{\theta}\in \mathbb{R}^d} \|\boldsymbol{\theta}\|^2, \,\textrm{subject to}\ \rvy = \mathbf{X}\boldsymbol{\theta},
\end{align*}
where $\mathbf{X}\in \mathbb{R}^{m\times d}, \boldsymbol{\theta}\in \mathbb{R}^d,$ and $\rvy\in \mathbb{R}^m$. If rank$(\mathbf{X})=N$, then the linear regression problem will have a unique global minimum 
\begin{align*}
	\boldsymbol{\theta} = \mathbf{X}^T(\mathbf{X}\mathbf{X}^T)^{-1}\rvy.
\end{align*}
This solution is called the minimum-norm least-squares solution. The proof of this solution is given by:
\begin{align*}
	\mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\lambda}) = \|\boldsymbol{\theta}\|^2 + \boldsymbol{\lambda}^T(\mathbf{X}\boldsymbol{\theta}-\rvy),
\end{align*}
where $\boldsymbol{\lambda}$ is a Lagrange multiplier. The solution of the constrained optimization is the stationary point of the Lagrangian. To find it, we take the derivatives w.r.t., $\boldsymbol{\lambda}$ and $\boldsymbol{\theta}$ as follows: 
\begin{align*}
	\nabla_{\boldsymbol{\theta}} &= 2 \boldsymbol{\theta} + \mathbf{X}^T\boldsymbol{\lambda} = 0\\ 
	\nabla_{\boldsymbol{\lambda}} &= \mathbf{X}\boldsymbol{\theta} - \rvy = 0
\end{align*}
The first equation gives us $\boldsymbol{\theta} = -\mathbf{X}^T\boldsymbol{\lambda}/2$. Substituting it into the second equation, and assuming that rank$(\mathbf{X})=N$ so that $\mathbf{X}^T\mathbf{X}$ is invertible, we have $\boldsymbol{\lambda} = -2 (\mathbf{X}\mathbf{X}^T)^{-1}\rvy.$ Thus, we have
\begin{align*}
	\boldsymbol{\theta} = \mathbf{X}^T(\mathbf{X}\mathbf{X}^T)^{-1}\rvy.
\end{align*}
Note that $\mathbf{X}\mathbf{X}^T$ is often called a \textit{Gram matrix}, $\mathbf{G}$ (\cf \Cref{sec:kernel:kernel_trick}). 

\section{Overfitting}
We examine the relationship between the number of training samples and the complexity of the model.


\section{Ridge Regression}
Regularization means that instead of seeking the model parameters by minimizing the training loss alone, we add a penalty term to force the parameters to ``behave better''. 


With the ridge regression principle, we can optimize it as follows:
\begin{align}
	J(\boldsymbol{\theta}) &= \|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2 + \lambda \|\boldsymbol{\theta}\|^2_2 \\
			&= (\mathbf{y}-\mathbf{X}\boldsymbol{\theta})^T(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})+\lambda\boldsymbol{\theta}^T\boldsymbol{\theta}\\
			&= (\mathbf{y}^T-\boldsymbol{\theta}^T\mathbf{X}^T)(\mathbf{y}-\mathbf{X}\boldsymbol{\theta})+\lambda\boldsymbol{\theta}^T\boldsymbol{\theta}\\
			&= \rvy^T\rvy-\boldsymbol{\theta}^T\mathbf{X}^T\rvy-\rvy^T\mathbf{X}\boldsymbol{\theta}+\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+\boldsymbol{\theta}^T\lambda\mathbf{I}\boldsymbol{\theta}\\
	\frac{\partial J}{\partial \boldsymbol{\theta}}&= -\mathbf{X}^T\rvy-\mathbf{X}^T\rvy+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}+2\lambda\mathbf{I}\boldsymbol{\theta} = 0\\
	\boldsymbol{\theta}	&= (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\rvy
	\label{eq:ridge_regression}
\end{align}

\begin{itemize}
	\item If $\lambda\to 0$, then $\|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2 + \underbrace{\lambda \|\boldsymbol{\theta}\|^2_2}_{=0}$ 
	\item $\lambda\to \infty$, then $\underbrace{\frac{1}{\lambda}\|\mathbf{y}-\mathbf{X}\boldsymbol{\theta}\|^2_2}_{=0} + \|\boldsymbol{\theta}\|^2_2$, since what we want to do is to minimize the objective function, we can divide it by $\lambda$. Therefore, the solution will be $\boldsymbol{\theta}=0$, because it is the smallest value the squared function can achieve. 
\end{itemize}
Note that $\mathbf{X}^T\mathbf{X}$ is always symmetric \footnote{$(\mathbf{X}^T\mathbf{X})^T = \mathbf{X}^T\mathbf{X}$.}. Thus, it can be decomposed as $Q\Lambda Q^T$ by the Spectral theorem. The $Q$ and $\Lambda$ are eigenvector and eigenvalue matrices, respectively. Then, the inverse operation in the ridge regression can be expressed as follows:
\begin{align*}
	\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I} &= \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T+\lambda\mathbf{I}\\
											 &= \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T+\lambda\mathbf{Q}\mathbf{Q}^T\\
											 &= \mathbf{Q}(\mathbf{\Lambda}+\lambda\mathbf{I})\mathbf{Q}^T.
\end{align*}
Even if the symmetric matrix is not invertible or close to not invertible, the regularization constant $\lambda$ makes it invertible (by making it to be a full-rank). 

Note that we can change the ridge regression into a dual form:
\begin{align}
	(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\boldsymbol{\theta}	&= (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\rvy\\
	(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\boldsymbol{\theta} &= \mathbf{X}^T\rvy\\ 
	\boldsymbol{\theta} &= \lambda^{-1}\mathbf{I}(\mathbf{X}^T\rvy-\mathbf{X}^T\mathbf{X}\boldsymbol{\theta})\\
	&= \mathbf{X}^T\lambda^{-1}(\rvy-\mathbf{X}\boldsymbol{\theta})\\
	&= \mathbf{X}^T\alpha\\
	\lambda\alpha &= (\rvy-\mathbf{X}\boldsymbol{\theta})\\
	&= (\rvy-\mathbf{X}\mathbf{X}^T\alpha)\\
	\rvy &= (\mathbf{X}\mathbf{X}^T\alpha+\lambda\alpha)\\
	\alpha &= (\mathbf{X}\mathbf{X}^T+\lambda)^{-1}\rvy\\
	\alpha &= (\mathbf{G}+\lambda)^{-1}\rvy.
	\label{eq:dual_regression}
\end{align}
This gives us the solution of the underdetermined problems.  




\section{Weighted LSE}
The OLEs assume an equal confidence on all the measurements. Now we look at varying confidence in the measurements. We assume that the noise for each measurement has zero mean and is independent, then the covariance matrix for all measurement noise is given by
\begin{align*}
	R &= \mathbb{E}(\eta\eta^T)\\
	  &= \begin{bmatrix}
		  \sigma_1^2 & \dots & 0\\
		  \vdots & \ddots & \vdots\\
		  0 & \ddots & \sigma_l^2\\
	  \end{bmatrix}
\end{align*}
By denoting the error vector $\rvy-\mathbf{X}\boldsymbol{\theta}$ as $\boldsymbol{\epsilon} = (\epsilon_1, \dots, \epsilon_l)^T$, we will minimize the sum of squared differences weighted over the variations of the measurements:
$$J(\tilde{\rvx}) = \boldsymbol{\epsilon}^TR^{-1}\boldsymbol{\epsilon}=\frac{\boldsymbol{\epsilon}_1^2}{\sigma_1^2}+\dots+\frac{\boldsymbol{\epsilon}_l^2}{\sigma_l^2}$$
% This is equivalent to
% $$\frac{1}{n}\sum_{i=1}^{n}\sum_{i=1}^n \alpha_i (y_i-\rvx_i\boldsymbol{\theta}).$$

The best estimate of the parameter is given by
$$\boldsymbol{\theta} = (\mathbf{X}^TR^{-1}\mathbf{X})^{-1}\mathbf{X}^TR^{-1}\rvy.$$
Note that the measurement noise matrix $R$ must be non-singular for a solution to exist.

\section{Robust Linear Regression}
\label{sec:robust_linear_regression}

The linear regression is based on the squared error criterion. This criterion often suffers from a serious drawback caused by outliers. By the definition of a squared error, our training loss is given by

\begin{align*}
	\mathcal{L}(\boldsymbol{\theta}) = \sum_{n=1}^N\bigg(y_n-g_{\boldsymbol{\theta}}(x_n)\bigg)^2.
\end{align*}
Let's assume that one of these error terms is large due to an outlier. 
\begin{align*}
	\mathcal{L}(\boldsymbol{\theta}) = \bigg(y_1-g_{\boldsymbol{\theta}}(x_1)\bigg)^2+\underbrace{\bigg(y_2-g_{\boldsymbol{\theta}}(x_2)\bigg)^2}_{\textrm{Large}}+\cdots
\end{align*}
If one or a few of these individual error terms are large, the square operation will amplify them.  Consequently, the outliers suddenly have a very large contribution to the error. Since the goal of linear regression is to minimize the total loss, the presence of the outliers will drive the optimization solution to compensate for the large error.



