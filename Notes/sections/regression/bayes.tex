\section{Bayesian Regression}
\label{sec:regression_bayes}

Chain rule: $P(A,B|C)=P(A|B,C)P(B|C)$
\begin{align*}
	P(heads \mid D) &= \int_{\theta} P(heads, \theta \mid D) d\theta\\ 
					&= \int_{\theta} P(heads \mid \theta, D) P(\theta \mid D) d\theta \\ 
					&= \int_{\theta} \theta P(\theta \mid D) d\theta\\ 
					&= E\left[\theta|D\right]\\ 
					&= \frac{n_H + \alpha}{n_H + \alpha + n_T + \beta}
\end{align*}

\section{Bayesian Regression}

Bayesian regression is derived by combining the concepts of linear regression with Bayesian probability. In this note, we will derive Bayesian linear regression step-by-step, starting from a basic linear model and incorporating Bayesian inference.

\section{Linear Regression Model}

We assume a simple linear model:
\begin{align*}
	y = X\beta + \epsilon,
\end{align*}
where:
\begin{itemize}
    \item $ y $ is the vector of observed responses (dependent variable),
    \item $ X $ is the matrix of observed features (independent variables),
    \item $ \beta $ is the vector of regression coefficients,
    \item $ \epsilon \sim \mathcal{N}(0, \sigma^2 I) $ is the noise term (normally distributed with mean 0 and variance $ \sigma^2 $).
\end{itemize}

This gives the likelihood function:
\begin{align*}
	p(y | X, \beta, \sigma^2) = \mathcal{N}(y | X\beta, \sigma^2 I)
\end{align*}

In the Bayesian framework, we treat the regression coefficients $\beta$ as random variables with a prior distribution. Bayesian regression involves the following key elements:

\begin{itemize}
    \item \textbf{Prior:} $ p(\beta) $, which represents our belief about $\beta$ before observing the data.
    \item \textbf{Likelihood:} $ p(y | X, \beta) $, the likelihood of observing the data given the regression coefficients.
    \item \textbf{Posterior:} $ p(\beta | y, X) $, which updates the prior belief based on the observed data.
\end{itemize}

According to Bayesâ€™ theorem, the posterior is:
\begin{align*}
	p(\beta | y, X) = \frac{p(y | X, \beta) p(\beta)}{p(y | X)}
\end{align*}
The term $ p(y | X) $ is the marginal likelihood (evidence), which acts as a normalizing constant to ensure that the posterior is a valid probability distribution.

\subsection{Prior on $\beta$}

A common choice for the prior distribution on $\beta$ is a Gaussian distribution:
\begin{align*}
	p(\beta) = \mathcal{N}(\beta | \mu_0, \Sigma_0),
\end{align*}
where $\mu_0$ is the prior mean (our prior belief about $\beta$), and $\Sigma_0$ is the prior covariance matrix (representing the uncertainty in our prior belief).

\subsection{Posterior Distribution}
The posterior distribution of $\beta$ is obtained by combining the likelihood with the prior using Bayes' theorem. The likelihood and prior are given by
\begin{align*}
	p(y | X, \beta) &= \mathcal{N}(y | X\beta, \sigma^2 I)\\
	p(\beta) &= \mathcal{N}(\beta | \mu_0, \Sigma_0).
\end{align*}
Since the posterior is proportional to the product of the prior and the likelihood (and applying properties of Gaussian distributions), we obtain the posterior distribution and it is also Gaussian:
\begin{align*}
	p(\beta | y, X) = \mathcal{N}(\beta | \mu_n, \Sigma_n),
\end{align*}
where:
\begin{itemize}
    \item The \textbf{posterior mean} $\mu_n$ is given by:
		\begin{align*}
			\mu_n = \Sigma_n \left( \Sigma_0^{-1} \mu_0 + \frac{1}{\sigma^2} X^T y \right)
		\end{align*}
    \item The \textbf{posterior covariance} $\Sigma_n$ is given by:
		\begin{align*}
			\Sigma_n = \left( \Sigma_0^{-1} + \frac{1}{\sigma^2} X^T X \right)^{-1}
		\end{align*}
\end{itemize}
We can find the best $\beta$ by computing the highest posterior probability and it is called \textit{maximum a posterior} (MAP) approach. 



\subsection{Prediction}
Once we have the posterior distribution $ p(\beta | y, X) $, we can make predictions on new data $ X_*$.

The predictive distribution for the new output $ y_* $ given new input $ X_* $ is obtained by integrating over the posterior distribution of $\beta$:
\begin{align*}
	p(y_* | X_*, y, X) = \int p(y_* | X_*, \beta) p(\beta | y, X) d\beta
\end{align*}
Since both the likelihood and posterior are Gaussian, the predictive distribution is also Gaussian:
\begin{align*}
	p(y_* | X_*, y, X) = \mathcal{N}(y_* | X_*\mu_n, X_*\Sigma_n X_*^T + \sigma^2 I)
\end{align*}

In Bayesian regression, we incorporate prior beliefs about the regression coefficients through the prior distribution $ p(\beta) $, and update this belief after observing data to obtain the posterior distribution $ p(\beta | y, X) $. This method provides not only point estimates of the regression coefficients but also quantifies the uncertainty in these estimates through the posterior distribution.

