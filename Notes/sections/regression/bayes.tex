\section{Bayesian Regression}
\label{sec:regression_bayes}

Chain rule: $P(A,B|C)=P(A|B,C)P(B|C)$
\begin{align*}
	P(heads \mid D) &= \int_{\theta} P(heads, \theta \mid D) d\theta\\ 
					&= \int_{\theta} P(heads \mid \theta, D) P(\theta \mid D) d\theta \\ 
					&= \int_{\theta} \theta P(\theta \mid D) d\theta\\ 
					&= E\left[\theta|D\right]\\ 
					&= \frac{n_H + \alpha}{n_H + \alpha + n_T + \beta}
\end{align*}

\section{Bayesian Regression}

Bayesian regression is derived by combining the concepts of linear regression with Bayesian probability. In this note, we will derive Bayesian linear regression step-by-step, starting from a basic linear model and incorporating Bayesian inference.

\section{Linear Regression Model}

We assume a simple linear model:
\begin{align*}
	y = X\beta + \epsilon,
\end{align*}
where:
\begin{itemize}
    \item $ y $ is the vector of observed responses (dependent variable),
    \item $ X $ is the matrix of observed features (independent variables),
    \item $ \beta $ is the vector of regression coefficients,
    \item $ \epsilon \sim \mathcal{N}(0, \sigma^2 I) $ is the noise term (normally distributed with mean 0 and variance $ \sigma^2 $).
\end{itemize}

This gives the likelihood function:
\begin{align*}
	p(y | X, \beta, \sigma^2) = \mathcal{N}(y | X\beta, \sigma^2 I)
\end{align*}

In the Bayesian framework, we treat the regression coefficients $\beta$ as random variables with a prior distribution. Bayesian regression involves the following key elements:

\begin{itemize}
    \item \textbf{Prior:} $ p(\beta) $, which represents our belief about $\beta$ before observing the data.
    \item \textbf{Likelihood:} $ p(y | X, \beta) $, the likelihood of observing the data given the regression coefficients.
    \item \textbf{Posterior:} $ p(\beta | y, X) $, which updates the prior belief based on the observed data.
\end{itemize}

According to Bayesâ€™ theorem, the posterior is:
\begin{align*}
	p(\beta | y, X) = \frac{p(y | X, \beta) p(\beta)}{p(y | X)}
\end{align*}
The term $ p(y | X) $ is the marginal likelihood (evidence), which acts as a normalizing constant to ensure that the posterior is a valid probability distribution.

\subsection{Prior on $\beta$}

A common choice for the prior distribution on $\beta$ is a Gaussian distribution:
\begin{align*}
	p(\beta) = \mathcal{N}(\beta | \mu_0, \Sigma_0),
\end{align*}
where $\mu_0$ is the prior mean (our prior belief about $\beta$), and $\Sigma_0$ is the prior covariance matrix (representing the uncertainty in our prior belief).

\subsection{Posterior Distribution}
The posterior distribution of $\beta$ is obtained by combining the likelihood with the prior using Bayes' theorem.

The likelihood is:
\begin{align*}
	p(y | X, \beta) = \mathcal{N}(y | X\beta, \sigma^2 I)
\end{align*}
The prior is given by
\begin{align*}
	p(\beta) = \mathcal{N}(\beta | \mu_0, \Sigma_0)
\end{align*}
Multiplying the prior and the likelihood (and applying properties of Gaussian distributions), we obtain the posterior distribution, which is also Gaussian:
\begin{align*}
	p(\beta | y, X) = \mathcal{N}(\beta | \mu_n, \Sigma_n),
\end{align*}
where:
\begin{itemize}
    \item The \textbf{posterior mean} $\mu_n$ is given by:
		\begin{align*}
			\mu_n = \Sigma_n \left( \Sigma_0^{-1} \mu_0 + \frac{1}{\sigma^2} X^T y \right)
		\end{align*}
    \item The \textbf{posterior covariance} $\Sigma_n$ is given by:
		\begin{align*}
			\Sigma_n = \left( \Sigma_0^{-1} + \frac{1}{\sigma^2} X^T X \right)^{-1}
		\end{align*}
\end{itemize}

% \begin{align*}
% 	\log p(\beta| \mathcal{D}) &= \log p(\beta)+\log p(\mathcal{D}\mid \beta) + \text{const}\\ 
% 							   &= \dots\\
% 							   &= \dots\\
% \end{align*}


\subsection{Prediction}
Once we have the posterior distribution $ p(\beta | y, X) $, we can make predictions on new data $ X_*$.

The predictive distribution for the new output $ y_* $ given new input $ X_* $ is obtained by integrating over the posterior distribution of $\beta$:
\begin{align*}
	p(y_* | X_*, y, X) = \int p(y_* | X_*, \beta) p(\beta | y, X) d\beta
\end{align*}
Since both the likelihood and posterior are Gaussian, the predictive distribution is also Gaussian:
\begin{align*}
	p(y_* | X_*, y, X) = \mathcal{N}(y_* | X_*\mu_n, X_*\Sigma_n X_*^T + \sigma^2 I)
\end{align*}

In Bayesian regression, we incorporate prior beliefs about the regression coefficients through the prior distribution $ p(\beta) $, and update this belief after observing data to obtain the posterior distribution $ p(\beta | y, X) $. This method provides not only point estimates of the regression coefficients but also quantifies the uncertainty in these estimates through the posterior distribution.


% We use the model (no intercept for simplicity):

% $$
% y = X\beta + \epsilon,\quad \epsilon\sim\mathcal{N}(0,\sigma^2 I),
% $$

% with $\beta\in\mathbb{R}^2$. Choose:

% $$
% X=\begin{bmatrix}
% 1 & 1\\
% 1 & -1
% \end{bmatrix},\qquad
% y=\begin{bmatrix}2\\0\end{bmatrix},\qquad
% \sigma^2=1.
% $$

% Prior:

% $$
% \beta \sim \mathcal{N}(\mu_0,\Sigma_0),\quad
% \mu_0=\begin{bmatrix}0\\0\end{bmatrix},\quad
% \Sigma_0=I_2.
% $$

% ## Posterior

% General conjugate formulas:

% $$
% \Sigma_n=\Big(\Sigma_0^{-1}+\tfrac{1}{\sigma^2}X^\top X\Big)^{-1},\qquad
% \mu_n=\Sigma_n\Big(\Sigma_0^{-1}\mu_0+\tfrac{1}{\sigma^2}X^\top y\Big).
% $$

% Here,

% $$
% X^\top X
% =\begin{bmatrix}
% 1&1\\
% 1&-1
% \end{bmatrix}^\top
% \begin{bmatrix}
% 1&1\\
% 1&-1
% \end{bmatrix}
% =
% \begin{bmatrix}
% 2&0\\
% 0&2
% \end{bmatrix}
% =2I_2,
% $$

% $$
% X^\top y
% =
% \begin{bmatrix}
% 1&1\\
% 1&-1
% \end{bmatrix}^\top
% \begin{bmatrix}
% 2\\0
% \end{bmatrix}
% =
% \begin{bmatrix}
% 2\\2
% \end{bmatrix}.
% $$

% Since $\Sigma_0^{-1}=I_2$ and $\sigma^2=1$,

% $$
% \Sigma_n=\big(I_2+2I_2\big)^{-1}=(3I_2)^{-1}=\tfrac{1}{3}I_2,
% $$

% $$
% \mu_n=\Sigma_n\,(X^\top y)=\tfrac{1}{3}I_2\begin{bmatrix}2\\2\end{bmatrix}
% =\begin{bmatrix}\tfrac{2}{3}\\[2pt]\tfrac{2}{3}\end{bmatrix}.
% $$

% **Result:**

% $$
% \beta \mid X,y \sim \mathcal{N}\!\left(
% \begin{bmatrix}\tfrac{2}{3}\\[2pt]\tfrac{2}{3}\end{bmatrix},
% \;\tfrac{1}{3}I_2
% \right).
% $$

% ## Prediction at a new point

% Take a new input $x_*=\begin{bmatrix}1\\2\end{bmatrix}$ (two features). The predictive distribution is:

% $$
% y_* \mid x_*,X,y \sim \mathcal{N}\!\big(x_*^\top \mu_n,\; x_*^\top \Sigma_n x_* + \sigma^2\big).
% $$

% Compute mean:

% $$
% x_*^\top \mu_n
% =
% \begin{bmatrix}1&2\end{bmatrix}
% \begin{bmatrix}\tfrac{2}{3}\\[2pt]\tfrac{2}{3}\end{bmatrix}
% =
% \tfrac{2}{3}+ \tfrac{4}{3}
% =2.
% $$

% Compute variance:

% $$
% x_*^\top \Sigma_n x_* + \sigma^2
% =
% \begin{bmatrix}1&2\end{bmatrix}
% \Big(\tfrac{1}{3}I_2\Big)
% \begin{bmatrix}1\\2\end{bmatrix}
% + 1
% =
% \tfrac{1}{3}(1^2+2^2) + 1
% =
% \tfrac{5}{3}+1
% =
% \tfrac{8}{3}.
% $$

% **Predictive distribution:**

% $$
% y_* \sim \mathcal{N}\!\left(2,\;\tfrac{8}{3}\right).
% $$

% ---

% ## Why this is easy by hand

% * $X^\top X=2I_2$ keeps inverses trivial.
% * Prior and noise variances are 1.
% * All matrix multiplications are small and orthogonal.

% If you want an intercept too, set $X=\begin{bmatrix}1&1&1\\1&1&-1\end{bmatrix}$ and carry out the same steps with a 3D prior; the algebra is identical, just one dimension larger.

