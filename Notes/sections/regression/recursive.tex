\section{Recursive Least Squares}
\label{sec:recursive_least_square}

The ordinary least-squares assumes that all measurements are available at a certain time. However, this often might not be the case in practice. \textbf{More often, we obtain measurements sequentially and want to update our estimate with each new measurement.} In this case, the matrix $\mathbf{X}$ needs to be augmented. This update can be very expensive especially if the number of measurements is extremely large, the solutions of the least squares problem become prohibitive to compute. 

This motivates the Recursive Least Squares (RLS). Suppose we have an estimate $\boldsymbol{\theta}_{k-1}$ after $(k-1)$ measurements and obtain a new measurement $\rvy_k$. How can we update our estimate without completely reworking on the pseudo-inverse problem?

A linear recursive estimator can be expressed in the following form:
\begin{align*}
	\rvy_k &= \mathbf{X}_k\boldsymbol{\theta}+\boldsymbol{\eta}_k\\
	\boldsymbol{\theta}_k &= \boldsymbol{\theta}_{k-1}+K_k (\rvy_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1})
\end{align*}
Here, $\mathbf{X}_k$ is an $m\times d$ matrix (observations) and $K_k$ is $d\times m$ and referred to as the \textit{estimator gain matrix}. We refer to $(\rvy_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1})$ as the \textit{correction term}. Also, $\boldsymbol{\eta}_k$ is the measurement error. The new estimate is modified from the previous estimate $\boldsymbol{\theta}_{k-1}$ with a correction via the gain matrix. 

Intuitively, we can notice that we have to compute the optimal gain matrix to update our estimate. To this end, we have to set an \textit{estimation error}, which is our learning objective. The error can be expressed as follows: 
\begin{align*}
	\boldsymbol{\epsilon}_k	&= \boldsymbol{\theta}-\boldsymbol{\theta}_k \\
							&= \boldsymbol{\theta}-\boldsymbol{\theta}_{k-1} - K_k (\rvy_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1})\\
							&= \boldsymbol{\epsilon}_{k-1}-K_k (\mathbf{X}_k\boldsymbol{\theta}+\boldsymbol{\eta}_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1})\\
							&= \boldsymbol{\epsilon}_{k-1}-K_k \mathbf{X}_k(\boldsymbol{\theta}-\boldsymbol{\theta}_{k-1})-K_k\boldsymbol{\eta}_k\\
							&= (I-K_k \mathbf{X}_k)\boldsymbol{\epsilon}_{k-1}-K_k\boldsymbol{\eta}_k,
\end{align*}
where $I$ is the $d\times d$ identity matrix. The mean of this error is then
$$\mathbb{E}[\boldsymbol{\epsilon}_{k}] = (I-K_k \mathbf{X}_k)\mathbb{E}[\boldsymbol{\epsilon}_{k-1}]-K_k\mathbb{E}[\boldsymbol{\eta}_{k}]$$
If $\mathbb{E}[\boldsymbol{\eta}_{k}]=0$ and $\mathbb{E}[\boldsymbol{\epsilon}_{k-1}]=0$, then $\mathbb{E}[\boldsymbol{\epsilon}_{k}]=0$. So if the measurement noise has zero mean for all $k$, and the initial estimate of $\boldsymbol{\theta}$ is set equal to its expected value, then $\boldsymbol{\theta}_k=\boldsymbol{\theta}_k, \forall k$. This property tells us that the estimator $\boldsymbol{\theta}_k = \boldsymbol{\theta}_{k-1}+K_k (\rvy_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1})$ is \textit{unbiased}. This property holds regardless of the value of the gain vector $K_k$. This means the estimate will be equal to the true value $\boldsymbol{\theta}$ on average. 

The key is to \textbf{determine the optimal value of the gain vector} $K_k$. The optimality criterion is to \textbf{minimize the aggregated variance of the estimation errors at time} $k$: 
\begin{align*}
	J_k &= \mathbb{E}[\|\boldsymbol{\theta}-\boldsymbol{\theta}_k\|^2]\\
		&= \mathbb{E}[\boldsymbol{\epsilon}_{k}^T\boldsymbol{\epsilon}_{k}]\\
		&= \mathbb{E}[tr(\boldsymbol{\epsilon}_{k}\boldsymbol{\epsilon}_{k}^T)]\\
		&= tr(P_k),
\end{align*}
where $P_k=\mathbb{E}[\boldsymbol{\epsilon}_{k}\boldsymbol{\epsilon}_{k}^T]$ is \textit{the estimation-error covariance} (\ie \textbf{covariance matrix}). Note that the third line holds by the trace of a product (\ie \textit{cyclic property}) and the expectation in the third line can go into the trace operator by its linearity. Next, we can obtain $P_k$ by
\begin{align*}
	P_k &= \mathbb{E}\bigg[\big((I-K_k \mathbf{X}_k)\boldsymbol{\epsilon}_{k-1}-K_k\boldsymbol{\eta}_k\big)\big((I-K_k \mathbf{X}_k)\boldsymbol{\epsilon}_{k-1}-K_k\boldsymbol{\eta}_k\big)^T\bigg]
\end{align*}
By rearranging the above equation with the property that the mean of noise is zero, we can get
\begin{align}
	P_k = (I-K_k \mathbf{X}_k)P_{k-1}(I-K_k \mathbf{X}_k)^T+K_kR_kK_k^T,
	\label{eq:rls_estimation_cov}
\end{align}
where $R_k = \mathbb{E}[\boldsymbol{\eta}_k\boldsymbol{\eta}_k^T]$ as covariance of $\boldsymbol{\eta}_k$. This equation is the recurrence for the covariance of the least squares estimation error. It is consistent with the intuition that as the measurement noise $R_k$ increases, the uncertainty in our estimate also increases (\ie $P_k$ increases).  Note that $P_k$ should be positive definite since it is a covariance matrix.

Next, let's compute $K_k$ that minimizes the cost function given by the error equation. We are going to utilize the following property:
\begin{align*}
	\frac{\partial tr(CA^T)}{\partial A} &= C\\
	\frac{\partial tr(ACA^T)}{\partial A} &= AC + AC^T
\end{align*}
Next, we are going to take a derivative to the objective function:
\begin{align*}
	\frac{\partial J_k}{\partial K_k} &= \frac{\partial tr(P_k)}{\partial K_k} = \frac{\partial tr}{\partial K_k}\underbrace{(I-K_k \mathbf{X}_k)P_{k-1}(I-K_k \mathbf{X}_k)^T}_{=ACA^T}+\frac{\partial}{\partial K_k}tr\left(K_k R_k K_k^T\right)\\ 
									  &= \frac{\partial tr(ACA^T)}{\partial (I-K_k \mathbf{X}_k)}\frac{\partial (I-K_k \mathbf{X}_k)}{\partial K_k} +\frac{\partial}{\partial K_k}tr\left(K_k R_k K_k^T\right) \quad \text{by Chain Rule}\\
	&= \left((I-K_k \mathbf{X}_k)P_{k-1}+ (I-K_k \mathbf{X}_k)P_{k-1}^T\right)(-\mathbf{X}_k^T) + \frac{\partial}{\partial K_k}tr\left(K_k R_k K_k^T\right)\\
	&= 2(I-K_k \mathbf{X}_k)P_{k-1}(-\mathbf{X}_k^T) + \frac{\partial}{\partial K_k}tr\left(K_k R_k K_k^T\right)\quad \text{, since } P_{k-1} \text{ is symmetric.}\\
									  &= -2(I-K_k \mathbf{X}_k)P_{k-1}\mathbf{X}_k^T+2K_kR_k
\end{align*}
By setting the partial derivative to zero, we get
$$K_k = P_{k-1}\mathbf{X}_k^T(\mathbf{X}_kP_{k-1}\mathbf{X}_k^T+R_k)^{-1}.$$

\subsection{Alternative Form}
Sometimes it is useful to write the equations for $P_k$ and $K_k$ in alternate forms. Although these alternate forms are mathematically identical, they can be beneficial from a computational point of view. Let's first set $\mathbf{X}_kP_{k-1}\mathbf{X}_k^T+R_k = S_k$, then we get 
$$K_k = P_{k-1}\mathbf{X}_k^TS_k^{-1}.$$
By putting this into \Cref{eq:rls_estimation_cov},
\begin{align*}
	P_k &= (I-P_{k-1}\mathbf{X}_k^TS_k^{-1} \mathbf{X}_k)P_{k-1}(I-P_{k-1}\mathbf{X}_k^TS_k^{-1} \mathbf{X}_k)^T+P_{k-1}\mathbf{X}_k^TS_k^{-1} R_k S_k^{-1}\mathbf{X}_kP_{k-1}\\ 
		&\quad \vdots\\
		&= P_{k-1}-P_{k-1}\mathbf{X}_k^TS_k^{-1}\mathbf{X}_k^TP_{k-1}\\
		&= (I-K_k\mathbf{X}_k)P_{k-1}.
\end{align*}
Note that $P_k$ is symmetric (\cf $P_k=\boldsymbol{\epsilon}_{k}\boldsymbol{\epsilon}_{k}^T$), since it is a covariance matrix, and so is $S_k$.

We take the inverse of both sides of 
$$P_{k-1}^{-1} = \bigg(\underbrace{P_{k-1}}_{A}-\underbrace{P_{k-1}\mathbf{X}_k^T}_{B}\big(\underbrace{\mathbf{X}_kP_{k-1}\mathbf{X}_k^T}_{D}\big)^{-1}\underbrace{\mathbf{X}_kP_{k-1}}_{C}\bigg)^{-1}.$$
Next, we apply the matrix inversion lemma which is known as \textit{Sherman-Morrison-Woodbury matrix identity} (or \textit{matrix inversion lemma}) identity: 
$$(A-BD^{-1}C)^{-1} = A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1}.$$
Then, rewrite $P_k^{-1}$ as follows:
\begin{align*}
	P_k^{-1} &= P_{k-1}^{-1}+P_{k-1}^{-1}P_{k-1}\mathbf{X}_k^T\big((\mathbf{X}_kP_{k-1}\mathbf{X}_k^T+R_k)-\mathbf{X}_kP_{k-1}P_{k-1}^{-1}(P_{k-1}\mathbf{X}_k^T)\big)^{-1}\mathbf{X}_kP_{k-1}P_{k-1}^{-1}\\ 
			 &= P_{k-1}^{-1}+\mathbf{X}_k^TR_{k}^{-1}\mathbf{X}_k
\end{align*}
This yields an alternative expression for the covariance matrix:
\begin{align*}
	P_k = \big(P_{k-1}^{-1}+\mathbf{X}_k^TR_{k}^{-1}\mathbf{X}_k\big)^{-1}
\end{align*}
We can also obtain
\begin{align*}
	K_k = P_{k}\mathbf{X}_k^TR_{k}^{-1}
\end{align*}
By
\begin{align*}
	P_k &= (I-K_k\mathbf{X}_k)P_{k-1}\\
	P_kP_{k-1}^{-1} &= (I-K_k\mathbf{X}_k)\\
	P_kP_k^{-1} &= P_kP_{k-1}^{-1}+P_k\mathbf{X}_k^TR_{k}^{-1}\mathbf{X}_k=I\\
	I &= (I-K_k\mathbf{X}_k)+P_k\mathbf{X}_k^TR_{k}^{-1}\mathbf{X}_k\\
	K_k &= P_{k}\mathbf{X}_k^TR_{k}^{-1}.
\end{align*}

\subsection{Summary of RLS}
In sum, RLS can be updated as follows: 
\begin{itemize}
	\item Update the gain matrix: 
		\begin{itemize}
			\item $K_k = P_{k-1}\mathbf{X}_k^T(\mathbf{X}_kP_{k-1}\mathbf{X}_k^T+R_k)^{-1}$ or
			\item $K_k = P_{k}\mathbf{X}_k^TR_{k}^{-1}$
		\end{itemize}
	\item Update estimate: $\boldsymbol{\theta}_k = \boldsymbol{\theta}_{k-1}+K_k (\rvy_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1})$
	\item Update error covariance matrix by either: 
		\begin{itemize}
			\item $P_k = (I-K_k\mathbf{X}_k)P_{k-1}$.
			\item $P_k = (I-K_k \mathbf{X}_k)P_{k-1}(I-K_k \mathbf{X}_k)^T+K_kR_kK_k^T,$
		\end{itemize}
\end{itemize}

\paragraph{Example: }
At sample time $k$, our measurement is
\begin{itemize}
	\item $y_k = X_k\theta+\eta_k$
	\item $X_k = 1$
	\item $R_k = \mathbb{E}[\eta_k^2]$
\end{itemize}
For this scalar problem, the measurement matrix $X_k$ is a scalar too, and the measurement noise covariance $R_k$ is also a scalar. We will suppose that each measurement has the same covariance so the measurement covariance $R_k$ is not a function of $k$, and can be written as $R$. Initially, before we have any measurements, we have some idea about the value of the $\theta$, and this forms our initial estimate. We also have some uncertainty about our initial estimate, and this forms our initial covariance:
\begin{align*}
	\hat{\theta}_0 &= \mathbb{E}[\theta]\\
	P_0 &= \mathbb{E}[(\theta-\hat{\theta}_0)(\theta-\hat{\theta}_0)^T]\\
		&= \mathbb{E}[(\theta-\hat{\theta}_0)^2]
\end{align*}
If we have absolutely no idea about $\theta$, then $P(0)=\infty I$. If we are 100\% certain about the $\theta$ before taking any measurements, then $P(0)=0$. Let's compute the gain matrix at $k=1$ by using the following equation:
$$K_k = P_{k-1}\mathbf{X}_k^T(\mathbf{X}_kP_{k-1}\mathbf{X}_k^T+R_k)^{-1}.$$
Then, we get
$$K_1 = P_{0}(P_{0}+R)^{-1}.$$
Similarly, by
$$\boldsymbol{\theta}_k = \boldsymbol{\theta}_{k-1}+K_k (\rvy_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1}),$$
we obtain
$$\hat{\theta}_1 = \hat{\theta}_{0}+\frac{P_{0}}{P_{0}+R} (y_1-\hat{\theta}_{0}).$$
Finally, let's update our covariance matrix $P_k$ by 
$$P_k = (I-K_k \mathbf{X}_k)P_{k-1}(I-K_k \mathbf{X}_k)^T+K_kR_kK_k^T.$$
Then, 
\begin{align*}
	P_1 &= \left(I-\frac{P_{0}}{P_{0}+R}\right)P_{0}I-\frac{P_{0}}{P_{0}+R}+\frac{P_{0}}{P_{0}+R}R\frac{P_{0}}{P_{0}+R}\\
		&= \left(\frac{P_0R^2}{(P_{0}+R)^2}\right)+\frac{P_{0}^2R}{(P_{0}+R)^2}\\
		&= \frac{P_{0}R(P_0+R)}{(P_{0}+R)^2}\\
		&= \frac{P_{0}R}{P_{0}+R}
\end{align*}
By repeating these calculations, we can update the above parameters and find general expressions: 
\begin{align*}
	P_{k-1}&= \frac{P_0R}{(k-1)P_0+R}\\
	K_{k}&= \frac{P_0}{kP_0+R}\\
	\hat{\theta}_{k}&= \frac{(k-1)P_0+R}{kP_0+R}\hat{\theta}_{k-1}+\frac{P_0}{kP_0+R}y_k
\end{align*}
Note that if $\theta$ is known perfectly \textit{a priori} (\ie $\theta$ is known perfectly before any measurements are obtained) then $P_0 =0$ and the above equation show that $K_k=0$ and $\hat{\theta} = \hat{\theta}_0$. That is, the optimal estimate of $\theta$ is independent of any measurements that are obtained. In sum, this indicates that no update from measurements is needed, as the estimate is already perfect.

On the other hand, if $x$ is completely unknown a priori, then $P_0\to \infty$, and the above equations simplify to
\begin{align*}
\hat{\theta}_k &= \frac{(k-1)P_0}{kP_0} \hat{\theta}_{k-1} + \frac{P_0}{kP_0} y_k \\
         &= \frac{k-1}{k} \hat{\theta}_{k-1} + \frac{1}{k} y_k \\
         &= \frac{1}{k} [(k-1)\hat{\theta}_{k-1} + y_k]
\end{align*}
In other words, the optimal estimate of \(\theta\) is equal to the running average of the measurements \(y_k\), which can be written as
\begin{align*}
\bar{y}_k &= \frac{1}{k} \sum_{j=1}^k y_j \\
          &= \frac{1}{k} \left(\sum_{j=1}^{k-1} y_j + y_k\right) \\
          &= \frac{1}{k} \left[(k-1) \frac{1}{k-1} \sum_{j=1}^{k-1} y_j + y_k\right] \\
          &= \frac{1}{k} [(k-1) \bar{y}_{k-1} + y_k]
\end{align*}


\subsection{Curve Fitting} 
In the recursive curve fitting problem, we measure data one sample at a time $(y_1, y_2 \dots, )$ and want to find the best fit of a curve to the data. The curve that we want to fit to the data could be constrained to be linear or quadratic and so on. 

\paragraph{Example:} Suppose that we want to fit a straight line to a set of data points. The linear data fitting problem can be written as 
\begin{align*}
	y_k &= \theta_1+\theta_2t_k+\eta_k\\
	\mathbb{E}[\eta_k] &= R_k
\end{align*}
$t_k$ is the independent variable, $y_k$ is the noisy data, and we want to find the linear relationship between $y_k$ and $t_k$. In sum, we want to estimate the constants $\theta_1$ and $\theta_2$. The measurement matrix can be written as 
\begin{align*}
	\mathbf{X}_k = \begin{bmatrix}
		1 & t_k
	\end{bmatrix}.
\end{align*}
Then, 
$$\rvy_k = \mathbf{X}_k\boldsymbol{\theta}+\boldsymbol{\eta}_k.$$


\subsection{Python Implementation}

\begin{lstlisting}[language=Python]
class RecursiveLeastSquares(object):
    
    # theta0 - initial estimate used to initialize the estimator
    # P0 - initial estimation error covariance matrix
    # R  - covariance matrix of the measurement noise
    def __init__(self,theta0,P0,R)
        
        # initialize the values
        self.theta0=theta0
        self.P0=P0
        self.R=R
        
        # this variable is used to track the current time step k of the estimator 
        # after every time step arrives, this variables increases for one 
        # in this way, we can track the number of variblaes
        self.curr_step=0
                  
        # this list is used to store the estimates xk starting from the initial estimate 
        self.estimates=[]
        self.estimates.append(theta0)
         
        # this list is used to store the estimation error covariance matrices Pk
        self.est_error_cov=[]
        self.est_error_cov.append(P0)
        
        # this list is used to store the gain matrices Kk
        self.gainMatrices=[]
         
        # this list is used to store estimation error vectors
        self.errors=[]
    
     
    # this function takes the current measurement and the current measurement matrix X
    # and computes the estimation error covariance matrix, updates the estimate, 
    # computes the gain matrix, and the estimation error
    # it fills the lists self.estimates, self.est_error_cov, self.gainMatrices, and self.errors
    # it also increments the variable curr_step for 1
    
    # measurementValue (theta) - measurement obtained at the time instant k
    # X - measurement matrix at the time instant k
    
    def predict(self,measurementValue,X):
        import numpy as np
        
        # Compute the L matrix and its inverse 
        # K_k = P_{k-1}X_k^T(R_k+X_kP_{k-1}X_k^T)^{-1}
        Lmatrix=self.R+np.matmul(X,np.matmul(self.est_error_cov[self.curr_step],X.T))
        LmatrixInv=np.linalg.inv(Lmatrix)
        # Compute the gain matrix
        gainMatrix=np.matmul(self.est_error_cov[self.curr_step],np.matmul(X.T,LmatrixInv))

        # Compute the estimation error                    
        # \theta_k = \theta_{k-1}+K_k (y_k-X_k\theta_{k-1})
        error=measurementValue-np.matmul(X,self.estimates[self.curr_step])
        # Compute the estimate
        estimate=self.estimates[self.curr_step]+np.matmul(gainMatrix,error)
        
        # Propagate the estimation error covariance matrix
        # P_k = (I-K_k X_k)P_{k-1}(I-K_k X_k)^T+K_kR_kK_k^T
        ImKc=np.eye(np.size(self.theta0),np.size(self.theta0))-np.matmul(gainMatrix,X)
        error_cov=np.matmul(ImKc,self.est_error_cov[self.curr_step])
        
        # add computed elements to the list
        self.estimates.append(estimate)
        self.est_error_cov.append(error_cov)
        self.gainMatrices.append(gainMatrix)
        self.errors.append(error)
        
        # increment the current time step
        self.curr_step=self.curr_step+1
\end{lstlisting}


\section{Alternate Derivation of RLS}
Suppose the training samples arrive one by one in the following sequence $\rvx_1, \dots, \rvx_m, \rvx_{m+1}$, where $\rvx_{m+1}$ denotes the newly arrived sample vector. These samples can be projected onto the feature space by linear projection and expressed into a matrix $\mathbf{P}^T \in \mathbb{R}^{(d+1)\times (m+1)}$ as follows:
\begin{align*}
	\mathbf{P}^T = [\rvp(\rvx_1), \dots, \rvp(\rvx_{m+1})],
\end{align*}
where $\rvp(\cdot)\in \mathbb{R}^{d+1}$. Subsequently, let 
\begin{align*}
	\mathbf{R}_{m+1} &= \mathbf{P}^T \mathbf{P} + b\mathbf{I}\\
	\mathbf{Q}_{m+1} &= \mathbf{P}^T \mathbf{y}.
\end{align*}
By separating the covariance of the newly arrived sample \( p(\rvx_{m+1}) \) from the remaining stack, we can write:
\begin{align*}
	\mathbf{P}^T \mathbf{P} &= \sum_{i=1}^{m+1} \rvp(\rvx_i) \rvp(\rvx_i)^T\\
							&= \sum_{i=1}^m \rvp(\rvx_i) \rvp(\rvx_i)^T + \rvp(\rvx_{m+1}) \rvp(\rvx_{m+1})^T\\
							&= \mathbf{P}_m^T \mathbf{P}_m + \rvp(\rvx_{m+1}) \rvp(\rvx_{m+1})^T.
\end{align*}
Hence, 
\begin{align*}
	\mathbf{R}_{m+1} &= \mathbf{P}^T \mathbf{P} + b\mathbf{I}\\
					 &= \left(\mathbf{P}_m^T \mathbf{P}_m + \rvp(\rvx_{m+1}) \rvp(\rvx_{m+1})^T\right)+ b\mathbf{I}\\
					 &= \underbrace{\mathbf{P}_m^T \mathbf{P}_m + b\mathbf{I}}_{=\mathbf{R}_m} + \rvp(\rvx_{m+1})\rvp(\rvx_{m+1})^T\\
					 &= \mathbf{R}_m + \rvp(\rvx_{m+1}) \rvp(\rvx_{m+1})^T
\end{align*}
Similarly, 
\begin{align*}
	\mathbf{Q}_{m+1} &= \mathbf{Q}_m + \rvp(\rvx_{m+1}) y_{m+1}
\end{align*}
If the system is designed to forget the old training samples (\ie weighted averaging), 
\begin{align*}
	\mathbf{R}_{m+1} &= (1 - \lambda) \mathbf{R}_m + \lambda \rvp(\rvx_{m+1}) \rvp(\rvx_{m+1})^T,\\
	\mathbf{Q}_{m+1} &= (1 - \lambda) \mathbf{Q}_m + \lambda \rvp(\rvx_{m+1}) y_{m+1},
\end{align*}
where \( \lambda \in (0, 1) \) is often called a \textit{forgetting factor}.

Let \( \mathbf{A} = \mathbf{R}_m, \mathbf{B} = p(\rvx_{m+1}), \mathbf{C} = 1 \) (scalar), \( \mathbf{D} = p(\rvx_{m+1})^T = \mathbf{p}^T \), then based on the matrix inversion lemma (Woodbury, 1950; Sherman and Morrison, 1950),
\begin{align*}
	(\mathbf{A} + \mathbf{BCD})^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{B} (\mathbf{C}^{-1} + \mathbf{DA}^{-1} \mathbf{B})^{-1} \mathbf{DA}^{-1},
\end{align*}
we have
\begin{align*}
	\mathbf{R}_{m+1}^{-1} &= \left[ (1 - \lambda) \mathbf{R}_m + \lambda \mathbf{p}\rvp^T \right]^{-1}\\
						  &= \frac{1}{1 - \lambda} \mathbf{R}_m^{-1} - \frac{1}{1 - \lambda} \mathbf{R}_m^{-1}\lambda \mathbf{p} \left( \mathbf{I} + \mathbf{p}^T \frac{\lambda}{1-\lambda} \mathbf{R}_m^{-1} \mathbf{p} \right)^{-1} \mathbf{p}^T \frac{1}{1-\lambda} \mathbf{R}_m^{-1}\\
						  &= \frac{1}{1 - \lambda} \mathbf{R}_m^{-1} - \frac{1}{(1 - \lambda)^2} \mathbf{R}_m^{-1} \mathbf{pp}^T \mathbf{R}_m^{-1} \left( \frac{1}{\lambda} + \frac{1}{1-\lambda} \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p} \right)^{-1}.
\end{align*}
We can obtain
\begin{align*}
	\rvw_{m+1} = \left( \mathbf{P}^T \mathbf{P} + b\mathbf{I} \right)^{-1} \mathbf{P}^T \mathbf{y} = \mathbf{R}_{m+1}^{-1} \mathbf{Q}_{m+1},
\end{align*}
Substitute \(\mathbf{R}_{m+1}^{-1}\) and \(\mathbf{Q}_{m+1} = (1 - \lambda) \mathbf{Q}_m +  \lambda\mathbf{p}(\rvx_{m+1}) y_{m+1}\):
\begin{align*}
	\mathbf{w}_{m+1} &= \left[ \frac{1}{1 - \lambda} \mathbf{R}_m^{-1} - \frac{1}{(1 - \lambda)^2} \mathbf{R}_m^{-1} \mathbf{p}\mathbf{p}^T \mathbf{R}_m^{-1} \left( \frac{1}{\lambda} + \frac{1}{1 - \lambda} \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p} \right)^{-1} \right] \left[ (1 - \lambda) \mathbf{Q}_m + \lambda y_{m+1} \mathbf{p} \right]\\
					 &= \underbrace{\mathbf{R}_m^{-1} \mathbf{Q}_m}_{=w_m} + \frac{1}{1 - \lambda}\mathbf{R}_m^{-1} \mathbf{p}\mathbf{p}^T \mathbf{R}_m^{-1} \left( \frac{1}{\lambda} + \frac{1}{1 - \lambda} \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p} \right)^{-1} \mathbf{Q}_m + \frac{\lambda}{1 - \lambda} \mathbf{R}_m^{-1} \mathbf{p} y_{m+1} \\
					 &\quad - \frac{\lambda}{(1 - \lambda)^2}\mathbf{R}_m^{-1} \mathbf{p}\mathbf{p}^T \mathbf{R}_m^{-1} \left( \frac{1}{\lambda} + \frac{1}{1 - \lambda} \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p} \right)^{-1}\mathbf{p} y_{m+1}\\
\end{align*}
Let 
\begin{align*}
	A &= \left( \frac{1}{\lambda} + \frac{1}{1 - \lambda} \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p} \right)^{-1} \\
	  &= \frac{\lambda(1-\lambda)}{\lambda \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p}+(1-\lambda)},
\end{align*}
which is a constant. Then 
\begin{align*}
	w_{m+1} &= w_m - \frac{\mathbf{R}_m^{-1} \mathbf{p}}{(1-\lambda)^2} \cdot A \cdot \left((1-\lambda)\mathbf{p}^T w_m + \lambda \mathbf{p}^T \mathbf{R}_m^{-1}\mathbf{p} y_{m+1}\right)+ \frac{\lambda}{1 - \lambda} \mathbf{R}_m^{-1} \mathbf{p} y_{m+1}\\
			&= w_m - \frac{\lambda \mathbf{R}_m^{-1} \mathbf{p}}{(1-\lambda)} \cdot \frac{1}{\lambda \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p}+(1-\lambda)}\left((1-\lambda)\mathbf{p}^T w_m + \lambda \mathbf{p}^T \mathbf{R}_m^{-1}\mathbf{p} y_{m+1}\right) + \frac{\lambda}{1 - \lambda} \mathbf{R}_m^{-1} \mathbf{p} y_{m+1}\\
			&= w_m - \frac{\lambda \mathbf{R}_m^{-1} \mathbf{p}}{(1-\lambda)} \cdot \frac{1}{\lambda \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p}+(1-\lambda)}\left((1-\lambda)\mathbf{p}^T w_m + \lambda \mathbf{p}^T \mathbf{R}_m^{-1}\mathbf{p} y_{m+1}\right) \\ 
			&\quad +\frac{\lambda}{(1 - \lambda)}\cdot \frac{\lambda \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p}+(1-\lambda)}{\lambda \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p}+(1-\lambda)} \mathbf{R}_m^{-1} \mathbf{p} y_{m+1} \,\, \text{, since $\mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p}$ is a scalar, we can put $\mathbf{R}_m^{-1} \mathbf{p}$ to the right}\\
			&= w_m + \frac{\lambda}{\lambda \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p}+(1-\lambda)}\cdot\frac{1}{(1-\lambda)}\big(-(1-\lambda)\mathbf{R}_m^{-1}\mathbf{p}\mathbf{p}^T w_m - \lambda \mathbf{R}_m^{-1}\mathbf{p}\mathbf{p}^T\mathbf{R}_m^{-1}\mathbf{p}y_{m+1}\\
			&\quad + \lambda \mathbf{R}_m^{-1}\mathbf{p}\mathbf{p}^T\mathbf{R}_m^{-1}\mathbf{p}y_{m+1} + (1-\lambda)\mathbf{R}_m^{-1}\mathbf{p}y_{m+1}\big)\\
			&= w_m + \frac{\lambda}{\lambda \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p}+(1-\lambda)}\cdot\frac{1}{(1-\lambda)}\big(-(1-\lambda)\mathbf{R}_m^{-1}\mathbf{p}\mathbf{p}^T w_m + (1-\lambda)\mathbf{R}_m^{-1}\mathbf{p}y_{m+1}\big)
\end{align*}
Thus, the final recursive solution for the weight vector \(\mathbf{w}_{m+1}\) is given by
\[
\mathbf{w}_{m+1} = \mathbf{w}_m + \frac{\lambda \mathbf{R}_m^{-1} \mathbf{p} (y_{m+1} - \mathbf{p}^T \mathbf{w}_m)}{\lambda \mathbf{p}^T \mathbf{R}_m^{-1} \mathbf{p} + (1 - \lambda)}
\]
Here, we note that $\lambda$ controls the strength of update with respect to the accumulated solution with $\lambda \to 1$ having the strongest weight for newly arrived sample. When $\lambda = 0.5$ in , we have the following regularized recursive least squares solution:
\begin{align*}
	w_{m+1} = w_m + \frac{\mathbf{R}_m^{-1} \rvp (y_{m+1} - \rvp^T w_m)}{\rvp^T \mathbf{R}_m^{-1} \rvp + 1}. 
\end{align*}



