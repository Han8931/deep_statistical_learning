\section{Recursive Least Squares}
\label{sec:recursive_least_square}

The ordinary least-squares assumes that all measurements are available at a certain time. However, this often might not be the case in practice. More often, we obtain measurements sequentially and want to update our estimate with each new measurement. In this case, the matrix $H$ needs to be augmented. This update can be very expensive. When then number of measurements is extremely large, the solutions of the least squares problem are difficult to compute. 

These motivate the RLS. Suppose we have an estimate $\boldsymbol{\theta}_{k-1}$ after $k-1$ measurements, and obtain a new measurement $\rvy_k$. To be general, every measurements is now a vector with values yielded by, say, several measuring instruments. 

A linear recursive estimator can be written in the following form:
\begin{align*}
	\rvy_k &= \mathbf{X}_k\boldsymbol{\theta}+\boldsymbol{\eta}_k\\
	\boldsymbol{\theta}_k &= \boldsymbol{\theta}_{k-1}+K_k (\rvy_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1})
\end{align*}
Here $\mathbf{X}_k$ is an $m\times d$ matrix (observations), and $K_k$ is $d\times m$ and referred to as the \textit{estimator gain matrix}. We refer to $\rvy_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1}$ as the \textit{correction term}. Namely, the new estimate is modified from the previous estimate $\boldsymbol{\theta}_{k-1}$ with a correction via the gain vector. 

The current estimation error can be expressed as follows:
\begin{align*}
	\boldsymbol{\epsilon}_k	&= \boldsymbol{\theta}-\boldsymbol{\theta}_k \\
							&= \boldsymbol{\theta}-\boldsymbol{\theta}_{k-1} - K_k (\rvy_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1})\\
							&= \boldsymbol{\epsilon}_{k-1}-K_k (\mathbf{X}_k\boldsymbol{\theta}+\boldsymbol{\eta}_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1})\\
							&= \boldsymbol{\epsilon}_{k-1}-K_k \mathbf{X}_k(\boldsymbol{\theta}-\boldsymbol{\theta}_{k-1})-K_k\boldsymbol{\eta}_k\\
							&= (I-K_k \mathbf{X}_k)\boldsymbol{\epsilon}_{k-1}-K_k\boldsymbol{\eta}_k,
\end{align*}
where $I$ is the $d\times d$ identity matrix. The mean of this error is then
$$\mathbb{E}[\boldsymbol{\epsilon}_{k}] = (I-K_k \mathbf{X}_k)\mathbb{E}[\boldsymbol{\epsilon}_{k-1}]-K_k\mathbb{E}[\boldsymbol{\eta}_{k}]$$
If $\mathbb{E}[\boldsymbol{\eta}_{k}]=0$ and $\mathbb{E}[\boldsymbol{\epsilon}_{k-1}]=0$, then $\mathbb{E}[\boldsymbol{\epsilon}_{k}]=0$. So if the measurement noise has zero mean for all $k$, and the initial estimate of $\boldsymbol{\theta}$ is set equal to its expected value, then $\boldsymbol{\theta}_k=\boldsymbol{\theta}_k, \forall k$. This property tells us that the estimator $\boldsymbol{\theta}_k = \boldsymbol{\theta}_{k-1}+K_k (\rvy_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1})$ is \textit{unbiased}. This property holds regardless of the value of the gain vector $K_k$. This means the estimate will be equal to the true value $\boldsymbol{\theta}$ on average. 

The key is to \textbf{determine the optimal value of the gain vector} $K_k$. The optimality criterion is to \textbf{minimize the aggregated variance of the estimation errors at time} $k$: 
\begin{align*}
	J_k &= \mathbb{E}[\|\boldsymbol{\theta}-\boldsymbol{\theta}_k\|^2]\\
		&= \mathbb{E}[\boldsymbol{\epsilon}_{k}^T\boldsymbol{\epsilon}_{k}]\\
		&= \mathbb{E}[tr(\boldsymbol{\epsilon}_{k}\boldsymbol{\epsilon}_{k}^T)]\\
		&= tr(P_k),
\end{align*}
where $P_k=\mathbb{E}[\boldsymbol{\epsilon}_{k}\boldsymbol{\epsilon}_{k}^T]$ is the estimation-error covariance (\ie covariance matrix). Note that the third line is done by the trace of a product (\ie cyclic property) and the expectation in the third line can go into the trace operator by its linearity. Next, we can obtain $P_k$ by
\begin{align*}
	P_k &= \mathbb{E}\bigg[\big((I-K_k \mathbf{X}_k)\boldsymbol{\epsilon}_{k-1}-K_k\boldsymbol{\eta}_k\big)\big((I-K_k \mathbf{X}_k)\boldsymbol{\epsilon}_{k-1}-K_k\boldsymbol{\eta}_k\big)^T\bigg]
\end{align*}
By rearranging the above equation with the property that the mean of noise is zero, we can get
\begin{align}
	P_k = (I-K_k \mathbf{X}_k)P_{k-1}(I-K_k \mathbf{X}_k)^T+K_kR_kK_k^T,
	\label{eq:rls_estimation_cov}
\end{align}
where $R_k = \mathbb{E}[\boldsymbol{\eta}_k\boldsymbol{\eta}_k^T]$ as covariance of $\boldsymbol{\eta}_k$.

This equation is the recurrence for the covariance of the least squares estimation error. It is consistent with the intuition that as the measurement noise $R_k$ increases, the uncertainty $P_k$ increases. 

Next, we have to compute $K_k$ that minimizes the cost function given by error equation. We are going to utilize the following property:
\begin{align*}
	\frac{\partial Tr(CX^T)}{\partial X} &= C\\
	\frac{\partial Tr(XCX^T)}{\partial X} &= XC + XC^T
\end{align*}
Next, we are going to take a derivative to the objective function:
\begin{align*}
	\frac{\partial J_k}{\partial K_k} &= \frac{\partial}{\partial K_k} tr\big(P_{k-1}- K_k\mathbf{X}_k P_{k-1} - P_{k-1}\mathbf{X}_k^TK_k^T+K_k\big(\mathbf{X}_kP_{k-1}\mathbf{X}_k^T\big)K_k^T\big)+ \frac{\partial}{\partial K_k}tr\big(K_k R_k K_k^T\big)\\
									  &= -2\frac{\partial}{\partial K_k} tr\big(P_{k-1}\mathbf{X}_k^TK_k^T\big)+2K_k\big(\mathbf{X}_kP_{k-1}\mathbf{X}_k^T\big)+2K_kR_k\\
									  &= -2P_{k-1}\mathbf{X}_k^T+ 2K_k\mathbf{X}_kP_{k-1}\mathbf{X}_k^T+2K_kR_k\\
									  &= -2P_{k-1}\mathbf{X}_k^T+ 2K_k(\mathbf{X}_kP_{k-1}\mathbf{X}_k^T+R_k).
\end{align*}
In the second step, we also used the property that $P_{k-1}$ is independent of $K_k$. Thus, the derivative of $P_{k-1}$ with respect to $K_k$ is zero. We also used that $K_k\mathbf{X}_kP_{k-1}$ and $P_{k-1}\mathbf{X}_k^TK_k^T$ are transpose of each other, since $P_{k-1}$ is symmetric. Thus, they have the same trace. By setting the partial derivative to zero, we get
$$K_k = P_{k-1}\mathbf{X}_k^T(\mathbf{X}_kP_{k-1}\mathbf{X}_k^T+R_k)^{-1}.$$
By setting $\mathbf{X}_kP_{k-1}\mathbf{X}_k^T+R_k = S_k$, we get 
$$K_k = P_{k-1}\mathbf{X}_k^TS_k^{-1}.$$
By putting this into \Cref{eq:rls_estimation_cov},
\begin{align*}
	P_k &= (I-P_{k-1}\mathbf{X}_k^TS_k^{-1} \mathbf{X}_k)P_{k-1}(I-P_{k-1}\mathbf{X}_k^TS_k^{-1} \mathbf{X}_k)^T+P_{k-1}\mathbf{X}_k^TS_k^{-1} R_k S_k^{-1}\mathbf{X}_kP_{k-1}\\ 
		&\quad \vdots\\
		&= P_{k-1}-P_{k-1}\mathbf{X}_k^TS_k^{-1}\mathbf{X}_k^TP_{k-1}\\
		&= (I-K_k\mathbf{X}_k)P_{k-1}.
\end{align*}
Note that $P_k$ is symmetric (\cf $P_k=\boldsymbol{\epsilon}_{k}\boldsymbol{\epsilon}_{k}^T$), since it is a covariance matrix, and so is $S_k$.

We take the inverse of both sides of 
$$P_{k-1}^{-1} = \bigg(\underbrace{P_{k-1}}_{A}-\underbrace{P_{k-1}\mathbf{X}_k^T}_{B}\big(\underbrace{\mathbf{X}_kP_{k-1}\mathbf{X}_k^T}_{D}\big)^{-1}\underbrace{\mathbf{X}_kP_{k-1}}_{C}\bigg)^{-1}.$$
Next, we apply the matrix inversion lemma which is known as \textit{Sherman-Morrison-Woodbury matrix identity} (or \textit{matrix inversion lemma}) identity: 
$$(A-BD^{-1}C)^{-1} = A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1}.$$
Then, rewrite $P_k^{-1}$ as follows:
\begin{align*}
	P_k^{-1} &= P_{k-1}^{-1}+P_{k-1}^{-1}P_{k-1}\mathbf{X}_k^T\big((\mathbf{X}_kP_{k-1}\mathbf{X}_k^T+R_k)-\mathbf{X}_kP_{k-1}P_{k-1}^{-1}(P_{k-1}\mathbf{X}_k^T)\big)^{-1}\mathbf{X}_kP_{k-1}P_{k-1}^{-1}\\ 
			 &= P_{k-1}^{-1}+\mathbf{X}_k^TR_{k}^{-1}\mathbf{X}_k
\end{align*}
This yields an alternative expression for the covariance matrix:
\begin{align*}
	P_k = \big(P_{k-1}^{-1}+\mathbf{X}_k^TR_{k}^{-1}\mathbf{X}_k\big)^{-1}
\end{align*}
We can also get
\begin{align*}
	K_k = P_{k}\mathbf{X}_k^TR_{k}^{-1}
\end{align*}
By
\begin{align*}
	P_k &= (I-K_k\mathbf{X}_k)P_{k-1}\\
	P_kP_{k-1}^{-1} &= (I-K_k\mathbf{X}_k)\\
	P_kP_k^{-1} &= P_kP_{k-1}^{-1}+P_k\mathbf{X}_k^TR_{k}^{-1}\mathbf{X}_k=I\\
	I &= (I-K_k\mathbf{X}_k)+P_k\mathbf{X}_k^TR_{k}^{-1}\mathbf{X}_k\\
	K_k &= P_{k}\mathbf{X}_k^TR_{k}^{-1}.
\end{align*}

In sum, RLS can be updated as follows: 
\begin{itemize}
	\item Update the gain matrix: $K_k = P_{k-1}\mathbf{X}_k^T(\mathbf{X}_kP_{k-1}\mathbf{X}_k^T+R_k)^{-1}$
	\item Update estimate: $\boldsymbol{\theta}_k = \boldsymbol{\theta}_{k-1}+K_k (\rvy_k-\mathbf{X}_k\boldsymbol{\theta}_{k-1})$
	\item Update error covariance matrix by either: 
		\begin{itemize}
			\item $P_k = (I-K_k\mathbf{X}_k)P_{k-1}$.
		\end{itemize}
\end{itemize}


% \paragraph{Example: }




