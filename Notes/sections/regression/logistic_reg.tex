\section{Logistic Regression}
\label{sec:logistic_regression}


Whereas linear regression maps inputs to $\mathbb{R}$, logistic regression
predicts values in the bounded interval $[0,1]$, letting us interpret the
outputs as probabilities.
We therefore pass the linear score through the \emph{sigmoid} (logistic)
function
\[
  \sigma(z) \;=\; \frac{1}{1+\exp(-z)}, 
  \qquad 
  z = \beta^{\!\top}\mathbf x .
\]
The class probabilities are
\[
  P\!\{Y=1\!\mid\!X\} = \sigma(z),
  \qquad
  P\!\{Y=0\!\mid\!X\} = 1 - \sigma(z).
\]
We are going to use the Maximum Likelihood Estimation (MLE) to find the best parameter. The MLE is a method used in statistics to estimate the parameters of a probability distribution by maximizing a likelihood function. Essentially, MLE finds the parameter values that make the observed data most probable.

Let's first define the Likelihood Function: Based on the probability distribution of the data, write down the likelihood function. For a set of observations \(X = \{x_1, x_2, \ldots, x_n\}\), and a parameter \(\beta\), the likelihood function \(L(\beta; X)\) represents the probability of observing the given data under the parameter \(\beta\).

Since the likelihood can be a product of probabilities, it might be easier to work with the natural logarithm of the likelihood function, called the log-likelihood. This transforms the product into a sum, simplifying the computation:
\[
\ell(\beta; X) = \log L(\beta; X)
\]

Finally, to find the optimal parameter, find the parameter value \(\hat{\beta}\) that maximizes the log-likelihood function.

For a dataset with \( n \) observations, the likelihood function is the product of the probabilities of observing the given outcomes. For logistic regression, the likelihood function \( L(\beta; X, Y) \) is given by:
\[
L(\beta; X, Y) = \prod_{i=1}^{n} P(Y_i | X_i, \beta)
\]
Given the binary nature of \( Y \), this can be written as:
\begin{align*}
	L(\beta; X, Y) = \prod_{i=1}^{n} [P(Y_i = 1 | X_i, \beta)]^{Y_i} [P(Y_i = 0 | X_i, \beta)]^{1 - Y_i}
\end{align*}
Substituting the logistic function, we get:
\[
L(\beta; X, Y) = \prod_{i=1}^{n} \left[\frac{1}{1 + \exp(-X_i \beta)}\right]^{Y_i} \left[1 - \frac{1}{1 + \exp(-X_i \beta)}\right]^{1 - Y_i}
\]
The negative log-likelihood for logistic regression is then given by
\begin{align*}
	\textrm{NLL}(\beta) = -\sum_{i=1}^N y_i\ln\sigma(\beta^T\rvx)+(1-y_i)\ln(1-\sigma(\beta^T\rvx)).
							 % &= -\sum_{i=1}^{N}\textrm{log}[\mu_i^{\mathds{I}(y_i=1)}\times (1-\mu_i)^{\mathds{I}(y_i=0)}]\\
	% &=-\sum_{i=1}^{N}[y_i\textrm{log}\mu_i + (1-y_i) \textrm{log}(1-\mu_i)], \textrm{ }
\end{align*}
This is also called \textbf{cross-entropy} error function. 
\footnotetext{$\mathds{I}(y_i=1) = y_i$, because $y_i\in \{0, 1\}$ is a binary variable} 

To compute the derivative of NLL, we first need to know the following tricks:
\begin{itemize}
	\item The derivative of $\ln (x)$:
$$\frac{\partial }{\partial x}\ln (x) = \frac{1}{x}.$$
\item The derivative of the sigmoid is given by:
$$\frac{\partial \sigma(z)}{\partial x} = \sigma(x)(1-\sigma(x)).$$
\item Finally, the chain rule of derivative. Suppose we are computing the derivative of a composite function $f(x) = u(v(x))$. The derivative of $f(x)$ is the derivative of $u(x)$ with respect to $v(x)$ times the derivative of $v(x)$ with respect to $x$.
$$\frac{\partial f}{\partial x} = \frac{\partial u}{\partial v} \frac{\partial v}{\partial x}$$
\end{itemize}
The derivative of the loss function w.r.t., a single weight $w_j$ is given by
\begin{align*}
	\frac{\partial \mathcal{L}}{\partial w_j} &= \frac{\partial }{\partial w_j} -[\rvy\ln \sigma(\rvw\rvx + \rvb)+(1-\rvy) \ln (1-\sigma(\rvw\rvx+\rvb))]\\
											  &=  -[\frac{\partial }{\partial w_j}y\ln \sigma(wx + b)+\frac{\partial }{\partial w_j}(1-y) \ln (1-\sigma(wx+b))]\\
											  &= -\frac{y}{\sigma(wx + b)}\frac{\partial }{\partial w_j}\sigma(wx + b) - \frac{1-y}{1-\sigma(wx+b)} \frac{\partial }{\partial w_j}\left[1-\sigma(wx+b)\right]\\
											  &= -\bigg[\frac{y}{\sigma(wx + b)}-\frac{1-y}{1-\sigma(wx + b)}\bigg]\frac{\partial }{\partial w_j}\sigma(wx + b)\\
											  &= -\bigg[\frac{y-\sigma(wx + b)}{\sigma(wx + b)[1-\sigma(wx + b)]}\bigg]\sigma(wx + b)[1-\sigma(wx + b)]\frac{\partial \sigma(wx + b)}{\partial w_j}\\
											  &= -\bigg[\frac{y-\sigma(wx + b)}{\sigma(wx + b)[1-\sigma(wx + b)]}\bigg]\sigma(wx + b)[1-\sigma(wx + b)]x_j\\ 
											  &= -( y-\sigma(wx + b) )x_j\\
											  &= ( \sigma(wx + b)-y )x_j.
\end{align*}
Thus, the gradient of the cost function $J(\boldsymbol{\theta})$ with respect to the weights $\boldsymbol{\theta}$ is:

\[
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) = \frac{1}{m} \mathbf{X}^T (\sigma(\mathbf{X} \boldsymbol{\theta}) - \mathbf{y})
\]

This gradient is used in gradient descent to update the weights $\boldsymbol{\theta}$:

\[
\boldsymbol{\theta} := \boldsymbol{\theta} - \alpha \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}),
\]
where $\alpha$ is the learning rate.

\subsection{Another Interpretation}

Logistic regression models \textit{logit}s (log odds) through a linear model. For binary data, the goal is to model the probability $p$ that one of two outcomes occurs. Recall that an ordinary linear regression model is not bounded. Thus, we will pass a linear model through a sigmoid function, which is also known as logistic function. 
$$\sigma(z) = \frac{1}{1+\exp^{z}},$$
where $z=wx+b.$
The sigmoid function has the property
$$\sigma(-x) = 1-\sigma(x).$$
The $z$ is often called the logit. Note that the inverse of the sigmoid is the log of the odds ratio $\frac{p}{1-p}.$

The logit function is $\textrm{log}\frac{p}{1-p}$, which varies between $-\infty$ and $+\infty$ as $p$ varies between $0$ and $1$.
$$\textrm{log}\frac{p}{1-p} = w_0x_0 +  w_1x_1 + \dots + w_nx_n.$$
Note that the logistic regression model assumes that the log-odds (also called the \textit{logit}) of an observation $y$ can be expressed as a linear function of the predictor variables. In this context, the logit function is referred to as the \textit{link} function because it ``links'' the probability of an event to a linear combination of the predictors. The logit provides information about the ratio of two outcomes. For example, suppose an event has a probability of occurring of 60\%. If a certain factor makes the event twice as likely to occur, the logit can be used to quantify this by comparing the probability of the event occurring to the probability of it not occurring.

