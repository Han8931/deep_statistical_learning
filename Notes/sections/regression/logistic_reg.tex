\section{Logistic Regression}
\label{sec:logistic_regression}

Logistic regression corresponds to the following binary classification model parameterized by $\rvw$:
$$p(y|\mathbf{x},\mathbf{w})=\textrm{Ber}(y|\sigma(\mathbf{w}^T\mathbf{x}))$$

Logistic regression models \textit{logit}s (log odds) through a linear model. For binary data, the goal is to model the probability $p$ that one of two outcomes occurs. Recall that an ordinary linear regression model is not bounded. Thus, we will pass a linear model through a sigmoid function, which is also known as logistic function. 

$$\sigma(z) = \frac{1}{1+\exp^{z}},$$
where $z=wx+b.$
The sigmoid function has the property
$$\sigma(-x) = 1-\sigma(x).$$
The $z$ is often called the logit. Note that the inverse of the sigmoid is the log of the odds ratio $\frac{p}{1-p}.$

The logit function is $\textrm{log}\frac{p}{1-p}$, which varies between $-\infty$ and $+\infty$ as $p$ varies between $0$ and $1$.
$$\textrm{log}\frac{p}{1-p} = w_0x_0 +  w_1x_1 + ... + w_nx_n$$
Note that \textbf{the logistic regression model assumes that the log-odds (\textit{logit}) of an observation $y$ can be expressed as a linear function}. In this context, the logit function is called the \textbf{\textit{link function}} because it ``links'' the probability to the linear function of the predictor variables.

% Simplest solution to model a dependant variable $y$ is a linear regression. However, $y$ should be in a range of $[0,1]$. So we need to introduce the logit function. 

% The linear regression can be generalized to the classification setting with two changes:
% \begin{itemize}
% 	\item Replacing the Gaussian distribution for $y$ with a Bernoulli distribution: $p(y|\mathbf{x},\mathbf{w})=Ber(y|\mu(\mathbf{x}))$
% 	\item Squashing input data into sigmoid function $\sigma(\eta)$ that range from 0 to 1: $\sigma(\eta)\triangleq \frac{1}{1+exp(-\eta)}$.
% \end{itemize}
% $$p(y|\mathbf{x},\mathbf{w})=Ber(y|\sigma(\mathbf{w}^T\mathbf{x})),\$$

The negative log-likelihood for logistic regression is given by
\begin{align*}
	\textrm{NLL}(\mathbf{w}) &= -\ln \prod_{i=1}^N p(\rvx)^{\mathds{I}(y_i=1)}(1-p(\rvx))^{\mathds{I}(y_i=0)}\,\footnotemark\\
							 &= -\ln \prod_{i=1}^N \sigma(\mathbf{w}^T\rvx)^{\mathds{I}(y_i=1)}(1-\sigma(\mathbf{w}^T\rvx))^{\mathds{I}(y_i=0)}\\
							 &= -\sum_{i=1}^N y_i\ln\sigma(\mathbf{w}^T\rvx)+\ln(1-y_i)(1-\sigma(\mathbf{w}^T\rvx)).
							 % &= -\sum_{i=1}^{N}\textrm{log}[\mu_i^{\mathds{I}(y_i=1)}\times (1-\mu_i)^{\mathds{I}(y_i=0)}]\\
	% &=-\sum_{i=1}^{N}[y_i\textrm{log}\mu_i + (1-y_i) \textrm{log}(1-\mu_i)], \textrm{ }
\end{align*}
This is also called \textbf{cross-entropy} error function. 
\footnotetext{$\mathds{I}(y_i=1) = y_i$, because $y_i\in \{0, 1\}$ is a binary variable} 

To compute the derivative of NLL, we first need to know the following tricks:
\begin{itemize}
	\item The derivative of $\ln (x)$:
$$\frac{\partial }{\partial x}\ln (x) = \frac{1}{x}.$$
\item The derivative of the sigmoid is given by:
$$\frac{\partial \sigma(z)}{\partial x} = \sigma(x)(1-\sigma(x)).$$
\item Finally, the chain rule of derivative. Suppose we are computing the derivative of a composite function $f(x) = u(v(x))$. The derivative of $f(x)$ is the derivative of $u(x)$ with respect to $v(x)$ times the derivative of $v(x)$ with respect to $x$.
$$\frac{\partial f}{\partial x} = \frac{\partial u}{\partial v} \frac{\partial v}{\partial x}$$
\end{itemize}
The derivative of the loss function w.r.t., a single weight $w_j$ is given by
\begin{align*}
	\frac{\partial \mathcal{L}}{\partial w_j} &= \frac{\partial }{\partial w_j} -[y\ln \sigma(wx + b)+(1-y) \ln (1-\sigma(wx+b))]\\
											  &=  -[\frac{\partial }{\partial w_j}y\ln \sigma(wx + b)+\frac{\partial }{\partial w_j}(1-y) \ln (1-\sigma(wx+b))]\\
											  &= -\frac{y}{\sigma(wx + b)}\frac{\partial }{\partial w_j}\sigma(wx + b) - \frac{1-y}{1-\sigma(wx+b)} \frac{\partial }{\partial w_j}1-\sigma(wx+b)\\
											  &= -\bigg[\frac{y}{\sigma(wx + b)}-\frac{1-y}{1-\sigma(wx + b)}\bigg]\frac{\partial }{\partial w_j}\sigma(wx + b)\\
											  &= -\bigg[\frac{y-\sigma(wx + b)}{\sigma(wx + b)[1-\sigma(wx + b)]}\bigg]\sigma(wx + b)[1-\sigma(wx + b)]\frac{\partial \sigma(wx + b)}{\partial w_j}\\
											  &= -\bigg[\frac{y-\sigma(wx + b)}{\sigma(wx + b)[1-\sigma(wx + b)]}\bigg]\sigma(wx + b)[1-\sigma(wx + b)]x_j\\ 
											  &= -( y-\sigma(wx + b) )x_j\\
											  &= ( \sigma(wx + b)-y )x_j.
\end{align*}





Another way to express \textrm{NLL} is as follows. Suppose $\hat{y}_i\in\{-1,+1\}$ instead of $y_i\in\{0,1\}$. We have $p(y=1)=\frac{1}{1+\mathrm{exp}(-\mathbf{w}^T\mathbf{x})}$ and $p(y=-1)=\frac{1}{1+\mathrm{exp}(+\mathbf{w}^T\mathbf{x})}$. Hence
\begin{align*}
	\textrm{NLL}(\mathbf{w}) &= -\frac{1}{N}\sum_{n=1}^N [\mathbb{I}(\hat{y}_n=1)\log(\sigma(a_n))+\mathbb{I}(\hat{y}_n=-1)\log(\sigma(-a_n))]\\
							 &= -\frac{1}{N}\sum_{n=1}^N \log(\sigma(\hat{y}_na_n))\\
							 &=  \frac{1}{N}\sum_{i=1}^{N}\textrm{log}(1+\mathrm{exp}(-\hat{y}_i\mathbf{w}^T\mathbf{x}_i).
\end{align*}
Note that the sigmoid is used for compressing the output into $[0,1]$ and $\sigma(-a_n) = 1-\sigma(a_n)$. Unlike the linear regression, there is no closed from solution for logistic regression, thus we need optimization algorithms for it. Typically, optimization process involves the gradient and Hessian. 
\begin{align*}
	\mathbf{g}&=\frac{d}{d\mathbf{w}}\mathrm{NLL}(\mathbf{w})=\frac{d}{d\mu_i}\mathrm{NLL}(\mathbf{w})\frac{d\mu_i}{d\mathbf{h}}\frac{d\mathbf{h}}{d\mathbf{w}}\\
	& = \sum_{i=1}\Bigg[-\frac{y_i}{\mu_i} + \frac{(1-y_i) }{(1-\mu_i)}\Bigg]\frac{d\mu_i}{d\mathbf{h}}\frac{d\mathbf{h}}{d\mathbf{w}}=\sum_{i=1}\Bigg[\frac{\mu_i-y_i }{\mu_i(1-\mu_i)}\Bigg]\frac{d\mu_i}{d\mathbf{h}}\frac{d\mathbf{h}}{d\mathbf{w}}\\
	&=\sum_{i}(\mu_i-y_i)\mathbf{x}_i=\mathbf{X}^T(\boldsymbol{\mu}-\mathbf{y})\\
	\frac{d\mu_i}{d\mathbf{h}}& = \mu_i(1-\mu_i)\\
	\frac{d\mathbf{h}}{d\mathbf{w}}& = \mathbf{x}_i
\end{align*}
where $\mathbf{h}=\mathbf{w}^T\mathbf{x}$. 

We can also use the second-order method. 
\begin{align*}
\mathbf{H}&=\frac{d}{d\mathbf{w}}g(\mathbf{w})^T=\sum_{i}(\nabla_{\mathbf{w}}\mu_i)\mathbf{x}_i^T=\sum_{i}\mu_i(1-\mu_i)\mathbf{x}_i\mathbf{x}_i^T\\
&=\mathbf{X}^T\mathbf{S}\mathbf{X},
\end{align*}
where $\mathbf{S}\triangleq \mathrm{diag}(\mu_i(1-\mu_i))$. Note that $\mathbf{H}$ is positive definite, because the \textrm{NLL} is convex and has a global minimum. 
