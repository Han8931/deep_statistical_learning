\section{Logistic Regression}
\label{sec:logistic_regression}

% Logistic regression corresponds to the following binary classification model parameterized by $\rvw$:
% $$p(y|\mathbf{x},\mathbf{w})=\textrm{Ber}(y|\sigma(\mathbf{w}^T\mathbf{x}))$$

Whereas linear regression maps inputs to $\mathbb{R}$, logistic regression
predicts values in the bounded interval $[0,1]$, letting us interpret the
outputs as probabilities.
We therefore pass the linear score through the \emph{sigmoid} (logistic)
function
\[
  \sigma(z) \;=\; \frac{1}{1+\exp(-z)}, 
  \qquad 
  z = \beta^{\!\top}\mathbf x .
\]
The class probabilities are
\[
  P\!\{Y=1\!\mid\!X\} = \sigma(z),
  \qquad
  P\!\{Y=0\!\mid\!X\} = 1 - \sigma(z).
\]

We are going to use the Maximum Likelihood Estimation (MLE) to find the best parameter. The MLE is a method used in statistics to estimate the parameters of a probability distribution by maximizing a likelihood function. Essentially, MLE finds the parameter values that make the observed data most probable.

Let's first define the Likelihood Function: Based on the probability distribution of the data, write down the likelihood function. For a set of observations \(X = \{x_1, x_2, \ldots, x_n\}\), and a parameter \(\beta\), the likelihood function \(L(\beta; X)\) represents the probability of observing the given data under the parameter \(\beta\).

Since the likelihood can be a product of probabilities, it might be easier to work with the natural logarithm of the likelihood function, called the log-likelihood. This transforms the product into a sum, simplifying the computation:
\[
\ell(\beta; X) = \log L(\beta; X)
\]

Finally, to find the optimal parameter, find the parameter value \(\hat{\beta}\) that maximizes the log-likelihood function.

For a dataset with \( n \) observations, the likelihood function is the product of the probabilities of observing the given outcomes. For logistic regression, the likelihood function \( L(\beta; X, Y) \) is given by:
\[
L(\beta; X, Y) = \prod_{i=1}^{n} P(Y_i | X_i, \beta)
\]
Given the binary nature of \( Y \), this can be written as:
\begin{align*}
	L(\beta; X, Y) = \prod_{i=1}^{n} [P(Y_i = 1 | X_i, \beta)]^{Y_i} [P(Y_i = 0 | X_i, \beta)]^{1 - Y_i}
\end{align*}
Substituting the logistic function, we get:
\[
L(\beta; X, Y) = \prod_{i=1}^{n} \left[\frac{1}{1 + \exp(-X_i \beta)}\right]^{Y_i} \left[1 - \frac{1}{1 + \exp(-X_i \beta)}\right]^{1 - Y_i}
\]
The negative log-likelihood for logistic regression is then given by
\begin{align*}
	\textrm{NLL}(\beta) = -\sum_{i=1}^N y_i\ln\sigma(\beta^T\rvx)+\ln(1-y_i)(1-\sigma(\beta^T\rvx)).
							 % &= -\sum_{i=1}^{N}\textrm{log}[\mu_i^{\mathds{I}(y_i=1)}\times (1-\mu_i)^{\mathds{I}(y_i=0)}]\\
	% &=-\sum_{i=1}^{N}[y_i\textrm{log}\mu_i + (1-y_i) \textrm{log}(1-\mu_i)], \textrm{ }
\end{align*}
This is also called \textbf{cross-entropy} error function. 
\footnotetext{$\mathds{I}(y_i=1) = y_i$, because $y_i\in \{0, 1\}$ is a binary variable} 

To compute the derivative of NLL, we first need to know the following tricks:
\begin{itemize}
	\item The derivative of $\ln (x)$:
$$\frac{\partial }{\partial x}\ln (x) = \frac{1}{x}.$$
\item The derivative of the sigmoid is given by:
$$\frac{\partial \sigma(z)}{\partial x} = \sigma(x)(1-\sigma(x)).$$
\item Finally, the chain rule of derivative. Suppose we are computing the derivative of a composite function $f(x) = u(v(x))$. The derivative of $f(x)$ is the derivative of $u(x)$ with respect to $v(x)$ times the derivative of $v(x)$ with respect to $x$.
$$\frac{\partial f}{\partial x} = \frac{\partial u}{\partial v} \frac{\partial v}{\partial x}$$
\end{itemize}
The derivative of the loss function w.r.t., a single weight $w_j$ is given by
\begin{align*}
	\frac{\partial \mathcal{L}}{\partial w_j} &= \frac{\partial }{\partial w_j} -[\rvy\ln \sigma(\rvw\rvx + \rvb)+(1-\rvy) \ln (1-\sigma(\rvw\rvx+\rvb))]\\
											  &=  -[\frac{\partial }{\partial w_j}y\ln \sigma(wx + b)+\frac{\partial }{\partial w_j}(1-y) \ln (1-\sigma(wx+b))]\\
											  &= -\frac{y}{\sigma(wx + b)}\frac{\partial }{\partial w_j}\sigma(wx + b) - \frac{1-y}{1-\sigma(wx+b)} \frac{\partial }{\partial w_j}\left[1-\sigma(wx+b)\right]\\
											  &= -\bigg[\frac{y}{\sigma(wx + b)}-\frac{1-y}{1-\sigma(wx + b)}\bigg]\frac{\partial }{\partial w_j}\sigma(wx + b)\\
											  &= -\bigg[\frac{y-\sigma(wx + b)}{\sigma(wx + b)[1-\sigma(wx + b)]}\bigg]\sigma(wx + b)[1-\sigma(wx + b)]\frac{\partial \sigma(wx + b)}{\partial w_j}\\
											  &= -\bigg[\frac{y-\sigma(wx + b)}{\sigma(wx + b)[1-\sigma(wx + b)]}\bigg]\sigma(wx + b)[1-\sigma(wx + b)]x_j\\ 
											  &= -( y-\sigma(wx + b) )x_j\\
											  &= ( \sigma(wx + b)-y )x_j.
\end{align*}
Thus, the gradient of the cost function $J(\boldsymbol{\theta})$ with respect to the weights $\boldsymbol{\theta}$ is:

\[
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) = \frac{1}{m} \mathbf{X}^T (\sigma(\mathbf{X} \boldsymbol{\theta}) - \mathbf{y})
\]

This gradient is used in gradient descent to update the weights $\boldsymbol{\theta}$:

\[
\boldsymbol{\theta} := \boldsymbol{\theta} - \alpha \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}),
\]
where $\alpha$ is the learning rate.

