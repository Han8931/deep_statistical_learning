\chapter{Introduction to Kernel Methods}

\section{Introduction}

In machine learning a \textit{kernel} is a function that measures similarity between two objects and, at the same time, implicitly maps those objects into (possibly very-high-dimensional) feature space without ever computing the coordinates of that space directly.

Any kernel methods solution comprises two parts: a module that performs the mapping into the embedding or feature space and a learning algorithm designed to discover linear patterns in that space. There are two main reasons why this approach should work. First of all, detecting linear relations has been the focus of much research in statistics and machine learning for decades, and the resulting algorithms are both well understood and efficient. Secondly, we will see that there is a computational shortcut which makes it possible to represent linear patterns efficiently in high-dimensional spaces to ensure adequate representational power. The shortcut is what we call a kernel function.

\begin{itemize}
	\item Data items are embedded into a vector space called the feature space. 
	\item Linear relations are sought among the images of the data items in the feature space.
	\item The algorithms are implemented in such a way that the coordinates of the embedded points are not needed, only their pairwise inner products. 
	\item The pairwise inner products can be computed efficiently directly from the original data items using a kernel function.
\end{itemize}
These four observations will imply that, despite restricting ourselves to algorithms that optimise linear functions, our approach will enable the development of a rich toolbox of efficient and well-founded methods for discovering nonlinear relations in data through the use of nonlinear embedding mappings.

When people say that \emph{kernels are more efficient}, they are comparing two strategies for working with a nonlinear feature map~$\varphi$:

\begin{center}
\begin{tabular}{|l|p{5.6cm}|l|}
\hline
\textbf{Approach} & \textbf{What we actually compute} & \textbf{Cost per evaluation}\\
\hline
Explicit map & build $\varphi(x)\in\mathbb{R}^{D}$ then take $\langle\varphi(x),\varphi(z)\rangle$ & $\mathcal{O}(D)$ (often gigantic)\\
Kernel trick & evaluate $K(x,z)=\langle\varphi(x),\varphi(z)\rangle$ directly & $\mathcal{O}(d)$ or $\mathcal{O}(d\,p)$, usually $d\ll D$\\
\hline
\end{tabular}
\end{center}

Most algorithms (SVMs, kernel PCA, Gaussian processes, \dots) depend \emph{only} on inner products, so they never need the coordinates of $\varphi(x)$ themselves.  Replacing those inner products by a kernel sidesteps the high‐dimensional features.

\begin{itemize}
  \item \textbf{Dimensionality explosion.}  
        A degree–$p$ polynomial map of a $d$–dimensional vector has
        \[
            D \;=\; \binom{d+p}{p}
        \]
        coordinates.  For $d=1000$ and $p=3$, that is $\sim1.7\times10^{8}$ numbers \emph{per sample}.  
        The same inner product via the kernel $(1+x^{\top}z)^{3}$ costs only $\mathcal{O}(d)=1000$ multiplications.
  \item \textbf{Infinite feature spaces.}  
        The Gaussian RBF kernel corresponds to an \emph{infinite}‐dimensional $\varphi$, yet
        \[
            K(x,z)=\exp\!\bigl(-\|x-z\|^{2}/2\sigma^{2}\bigr)
        \]
        is a single distance and exponential.
  \item \textbf{Memory.}  
        Storing $n$ explicit feature vectors costs $\mathcal{O}(nD)$, while an $n\times n$ Gram matrix costs $\mathcal{O}(n^{2})$.  
        Whenever $D\gg n$, the kernel representation is lighter.
  \item \textbf{Algorithmic simplifications.}  
        Many solvers work entirely on the Gram matrix, letting you leverage highly–optimised linear algebra without feature‐wise loops.
\end{itemize}


\section{Hilbert Spaces}

First we recall what is meant by a linear function. Given a vector space $X$ over the reals, a function
\begin{align*}
	f: X\longrightarrow \mathbb{R}
\end{align*}
is linear if $f(\alpha\rvx) = \alpha f(\rvx)$ and $f(\rvx+\rvz) = f(\rvx)+ f(\rvz), \forall \rvx,\rvz \in X, \alpha\in \mathbb{R}$. 

\paragraph{Inner product space} A vector space $X$ over the reals \mathbb{R} is an \textit{inner product space} if there exists a real-valued symmetric bilinear map $\left<\cdot, \cdot\right>$, that satisfies
\begin{align*}
	\left<\rvx,\rvx\right>\geq 0.
\end{align*}
The bilinear map is known as the inner, dot or scalar product. Furthermore we will say the inner product is strict if 
\begin{align*}
	\left<\rvx, \rvx\right>=0 \iff \rvx=\mathbf{0}.
\end{align*}

Given a strict inner product space, we can define a norm on the space $X$ by 
\begin{align*}
	\left|\rvx\right|_2=\sqrt{\left<\rvx, \rvx\right>}.
\end{align*}
The associated metric or distance between two vectors is defined as $d(\rvx, \rvz) = \left|\rvx-\rvz\right|_2$. For the vector space $\mathbb{R}^n$ the standard inner product is given by 
\begin{align*}
	\left<\rvx, \rvz\right> = \sum_{i=1}^n x_i,z_i.
\end{align*}
An inner product space is sometimes referred to as a \textit{Hilbert space}, though most researchers require the additional properties of completeness and separability, as well as sometimes requiring that the dimension be infinite. We give a formal definition.

\begin{Definition}{Hilbert Space}
	A Hilbert Space $\mathcal{F}$ is an inner product space with the additional properties that it is \textit{separable} and \textit{complete}. Completeness refers to the property that every Cauchy sequence $\left\{h_n\right\}_{n\geq 1}$ of elements of $\mathcal{F}$ converges to an element $h\in \mathcal{F}$, where a Cauchy sequence is one satisfying the property that 
	\begin{align*}
		\sup_{m>n}\left|h_n-h_m\right|\to 0, \text{ as } n\to \infty.
	\end{align*}
	A space$\mathcal{F}$ is separable if for any $\epsilon>0$ there is a finite set of elements $h_1,\dots, h_N$ of $\mathcal{F}$ such taht for all $h\in \mathcal{F}$
	\begin{align*}
		\min_i \left|h_i-h\right|<\epsilon.
	\end{align*}
\end{Definition}

For instance, Let $X$ be the set of all countable sequences of real numbers $\rvx = (x_1,\dots, x_n, \dots)$, such that the sum 
\begin{align*}
	\sum_{i=1}^{\infty} x_i^2<\infty,
\end{align*}
with the inner product between two sequences $\rvx$ and $\rvy$ defined by
\begin{align*}
	\left<\rvx, \rvy\right> = \sum_{i=1}^{\infty} x_iy_i.
\end{align*}
This is the space known as $L_2$.

\begin{itemize}
	\item A vector space is just a collection of objects you can add and scale (\eg $\mathbf v + \mathbf w$, $3\mathbf v$) and all the usual algebra rules hold.
	\item An inner product $\langle,\cdot,\cdot\rangle$ on a vector space is a generalisation of the dot product. It must satisfy
		\begin{itemize}
			\item Bilinearity $\langle a\mathbf v + b\mathbf w,\mathbf z\rangle = a\langle\mathbf v,\mathbf z\rangle + b\langle\mathbf w,\mathbf z\rangle$ (and similarly in the second slot).
			\item Symmetry $\langle\mathbf v,\mathbf w\rangle = \langle\mathbf w,\mathbf v\rangle$.
			\item Positive‑definiteness $\langle\mathbf v,\mathbf v\rangle \ge 0$ with equality only when $\mathbf v=0$.
		\end{itemize}
	\item A metric space is \textit{complete} when every Cauchy sequence actually converges to a limit inside the space. 
		\begin{itemize}
			\item Analogy : rational numbers $\mathbb Q$ are not complete as Cauchy sequences like $3, 3.1, 3.14, 3.141,\dots$ hover toward $\pi$, which is missing from $\mathbb Q$. The reals $\mathbb R$ are complete. 
			\item Completeness is crucial for analysis: it guarantees limits, projections, series expansions (\eg Fourier), and many theorems work internally without you stepping outside the space.
		\end{itemize}
\end{itemize}





\section{Properties}

Given a finite subset $\mathcal{S} = \{\rvx_1, \dots, \rvx_{l}\}$ of an input space $\mathbf{X}$, a kernel $\kappa (\rvx, \rvz)$ and a feature map $\boldface{\phi}$ into a feature space $F$ satisfying
\begin{align*}
	\kappa \left<\boldface{\phi}(\rvx), \boldface{\phi}(\rvz)\right>,
\end{align*}
let $\boldface{\phi}(S) = \{\boldface{\phi}(\rvx_1), \dots, \boldface{\phi}(\rvx_l)\}$ be the image of $S$ under the map $\boldface{\phi}$. Hence $\boldface{\phi}(S)$ is a subset of the inner product of space $F$. Then the kernel matrix $\mathbf{K}$ of kernel evaluations between all pairs of elements of $S$ is given by
\begin{align*}
	\mathbf{K}_{ij} =\kappa \left<\boldface{\phi}(\rvx_i), \boldface{\phi}(\rvx_j)\right>, \quad i,j=1,\dots,l
\end{align*}
Working in a kernel-defined feature space means that we are not able to explicitly represent points. For example the image of an input point $\rvx$ is $\boldface{\phi}(\rvx)$, but we do not have access to the components of this vector, only to the evaluation of inner products between this point and the images of other points. Despite this handicap there is a surprising amount of useful information that can be gleaned about $\boldface{\phi}(\mathbf{S})$.

For two data points the transformed kernel $\hat{\kappa}$ is given by
\begin{align*}
\kappa(\rvx, \rvz) &= \left<\hat{\boldface{\phi}}(\rvx), \hat{\boldface{\phi}}(\rvz)\right> = \left<\frac{\boldface{\phi}(\rvx)}{\|\boldface{\phi}(\rvx)\|}, \frac{\boldface{\phi}(\rvz)}{\|\boldface{\phi}(\rvz)\|}\right> = \frac{\boldface{\phi}(\rvx)\boldface{\phi}(\rvz)}{\|\boldface{\phi}(\rvx)\|\|\boldface{\phi}(\rvz)\|}\\
				   &= \frac{\kappa(\rvx, \rvz)}{\sqrt{\kappa(\rvx, \rvx)\kappa(\rvz, \rvz)}}
\end{align*}

\begin{lstlisting}[language=Python]
% original kernel matrix stored in variable K
% output uses the same variable K
% D is a diagonal matrix storing the inverse of the norms
import numpy as np

# Example kernel matrix (must be symmetric with non‑negative diagonal entries)
K = np.array([[4.0, 2.0, 2.0],
              [2.0, 3.0, 0.5],
              [2.0, 0.5, 1.0]])

D = np.diag(1.0 / np.sqrt(np.diag(K)))

# Normalize the kernel
K = D @ K @ D            # same variable name as in MATLAB code
\end{lstlisting}

We can also evaluate the norms of linear combinations of images in the feature space. For example, we have 
\begin{align*}
	\left\|\sum_{i=1}^l\alpha_i\boldface{\phi}(\rvx_i)\right\|^2 &= \left<\sum_{i=1}^l\alpha_i\boldface{\phi}(\rvx_i), \sum_{i=j}^l\alpha_j\boldface{\phi}(\rvx_j)\right>\\
																 &=\sum_{i=1}^l\alpha_i\sum_{i=j}^l\alpha_j \left<\boldface{\phi}(\rvx), \boldface{\phi}(\rvz)\right>\\
																 &= \sum_{i,j=1}^l \alpha_i\alpha_j \kappa(\rvx, \rvz). 
\end{align*}

A special case of the norm is the length of the line joining two images $\boldface{\phi}(\rvx)$ and \boldface{\phi}(\rvz), which can be computed as
\begin{align*}
	\left|\boldface{\phi}(\rvx)-\boldface{\phi}(\rvz)\right|^2 &= \left<\boldface{\phi}(\rvx)-\boldface{\phi}(\rvz), \boldface{\phi}(\rvx)-\boldface{\phi}(\rvz)\right>\\
															   &= 
\end{align*}






\section{Kernel Regression}
\begin{align*}
	\hat{\boldsymbol{\theta}} = \mathbf{X}^T(\mathbf{X}\mathbf{X}^T+\lambda\mathbf{I})^{-1}\rvy.
\end{align*}
By substituting into $\hat{y}(\rvx') = \hat{\boldsymbol{\theta}}^T\rvx' = (\rvx')^T\hat{\boldsymbol{\theta}}$ gives
\begin{align*}
	\hat{y}(\rvx') = (\rvx')^T\hat{\boldsymbol{\theta}} = (\rvx')^T\mathbf{X}^T(\mathbf{X}\mathbf{X}^T+\lambda\mathbf{I})^{-1}\rvy.
\end{align*}
The prediction depends on the data only though inner products, since
\begin{align*}
	(\rvx')^T\mathbf{X}^T = \begin{bmatrix}
		\langle \rvx', \rvx_1 \rangle\\
		\vdots\\
		\langle \rvx', \rvx_n \rangle
	\end{bmatrix}, \quad
	\mathbf{X}\mathbf{X}^T = \begin{bmatrix}
		\langle \rvx_1, \rvx_1 \rangle & \cdots & \langle \rvx_1, \rvx_n \rangle\\
		\vdots & \ddots & \vdots\\
		\langle \rvx_n, \rvx_1 \rangle & \cdots & \langle \rvx_n, \rvx_n \rangle
	\end{bmatrix},
\end{align*}
\textbf{Kernel trick} replaces the inner products by kernel evaluations, which can be represented as follows:
\begin{align*}
	\hat{y}(\rvx') = k(\rvx')(K+\lambda \mathbf{I})^{-1}\rvy,
\end{align*}
where 
\begin{align*}
	k(\rvx') = \begin{bmatrix}
		k(\rvx', \rvx_1)\\
		\vdots\\
		k(\rvx', \rvx_n)
	\end{bmatrix}
	K = \begin{bmatrix}
		k(\rvx_1, \rvx_1) & \cdots & k(\rvx_1, \rvx_n)\\
		\vdots & \ddots & \vdots\\
		k(\rvx_n, \rvx_1) & \cdots & k(\rvx_n, \rvx_n)
	\end{bmatrix},
\end{align*}

\subsection{Non-Parametric Regression}
Kernel methods corresponding to infinite-dimensional $\phi(\rvx)$ can usually be considered as \textbf{non-parametric}.
For instance, 1-D case can be considered as a joining the dots by interpolation. 

% \subsection{Bayesian Posterior Derivation}

% Both $v(\rvx) = \langle \boldsymbol{\theta}, \rvx \rangle$ and $v(\rvx') = \langle \boldsymbol{\theta}, \rvx' \rangle$ have mean zero, so their covariance is 
% \begin{align*}
% 	Cov[v(\rvx), v(\rvx')] &= \mathbb{E}[(\rvx^T\boldsymbol{\theta})(\rvx')^T\boldsymbol{\theta}] = \mathbb{E}[(\rvx^T\boldsymbol{\theta})(\boldsymbol{\theta}^T\rvx')]\\
% 						   &= \rvx^T\mathbb{E}[\boldsymbol{\theta}\boldsymbol{\theta}^T]\rvx' = \langle \boldsymbol{\theta}, \rvx \rangle
% \end{align*}
% since $\mathbb{E}[\boldsymbol{\theta}\boldsymbol{\theta}^T] = \mathbf{I}$ for $\boldsymbol{\theta}\sim \mathcal{N}(\mathbf{0}, \mathbf{I})$. 









% We will define the kind of kernels (so-called (\textit{Mercer}) kernels) that are used in kernel methods. Those kernels are covariance functions that have to be \textit{symmetric} and \textit{positive(-semi) definite}.

% Basic idea: we can make linear ML models non-linear by applying basis function (feature) transformations on the input space. For an input vector $\mathbf{x}\in \mathbb{R}^d$, do transformation $\mathbf{x}\to \phi(\rvx)$ where $\phi(\rvx)\in \mathbb{R}^D$. Usually, $D\gg d$, because we add dimensions that capture non-linear interactions among features. 

% \begin{itemize}
% 	\item The main idea is to use large set of fixed non-linear basis functions.
% 	\item The complexity depends on number of basis functions, but dual trick changes it to a size of dataset. 
% 	\item The kernel function is a measure of similarity between $\rvx_i$and $\rvx_j$ 
% 	\item The key idea of kernel method is to represent the raw data based on a similarity function called kernel function over pairs of data points. 
% \end{itemize}


% \section{Kernel Trick}
% \label{sec:kernel:kernel_trick}

% Kernel trick aims to avoid computing explicit feature transformations $\phi(\rvx)$ by changing the entire ML algorithm to use only \textit{inner products} of feature transformations $\langle \phi(\rvx), \phi(\rvx')\rangle$ which are computed directly using a \textit{kernel} (rather than the feature transformations themselves.). Note that even the RBF kernel can be expressed as an inner product. 

% \subsection{Linear Model}

% Consider a linear model where we use gradient descent with a squared loss function to learn the parameters. So, we have $h(\rvx) = \mathbf{w}^T\mathbf{x}$ and the squared loss function:
% $$l(\rvw) = \sum_{i=1}^{n}(\rvw^T\rvx - y_i)^2.$$

% The gradient w.r.t., $\rvw$ is given by 

% $$g(\rvw) = \frac{\partial l}{\partial \rvw} = \sum_{i=1}^{n}2(\rvw^T\rvx - y_i)\rvx_i.$$
% Since the loss is convex, the final solution is independent of the initialization, and we can initialize $\rvw_0$ to be whatever we want. For convenience, we choose $\rvw_0=\mathbf{0}.$

% Recall that the gradient update rule is given by
% $$\rvw_{t+1} = \rvw_t-\eta g(\rvw_t),$$
% where $\eta$ is the step size. We can show that for every $t$, $\rvw_t$ is a linear combination of $\rvx_i$ by setting 
% $$\rvw_t = \sum_{i=1}^{n}\alpha_i^{(t)}\rvx_i.$$
% Then,
% \begin{align*}
% 	\rvw_{t+1} &= \rvw_t - \eta g(w)\\
% 			   &= \sum_{i=1}^{n}\alpha_i^{(t)}\rvx_i-\eta\sum_{i=1}^{n}2(\rvw^T\rvx - y_i)\rvw_i\\
% 			   &= \sum_{i=1}^{n}\alpha_i^{(t+1)}\rvx_i,
% \end{align*}
% where $\alpha_i^{(t+1)} = \alpha_i^{(t)}2\eta(\rvw^T\rvx - y_i)$. So instead of updating $\rvw_t$, we can update $\boldsymbol{\alpha}^{(t)}$.

% Now, $\rvw$ can be written as a linear combination of the training set. Then, we can also express the inner-product of $\rvw$ with any input $\rvx_i$ purely in terms of inner-products between training inputs:

% $$\rvw_t^T\rvx_i = \sum_{i=1}^{n}\alpha_j^{(t+1)}\rvx_j^T\rvx_i.$$
% Consequently, the entire loss function is not relying on $\rvx_j^T\rvx_i$, which can be replaced by $\langle \phi(\rvx_i), \phi(\rvx_j)\rangle$, kernel trick. 

% Note that we have transformed our learning problem that has $d$ parameters into a $m$-parameter problem. 

\section{From Feature Transformations to Kernels}
\label{sec:kernel:kernels}
Instead of computing $\phi(\rvx_i), \forall i$ , we pre-compute $\langle \phi(\rvx_i), \phi(\rvx_j)\rangle$ for all $\rvx_i$ and $\rvx_j$. Then we store the values in an $n\times n$ matrix $K$ where $K_{ij}= \langle \phi(\rvx_i), \phi(\rvx_j)\rangle$. $K$ is called the kernel matrix or Gram matrix. 

Computing $\phi(\rvx_i)$ is $\mathcal{O}(2^d)\times n = \mathcal{O}(2^dn)$. On the other hand, pre-computing the inner products can be more efficient as follows:
\begin{align*}
	\langle \phi(\rvx), \phi(\rvx')\rangle &= \phi(\rvx)^T\phi(\rvx')\\ 
										   &= \prod_{k=1}^d(1+x_kx'_k).
\end{align*}
This takes $\mathcal{O}(d)$ and we can compute the entire kernel matrix $K$ in $\mathcal{O}(dn^2)$. 

Note that all kernel methods are non-parametric models as we need to keep training data to be able to compute the kernel values between new test inputs and the training inputs.

\section{Kernels}
\label{sec:kernel:kernel_def}

$k(\cdot, \cdot)$ is a valid or well-defined kernel, if the function $k(\rvx, \rvx')$ is both 
\begin{itemize}
	\item Symmetric: $k(\rvx, \rvx') = k(\rvx', \rvx)$ for all $\rvx, \rvx'$.
	\item Positive semi-definite: $k(\cdot, \cdot)$ is PSD if for all finite subsets $\{\rvx_1,\dots,\rvx_m\}$, $\rvx_i\in \mathcal{X}$, $K$ is a PSD matrix. 
\end{itemize}

Note that 
\begin{itemize}
	\item A matrix $A\in \mathbb{R}^{m\times m}$ is PSD iff $\forall \rvq\in \mathbb{R}^m$ the following holds: $\rvq^TA\rvq\geq 0$.
	\item A symmetric matrix $A\in \mathbb{R}^{m\times m}$ is PSD iff all eigenvalues $\lambda$ of $A$ are non-negative. 
	\item A symmetric matrix $A$ is PSD if all its upper left sub-matrices have non-negative determinants.
\end{itemize}

For symmetric matrices all of the above are equivalent. 

Some of the most popular kernels are
\begin{itemize}
	\item Linear kernel: $k(\rvx, \rvx') = \rvx^T\rvx'$
	\item Polynomial kernel: $k(\rvx, \rvx') = (1+\rvx^T\rvx')^{\tilde{d}}$, where $\tilde{d}$ is the degree of the polynomial
	% \item RBF: $k(\rvx, \rvx') = ||\rvx-\rvx'||^2$
\end{itemize}

Note that some kernels such as the RBF kernel have an infinite-dimensional feature space transformation. Hence, it is impossible to compute the feature space transformation explicitly. The RBF kernel is given by
\begin{align*}
	k(\rvx, \rvx') = \exp(-\gamma \|\rvx-\rvx'\|^2)
\end{align*}
Let, $\gamma=\frac{1}{2}$, 
\begin{align*}
	k(\rvx, \rvx') &= \exp(-\frac{1}{2} \|\rvx-\rvx'\|^2)\\
				   &= \exp\bigg[-\frac{1}{2} \langle \rvx-\rvx', \rvx-\rvx' \rangle \bigg]\\
				   &= \exp\bigg[-\frac{1}{2} \langle \rvx, \rvx-\rvx' \rangle-\langle \rvx', \rvx-\rvx' \rangle\bigg]\\
				   &= \exp\bigg[-\frac{1}{2} \big(\|\rvx\|^2+\|\rvx'\|^2-2 \langle \rvx, \rvx' \rangle\big) \bigg]\\
				   &= C\exp\langle \rvx, \rvx' \rangle\\
				   &= C\sum_{n=0}^\infty \frac{\langle \rvx, \rvx' \rangle}{n!}\\
				   &= C\sum_{n=0}^\infty \frac{k_{\textrm{poly}}(\rvx, \rvx')}{n!}\\
\end{align*}
The second to last step is by Taylor expansion of $e^x$.

\subsection{Some Intuitions}

Let $y=f(\rvx)+\epsilon$. Then $f$ does not vary a lot if $\rvx$, $\rvx'$ are close enough. This can be modeled by the co-variance of the $y'$s.

Let's represent the co-variance of the outputs in terms of the co-variance of the inputs. 

$$cov(y,y') = k(\rvx, \rvx')+\underbrace{\sigma_n^2\delta_{\rvx,\rvx'}}_{\textrm{noise term}}.$$

As $y$ is a function values of $f(\rvx)$, the kernel function can be viewed as an extension of the covariance matrix $\Sigma$ of random vectors to the covariance of (random) functions. 

% \section{Kernel Machines}
% \label{sec:kernel:kernel_machines}

% There are two steps to kernelize an ML method.
% \begin{itemize}
% 	\item Rewriting learning and prediction algorithm entirely in terms of inner-products.
% 	\item Define a kernel function and substitute $k(\rvx_i, \rvx_j)$ for $\rvx_i^T\rvx_j$.
% \end{itemize}


% Kernel function: Let $\phi(\rvx)$ be a set of basis functions that map inputs $\rvx$ to a feature space. In many algorithms, this feature space only appears in a dot product $\phi(\rvx)^T\phi(\rvx')$ of input pairs $\rvx$ and $\rvx'$. Then, kernel function can be defined as $k(\rvx, \rvx') = \phi(\rvx)^T\phi(\rvx')$. Note that we only need to know $k(\rvx, \rvx')$, not $\phi(\rvx)$.

% A function $k: \mathbb{R}^d\times \mathbb{R}^d\to \mathbb{R}$ is said to be a positive semidefinite kernel if it is symmetric, (\ie $k(\rvx',\rvx)= k(\rvx,\rvx')$).

% Recall that the linear regression can be modeled as follows:
% $$f(\rvx) = \mathbf{X}\rvw,$$
% where $\rvx = [x_1, \dots, x_d]$ and $\rvw = [w_1,\dots,w_d]^T$. The ridge regression for $\mathbf{X}\in \mathbb{R}^{m\times d}$ matrix can be modeled as follows:
% \begin{align}
% 	J(\rvw) &= \|\mathbf{y}-\mathbf{X}\rvw\|^2_2 + \lambda \|\rvw\|^2_2 \\
% 			&= (\mathbf{y}-\mathbf{X}\rvw)^T(\mathbf{y}-\mathbf{X}\rvw)+\lambda\rvw^T\rvw\\
% 			&= (\mathbf{y}^T-\rvw^T\mathbf{X}^T)(\mathbf{y}-\mathbf{X}\rvw)+\lambda\rvw^T\rvw\\
% 			&= \rvy^T\rvy-\rvw^T\mathbf{X}^T\rvy-\rvy^T\mathbf{X}\rvw+\rvw^T\mathbf{X}^T\mathbf{X}\rvw+\rvw^T\lambda\mathbf{I}\rvw\\
% 	\frac{\partial J}{\partial \rvw}&= -\mathbf{X}^T\rvy-\mathbf{X}^T\rvy+\mathbf{X}^T\mathbf{X}\rvw+\mathbf{X}^T\mathbf{X}\rvw+2\lambda\mathbf{I}\rvw = 0\\
% 	\rvw	&= (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\rvy
% 	\label{eq:ridge_regression}
% \end{align}
% Let's change it into a dual form:
% \begin{align}
% 	(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\rvw	&= (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\rvy\\
% 	(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\rvw &= \mathbf{X}^T\rvy\\ 
% 	\rvw &= \lambda^{-1}\mathbf{I}(\mathbf{X}^T\rvy-\mathbf{X}^T\mathbf{X}\rvw)\\
% 	&= \mathbf{X}^T\lambda^{-1}(\rvy-\mathbf{X}\rvw)\\
% 	&= \mathbf{X}^T\alpha\\
% 	\lambda\alpha &= (\rvy-\mathbf{X}\rvw)\\
% 	&= (\rvy-\mathbf{X}\mathbf{X}^T\alpha)\\
% 	\rvy &= (\mathbf{X}\mathbf{X}^T\alpha+\lambda\alpha)\\
% 	\alpha &= (\mathbf{X}\mathbf{X}^T+\lambda)^{-1}\rvy\\
% 	\alpha &= (\mathbf{G}+\lambda)^{-1}\rvy,
% 	\label{eq:dual_regression}
% \end{align}
% where $\mathbf{G} = \mathbf{X}\mathbf{X}^T$ is a \textit{Gram matrix}. Thus, we just have to solve $m\times m$ matrix. 

% Therefore, given a new input $\rvx'\in \mathbb{R}^d$, the prediction $f(\rvx') = \rvx'\rvw $ can be expressed as follows:
% $$f(\rvx') = \rvx'\mathbf{X}^T(\mathbf{X}\mathbf{X}^T+\lambda\mathbf{I})^{-1}\rvy.$$

% Note that this formula \textbf{depends only on the data via inner products.}
% \begin{align*}
% 	\rvx'\mathbf{X}^T = 
% 	\begin{bmatrix}
% 		\langle \rvx',\rvx_1\rangle\\
% 		\vdots\\
% 		\langle \rvx',\rvx_n\rangle
% 	\end{bmatrix}^T,\quad
% 	\mathbf{X}\mathbf{X}^T = 
% 	\begin{bmatrix}
% 		\langle \rvx_1,\rvx_1\rangle& \dots& \langle \rvx_1,\rvx_n\rangle\\
% 		\vdots&\ddots&\vdots\\
% 		\langle \rvx_n,\rvx_1\rangle &\dots &\langle \rvx_n,\rvx_n\rangle
% 	\end{bmatrix}
% \end{align*}
% Therefore, we can apply the kernel trick and consider the more general prediction function. In other words, we can leverage a very large number of basis functions. 


% % Then, the closed-form solution of the ridge regression is given by
% % $$\hat{\boldsymbol{\theta}} = (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}.$$


% % We can rewrite it by $\rvw = \Phi \rva$, where $\Phi = [\phi(\rvx_1),\dots,\phi(\rvx_N)]$, $\rva = [a_1,\dots,a_N]^T$, and $a_n =-\frac{1}{\lambda} (\rvw^T\phi(\rvx_n)-y_n).$. Then, we get a dual objective, which aims to minimize $E$ with respect to $\rva$.

% % Let $K = \Phi^T\Phi$ be the Gram matrix. 

