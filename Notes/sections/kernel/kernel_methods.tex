\chapter{Introduction to Kernel Methods}

In machine learning a \textit{kernel} is a function that measures similarity between two objects and, at the same time, implicitly maps those objects into (possibly very-high-dimensional) feature space without ever computing the coordinates of that space directly.


\section{Kernels}
\begin{align*}
	\hat{\boldsymbol{\theta}} = \mathbf{X}^T(\mathbf{X}\mathbf{X}^T+\lambda\mathbf{I})^{-1}\rvy.
\end{align*}
By substituting into $\hat{y}(\rvx') = \hat{\boldsymbol{\theta}}^T\rvx' = (\rvx')^T\hat{\boldsymbol{\theta}}$ gives
\begin{align*}
	\hat{y}(\rvx') = (\rvx')^T\hat{\boldsymbol{\theta}} = (\rvx')^T\mathbf{X}^T(\mathbf{X}\mathbf{X}^T+\lambda\mathbf{I})^{-1}\rvy.
\end{align*}
The prediction depends on the data only though inner products, since
\begin{align*}
	(\rvx')^T\mathbf{X}^T = \begin{bmatrix}
		\langle \rvx', \rvx_1 \rangle\\
		\vdots\\
		\langle \rvx', \rvx_n \rangle
	\end{bmatrix}, \quad
	\mathbf{X}\mathbf{X}^T = \begin{bmatrix}
		\langle \rvx_1, \rvx_1 \rangle & \cdots & \langle \rvx_1, \rvx_n \rangle\\
		\vdots & \ddots & \vdots\\
		\langle \rvx_n, \rvx_1 \rangle & \cdots & \langle \rvx_n, \rvx_n \rangle
	\end{bmatrix},
\end{align*}
\textbf{Kernel trick} replaces the inner products by kernel evaluations, which can be represented as follows:
\begin{align*}
	\hat{y}(\rvx') = k(\rvx')(K+\lambda \mathbf{I})^{-1}\rvy,
\end{align*}
where 
\begin{align*}
	k(\rvx') = \begin{bmatrix}
		k(\rvx', \rvx_1)\\
		\vdots\\
		k(\rvx', \rvx_n)
	\end{bmatrix}
	K = \begin{bmatrix}
		k(\rvx_1, \rvx_1) & \cdots & k(\rvx_1, \rvx_n)\\
		\vdots & \ddots & \vdots\\
		k(\rvx_n, \rvx_1) & \cdots & k(\rvx_n, \rvx_n)
	\end{bmatrix},
\end{align*}

\subsection{Non-Parametric Regression}
Kernel methods corresponding to infinite-dimensional $\phi(\rvx)$ can usually be considered as \textbf{non-parametric}.
For instance, 1-D case can be considered as a joining the dots by interpolation. 

\subsection{Bayesian Posterior Derivation}

Both $v(\rvx) = \langle \boldsymbol{\theta}, \rvx \rangle$ and $v(\rvx') = \langle \boldsymbol{\theta}, \rvx' \rangle$ have mean zero, so their covariance is 
\begin{align*}
	Cov[v(\rvx), v(\rvx')] &= \mathbb{E}[(\rvx^T\boldsymbol{\theta})(\rvx')^T\boldsymbol{\theta}] = \mathbb{E}[(\rvx^T\boldsymbol{\theta})(\boldsymbol{\theta}^T\rvx')]\\
						   &= \rvx^T\mathbb{E}[\boldsymbol{\theta}\boldsymbol{\theta}^T]\rvx' = \langle \boldsymbol{\theta}, \rvx \rangle
\end{align*}
since $\mathbb{E}[\boldsymbol{\theta}\boldsymbol{\theta}^T] = \mathbf{I}$ for $\boldsymbol{\theta}\sim \mathcal{N}(\mathbf{0}, \mathbf{I})$. 









% We will define the kind of kernels (so-called (\textit{Mercer}) kernels) that are used in kernel methods. Those kernels are covariance functions that have to be \textit{symmetric} and \textit{positive(-semi) definite}.

% Basic idea: we can make linear ML models non-linear by applying basis function (feature) transformations on the input space. For an input vector $\mathbf{x}\in \mathbb{R}^d$, do transformation $\mathbf{x}\to \phi(\rvx)$ where $\phi(\rvx)\in \mathbb{R}^D$. Usually, $D\gg d$, because we add dimensions that capture non-linear interactions among features. 

% \begin{itemize}
% 	\item The main idea is to use large set of fixed non-linear basis functions.
% 	\item The complexity depends on number of basis functions, but dual trick changes it to a size of dataset. 
% 	\item The kernel function is a measure of similarity between $\rvx_i$and $\rvx_j$ 
% 	\item The key idea of kernel method is to represent the raw data based on a similarity function called kernel function over pairs of data points. 
% \end{itemize}


% \section{Kernel Trick}
% \label{sec:kernel:kernel_trick}

% Kernel trick aims to avoid computing explicit feature transformations $\phi(\rvx)$ by changing the entire ML algorithm to use only \textit{inner products} of feature transformations $\langle \phi(\rvx), \phi(\rvx')\rangle$ which are computed directly using a \textit{kernel} (rather than the feature transformations themselves.). Note that even the RBF kernel can be expressed as an inner product. 

% \subsection{Linear Model}

% Consider a linear model where we use gradient descent with a squared loss function to learn the parameters. So, we have $h(\rvx) = \mathbf{w}^T\mathbf{x}$ and the squared loss function:
% $$l(\rvw) = \sum_{i=1}^{n}(\rvw^T\rvx - y_i)^2.$$

% The gradient w.r.t., $\rvw$ is given by 

% $$g(\rvw) = \frac{\partial l}{\partial \rvw} = \sum_{i=1}^{n}2(\rvw^T\rvx - y_i)\rvx_i.$$
% Since the loss is convex, the final solution is independent of the initialization, and we can initialize $\rvw_0$ to be whatever we want. For convenience, we choose $\rvw_0=\mathbf{0}.$

% Recall that the gradient update rule is given by
% $$\rvw_{t+1} = \rvw_t-\eta g(\rvw_t),$$
% where $\eta$ is the step size. We can show that for every $t$, $\rvw_t$ is a linear combination of $\rvx_i$ by setting 
% $$\rvw_t = \sum_{i=1}^{n}\alpha_i^{(t)}\rvx_i.$$
% Then,
% \begin{align*}
% 	\rvw_{t+1} &= \rvw_t - \eta g(w)\\
% 			   &= \sum_{i=1}^{n}\alpha_i^{(t)}\rvx_i-\eta\sum_{i=1}^{n}2(\rvw^T\rvx - y_i)\rvw_i\\
% 			   &= \sum_{i=1}^{n}\alpha_i^{(t+1)}\rvx_i,
% \end{align*}
% where $\alpha_i^{(t+1)} = \alpha_i^{(t)}2\eta(\rvw^T\rvx - y_i)$. So instead of updating $\rvw_t$, we can update $\boldsymbol{\alpha}^{(t)}$.

% Now, $\rvw$ can be written as a linear combination of the training set. Then, we can also express the inner-product of $\rvw$ with any input $\rvx_i$ purely in terms of inner-products between training inputs:

% $$\rvw_t^T\rvx_i = \sum_{i=1}^{n}\alpha_j^{(t+1)}\rvx_j^T\rvx_i.$$
% Consequently, the entire loss function is not relying on $\rvx_j^T\rvx_i$, which can be replaced by $\langle \phi(\rvx_i), \phi(\rvx_j)\rangle$, kernel trick. 

% Note that we have transformed our learning problem that has $d$ parameters into a $m$-parameter problem. 

\section{From Feature Transformations to Kernels}
\label{sec:kernel:kernels}
Instead of computing $\phi(\rvx_i), \forall i$ , we pre-compute $\langle \phi(\rvx_i), \phi(\rvx_j)\rangle$ for all $\rvx_i$ and $\rvx_j$. Then we store the values in an $n\times n$ matrix $K$ where $K_{ij}= \langle \phi(\rvx_i), \phi(\rvx_j)\rangle$. $K$ is called the kernel matrix or Gram matrix. 

Computing $\phi(\rvx_i)$ is $\mathcal{O}(2^d)\times n = \mathcal{O}(2^dn)$. On the other hand, pre-computing the inner products can be more efficient as follows:
\begin{align*}
	\langle \phi(\rvx), \phi(\rvx')\rangle &= \phi(\rvx)^T\phi(\rvx')\\ 
										   &= \prod_{k=1}^d(1+x_kx'_k).
\end{align*}
This takes $\mathcal{O}(d)$ and we can compute the entire kernel matrix $K$ in $\mathcal{O}(dn^2)$. 

Note that all kernel methods are non-parametric models as we need to keep training data to be able to compute the kernel values between new test inputs and the training inputs.

\section{Kernels}
\label{sec:kernel:kernel_def}

$k(\cdot, \cdot)$ is a valid or well-defined kernel, if the function $k(\rvx, \rvx')$ is both 
\begin{itemize}
	\item Symmetric: $k(\rvx, \rvx') = k(\rvx', \rvx)$ for all $\rvx, \rvx'$.
	\item Positive semi-definite: $k(\cdot, \cdot)$ is PSD if for all finite subsets $\{\rvx_1,\dots,\rvx_m\}$, $\rvx_i\in \mathcal{X}$, $K$ is a PSD matrix. 
\end{itemize}

Note that 
\begin{itemize}
	\item A matrix $A\in \mathbb{R}^{m\times m}$ is PSD iff $\forall \rvq\in \mathbb{R}^m$ the following holds: $\rvq^TA\rvq\geq 0$.
	\item A symmetric matrix $A\in \mathbb{R}^{m\times m}$ is PSD iff all eigenvalues $\lambda$ of $A$ are non-negative. 
	\item A symmetric matrix $A$ is PSD if all its upper left sub-matrices have non-negative determinants.
\end{itemize}

For symmetric matrices all of the above are equivalent. 

Some of the most popular kernels are
\begin{itemize}
	\item Linear kernel: $k(\rvx, \rvx') = \rvx^T\rvx'$
	\item Polynomial kernel: $k(\rvx, \rvx') = (1+\rvx^T\rvx')^{\tilde{d}}$, where $\tilde{d}$ is the degree of the polynomial
	% \item RBF: $k(\rvx, \rvx') = ||\rvx-\rvx'||^2$
\end{itemize}

Note that some kernels such as the RBF kernel have an infinite-dimensional feature space transformation. Hence, it is impossible to compute the feature space transformation explicitly. The RBF kernel is given by
\begin{align*}
	k(\rvx, \rvx') = \exp(-\gamma \|\rvx-\rvx'\|^2)
\end{align*}
Let, $\gamma=\frac{1}{2}$, 
\begin{align*}
	k(\rvx, \rvx') &= \exp(-\frac{1}{2} \|\rvx-\rvx'\|^2)\\
				   &= \exp\bigg[-\frac{1}{2} \langle \rvx-\rvx', \rvx-\rvx' \rangle \bigg]\\
				   &= \exp\bigg[-\frac{1}{2} \langle \rvx, \rvx-\rvx' \rangle-\langle \rvx', \rvx-\rvx' \rangle\bigg]\\
				   &= \exp\bigg[-\frac{1}{2} \big(\|\rvx\|^2+\|\rvx'\|^2-2 \langle \rvx, \rvx' \rangle\big) \bigg]\\
				   &= C\exp\langle \rvx, \rvx' \rangle\\
				   &= C\sum_{n=0}^\infty \frac{\langle \rvx, \rvx' \rangle}{n!}\\
				   &= C\sum_{n=0}^\infty \frac{k_{\textrm{poly}}(\rvx, \rvx')}{n!}\\
\end{align*}
The second to last step is by Taylor expansion of $e^x$.

\subsection{Some Intuitions}

Let $y=f(\rvx)+\epsilon$. Then $f$ does not vary a lot if $\rvx$, $\rvx'$ are close enough. This can be modeled by the co-variance of the $y'$s.

Let's represent the co-variance of the outputs in terms of the co-variance of the inputs. 

$$cov(y,y') = k(\rvx, \rvx')+\underbrace{\sigma_n^2\delta_{\rvx,\rvx'}}_{\textrm{noise term}}.$$

As $y$ is a function values of $f(\rvx)$, the kernel function can be viewed as an extension of the covariance matrix $\Sigma$ of random vectors to the covariance of (random) functions. 

% \section{Kernel Machines}
% \label{sec:kernel:kernel_machines}

% There are two steps to kernelize an ML method.
% \begin{itemize}
% 	\item Rewriting learning and prediction algorithm entirely in terms of inner-products.
% 	\item Define a kernel function and substitute $k(\rvx_i, \rvx_j)$ for $\rvx_i^T\rvx_j$.
% \end{itemize}


% Kernel function: Let $\phi(\rvx)$ be a set of basis functions that map inputs $\rvx$ to a feature space. In many algorithms, this feature space only appears in a dot product $\phi(\rvx)^T\phi(\rvx')$ of input pairs $\rvx$ and $\rvx'$. Then, kernel function can be defined as $k(\rvx, \rvx') = \phi(\rvx)^T\phi(\rvx')$. Note that we only need to know $k(\rvx, \rvx')$, not $\phi(\rvx)$.

% A function $k: \mathbb{R}^d\times \mathbb{R}^d\to \mathbb{R}$ is said to be a positive semidefinite kernel if it is symmetric, (\ie $k(\rvx',\rvx)= k(\rvx,\rvx')$).

% Recall that the linear regression can be modeled as follows:
% $$f(\rvx) = \mathbf{X}\rvw,$$
% where $\rvx = [x_1, \dots, x_d]$ and $\rvw = [w_1,\dots,w_d]^T$. The ridge regression for $\mathbf{X}\in \mathbb{R}^{m\times d}$ matrix can be modeled as follows:
% \begin{align}
% 	J(\rvw) &= \|\mathbf{y}-\mathbf{X}\rvw\|^2_2 + \lambda \|\rvw\|^2_2 \\
% 			&= (\mathbf{y}-\mathbf{X}\rvw)^T(\mathbf{y}-\mathbf{X}\rvw)+\lambda\rvw^T\rvw\\
% 			&= (\mathbf{y}^T-\rvw^T\mathbf{X}^T)(\mathbf{y}-\mathbf{X}\rvw)+\lambda\rvw^T\rvw\\
% 			&= \rvy^T\rvy-\rvw^T\mathbf{X}^T\rvy-\rvy^T\mathbf{X}\rvw+\rvw^T\mathbf{X}^T\mathbf{X}\rvw+\rvw^T\lambda\mathbf{I}\rvw\\
% 	\frac{\partial J}{\partial \rvw}&= -\mathbf{X}^T\rvy-\mathbf{X}^T\rvy+\mathbf{X}^T\mathbf{X}\rvw+\mathbf{X}^T\mathbf{X}\rvw+2\lambda\mathbf{I}\rvw = 0\\
% 	\rvw	&= (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\rvy
% 	\label{eq:ridge_regression}
% \end{align}
% Let's change it into a dual form:
% \begin{align}
% 	(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\rvw	&= (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\rvy\\
% 	(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\rvw &= \mathbf{X}^T\rvy\\ 
% 	\rvw &= \lambda^{-1}\mathbf{I}(\mathbf{X}^T\rvy-\mathbf{X}^T\mathbf{X}\rvw)\\
% 	&= \mathbf{X}^T\lambda^{-1}(\rvy-\mathbf{X}\rvw)\\
% 	&= \mathbf{X}^T\alpha\\
% 	\lambda\alpha &= (\rvy-\mathbf{X}\rvw)\\
% 	&= (\rvy-\mathbf{X}\mathbf{X}^T\alpha)\\
% 	\rvy &= (\mathbf{X}\mathbf{X}^T\alpha+\lambda\alpha)\\
% 	\alpha &= (\mathbf{X}\mathbf{X}^T+\lambda)^{-1}\rvy\\
% 	\alpha &= (\mathbf{G}+\lambda)^{-1}\rvy,
% 	\label{eq:dual_regression}
% \end{align}
% where $\mathbf{G} = \mathbf{X}\mathbf{X}^T$ is a \textit{Gram matrix}. Thus, we just have to solve $m\times m$ matrix. 

% Therefore, given a new input $\rvx'\in \mathbb{R}^d$, the prediction $f(\rvx') = \rvx'\rvw $ can be expressed as follows:
% $$f(\rvx') = \rvx'\mathbf{X}^T(\mathbf{X}\mathbf{X}^T+\lambda\mathbf{I})^{-1}\rvy.$$

% Note that this formula \textbf{depends only on the data via inner products.}
% \begin{align*}
% 	\rvx'\mathbf{X}^T = 
% 	\begin{bmatrix}
% 		\langle \rvx',\rvx_1\rangle\\
% 		\vdots\\
% 		\langle \rvx',\rvx_n\rangle
% 	\end{bmatrix}^T,\quad
% 	\mathbf{X}\mathbf{X}^T = 
% 	\begin{bmatrix}
% 		\langle \rvx_1,\rvx_1\rangle& \dots& \langle \rvx_1,\rvx_n\rangle\\
% 		\vdots&\ddots&\vdots\\
% 		\langle \rvx_n,\rvx_1\rangle &\dots &\langle \rvx_n,\rvx_n\rangle
% 	\end{bmatrix}
% \end{align*}
% Therefore, we can apply the kernel trick and consider the more general prediction function. In other words, we can leverage a very large number of basis functions. 


% % Then, the closed-form solution of the ridge regression is given by
% % $$\hat{\boldsymbol{\theta}} = (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}.$$


% % We can rewrite it by $\rvw = \Phi \rva$, where $\Phi = [\phi(\rvx_1),\dots,\phi(\rvx_N)]$, $\rva = [a_1,\dots,a_N]^T$, and $a_n =-\frac{1}{\lambda} (\rvw^T\phi(\rvx_n)-y_n).$. Then, we get a dual objective, which aims to minimize $E$ with respect to $\rva$.

% % Let $K = \Phi^T\Phi$ be the Gram matrix. 

