\section{Direct Preference Optimization}
\label{sec:nlp_dpo}

\begin{align}
	\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, w_l)\sim \mathcal{D}}\bigg[\log \sigma \bigg(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)}-\beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\bigg)\bigg].
	\label{eq:dpo_loss}
\end{align}
