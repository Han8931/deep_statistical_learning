\section{Evaluation Metrics}
\label{sec:nlp_eval_metrics}

In the context of a bigram model, likelihood refers to the probability of observing a sequence of words in a corpus based on the bigram model's parameters. The bigram model assumes that the probability of a word in a sequence depends only on the immediately preceding word, making it a **Markov model of order 1**.

A bigram model represents the probability of a word sequence \( W = (w_1, w_2, \ldots, w_n) \) as a product of conditional probabilities:
\[
P(W) = P(w_1) \prod_{i=2}^n P(w_i | w_{i-1})
\]
Here, \( P(w_i | w_{i-1}) \) is the conditional probability of word \( w_i \) given the preceding word \( w_{i-1} \).

The **likelihood** of a sequence of words (data) \( W = (w_1, w_2, \ldots, w_n) \) under the bigram model is:
\[
L(\theta | W) = P(W | \theta) = P(w_1 | \theta) \prod_{i=2}^n P(w_i | w_{i-1}, \theta)
\]
Here, \( \theta \) represents the model parameters, specifically the probabilities \( P(w_i | w_{i-1}) \).

\paragraph{Maximum Likelihood Estimation (MLE) in the Bigram Model}

To find the parameters \( P(w_i | w_{i-1}) \) that maximize the likelihood of the observed data, the **maximum likelihood estimation (MLE)** approach is commonly used.

The bigram probabilities \( P(w_i | w_{i-1}) \) are estimated from the frequency counts in the training corpus:
\[
P(w_i | w_{i-1}) = \frac{\text{Count}(w_{i-1}, w_i)}{\text{Count}(w_{i-1})}
\]
where:
\begin{itemize}
	\item \( \text{Count}(w_{i-1}, w_i) \) is the number of times the bigram \( (w_{i-1}, w_i) \) appears in the corpus.
	\item \( \text{Count}(w_{i-1}) \) is the number of times the word \( w_{i-1} \) appears.
\end{itemize}

To simplify computations, the log-likelihood of the sequence is used:
\[
\log L(\theta | W) = \log P(w_1 | \theta) + \sum_{i=2}^n \log P(w_i | w_{i-1}, \theta)
\]

\begin{itemize}
	\item Recall: TP/(TP+FN). Find all relevant cases whithin a dataset.
	\item Precision: TP/(TP+FP): While recall expresses the ability to find all relevant instances in a dataset, precision expresses the proportion of the data points our model says was relevant actually were relevant.
	\item The F1 score is the harmonic mean of precision and recall taking both metrics into account in the following equation:
\end{itemize}

\subsection{Perplexity}

Intuitively, perplexity can be understood as a \textit{measure of uncertainty}. The perplexity of a language model can be seen as the level of perplexity. Consider a language model with an entropy of three bits, in which each bit encodes two possible outcomes of equal probability. This means that when predicting a symbol, that language model has to choose among $2^3=8$ possible options. Thus, we can argue that this language model has a perplexity of 8.

It can be modeled as $2^H(P,Q)$:
\begin{align*}
	PPL(W) &= P(w_1,\cdots, w_N)^{-\frac{1}{N}}\\
	&\approx \Bigg(\prod_{i=1}^N P(w_i|w_{<i})\Bigg)^{-\frac{1}{N}}\\
	&= \sqrt[n]{\frac{1}{\prod_{i=1}^{N} P(w_{i}|w_{<i})}}
\end{align*}

Let's derive it from a cross-entropy. We want to optimize $P_\theta$ instead of the true distribution $P$:
\begin{align}
	% H & \approx -\sum_{i=1}^{N} \log P(w_{i}|w_{<i})\\
	\mathcal{L}_{CE} &= -\mathbb{E}_{w\sim P} [P_\theta(w_{i}|w_{<i})]\\
	&\approx -\frac{1}{N} \sum_{i=1}^{N} \log P_\theta(w_{i}|w_{<i})\\
	&= -\frac{1}{N} \log \prod_{i=1}^{N} P_\theta(w_{i}|w_{<i})\\
	&=  \log \Bigg(\prod_{i=1}^{N} P_\theta(w_{i}|w_{<i}) \Bigg)^{-\frac{1}{N}}\\
	&=  \log \sqrt[N]{\frac{1}{\prod_{i=1}^{N} P_\theta(w_{i}|w_{<i})}}\\
	\label{eq:ppl_entropy}
\end{align}
Thus, $PPL(W) = \exp\Big(\mathcal{L}_{CE}\Big).$

\subsection{Cross-Entropy and Perplexity}
\begin{align*}
	H(P,Q) &= -\sum_{x}P(x)\log Q(x) \\
	&= -\sum_{x}P(x) [\log P(x) + \log Q(x) - \log P(x)] \\
	&= -\sum_{x}P(x)\Bigg[\log P(x) + \log\frac{Q(x)}{P(x)}\Bigg] \\
	&= H(P) + D_{KL}(P||Q)
\end{align*}
It should be noted that since the empirical entropy $H(P)$ is unoptimizable, when we train a language model with the objective of minimizing the cross entropy loss, the true objective is to minimize the $KL$-divergence of the distribution, which was learned by our language model from the empirical distribution of the language.
