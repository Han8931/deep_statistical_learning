\include{./sections/nlp/attention}
   
% \subsection{Skip Connection}
% This is a regularization technique.

\section{Positional Embedding}

The self-attention mechanism in transformers treats all tokens in a sequence in parallel without an inherent notion of order. This means that, by itself, self-attention is invariant to the order of input tokens. Positional encoding is introduced to inject order information so that the model can differentiate between tokens based on their positions.
\begin{itemize}
	\item \textbf{Absolute Positional Embeddings}: Each position in the sequence is assigned a unique vector. Although straightforward, these embeddings don't scale well to longer sequences and fail to capture the nuances of relative positions between tokens.
	\item \textbf{Relative Positional Embeddings}: These embeddings focus on the distance between tokens, which can improve the model’s understanding of token relationships. However, they typically introduce additional complexity into the model architecture.
\end{itemize}

\subsection{Permutation Invariance of Self-Attention}

Without positional embeddings, the transformer's self-attention would treat the input as a bag of tokens, ignoring the order entirely. The added positional embeddings break this permutation invariance by encoding the position directly into the token representation. As a result, even if the same tokens are present, the model can infer their relative order and roles in the sentence.

Imagine you have a short sentence, ``I feel good''. If you simply pass this sentence into a transformer, the model wouldn't know the order of the words. The same set of token (\ie word) embeddings could represent the sentence ``good feel I'' if no ordering information were provided.

We formulate this as follows: With a permutation matrix \( P \) of shape \( (n, n) \), the input tokens can be permuted as 
\[
X' = P X.
\]
Let's follow the same self-attention process for \( X' \):
\[
Q' = X'W_q = P X W_q = P Q,
\]
\[
K' = X'W_k = P X W_k = P K,
\]
\[
V' = X'W_v = P X W_v = P V.
\]
Then, compute the scores using \( Q' \) and \( K' \):
\[
S' = \frac{Q'(K')^T}{\sqrt{d_k}} = \frac{(P Q)(P K)^T}{\sqrt{d_k}}.
\]
   
Note that
\[
(P K)^T = K^T P^T,
\]
so
\[
S' = \frac{P Q K^T P^T}{\sqrt{d_k}} = P \, S \, P^T.
\]

The softmax is applied row-wise. 
\[
A' = \text{Softmax}(S').
\]
   
Since \( P \) and \( P^T \) are just reordering rows and columns, respectively, the attention weights $A$ are simply permuted like below:
\[
A' = P \, A \, P^T.
\]
Finally, the output for the permuted input is:
\[
\text{Attention}(X') = A' V' = (P A P^T)(P V) = P A V = P \, \text{Attention}(X).
\]
As you can see the self-attention mechanism is \textit{equivariant} to permutations. This means that if you permute the input tokens, the output is permuted in the same way. There is no mechanism in the equations above that distinguishes one ordering from another; the operations treat all tokens symmetrically.

Thus, without additional positional encodings, if you were to shuffle the tokens, the model would compute the same set of pairwise interactions—just in a different order. The structure of the equations does not provide any mechanism for the model to know that one token came before or after another.

\subsection{Sinusoidal Positional Encoding}

In a Transformer model, positional encoding vectors are added to the token (word) embeddings before the input is fed into the self-attention layers. This addition gives the model a sense of the order in the sequence, enabling it to capture the sequential relationships between tokens despite processing them in parallel.

\begin{itemize}
	\item $\text{PE}(pos, 2i) &= \sin\Bigg(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\Bigg)$
	\item $\text{PE}(pos, 2i+1) &= \cos\Bigg(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\Bigg)$,
\end{itemize}
where:
\begin{itemize}
	\item \( pos \): the position of the token in the sequence (starting at 0),
	\item \( i \): the index along the embedding dimension,
	\item \( d_{\text{model}} \): the total dimension of the model’s embeddings.
\end{itemize}

Let's work through a concrete example with a very small embedding dimension:
\begin{itemize}
	\item Assume \( d_{\text{model}} = 4 \).  
	\item We’ll compute the positional encoding for two positions: \( pos = 0 \) and \( pos = 1 \).
\end{itemize}

Since \( d_{\text{model}} = 4 \), we have 4 dimensions indexed \(0, 1, 2, 3\). The formulas split the dimensions into even and odd indices:

\paragraph{For \( pos = 0 \)}
\begin{itemize}
\item Dimension 0 (even index, \( i = 0 \)):
 \[
 \text{PE}(0,0) = \sin\Bigl(\frac{0}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \sin\Bigl(\frac{0}{10000^0}\Bigr) = \sin(0) = 0.
 \]
\item Dimension 1 (odd index, \( i = 0 \)):
 \[
 \text{PE}(0,1) = \cos\Bigl(\frac{0}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \cos(0) = 1.
 \]
\item Dimension 2 (even index, \( i = 1 \)):
 \[
 \text{PE}(0,2) = \sin\Bigl(\frac{0}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \sin\Bigl(\frac{0}{10000^{0.5}}\Bigr) = \sin(0) = 0.
 \]

\item Dimension 3 (odd index, \( i = 1 \)):
 \[
 \text{PE}(0,3) = \cos\Bigl(\frac{0}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \cos(0) = 1.
 \]
\item So, the positional encoding for \( pos = 0 \) is:
\[
[0,\, 1,\, 0,\, 1].
\]
\end{itemize}

\paragraph{For \( pos = 1 \)}
\begin{itemize}
\item Dimension 0 (even index, \( i = 0 \)):
\[
\text{PE}(1,0) = \sin\Bigl(\frac{1}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \sin\Bigl(\frac{1}{10000^0}\Bigr) = \sin(1) \approx 0.84147.
\]

\item Dimension 1 (odd index, \( i = 0 \)):
\[
\text{PE}(1,1) = \cos\Bigl(\frac{1}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \cos(1) \approx 0.54030.
\]
\item Dimension 2 (even index, \( i = 1 \)):
\[
\text{PE}(1,2) = \sin\Bigl(\frac{1}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \sin\Bigl(\frac{1}{10000^{0.5}}\Bigr) = \sin\Bigl(\frac{1}{100}\Bigr) = \sin(0.01) \approx 0.00999983.
\]

\item Dimension 3 (odd index, \( i = 1 \)):
\[
\text{PE}(1,3) = \cos\Bigl(\frac{1}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \cos(0.01) \approx 0.99995.
\]
\item Thus, the positional encoding for \( pos = 1 \) is approximately:
\[
[0.84147,\, 0.54030,\, 0.00999983,\, 0.99995].
\]
\end{itemize}


\subsection{RoPE}

Rather than adding a positional vector to the token embeddings, \textbf{RoPE rotates the embeddings by a position-specific angle.} Think of it as ``twisting'' the embedding in space based on its position. For instance, if you have a simple two-dimensional embedding for the word ``dog'', you can imagine its vector being rotated by an angle \(\theta\) if it's the first word, \(2\theta\) if it’s the second word, and so on.

For high-dimensional embeddings (\eg in \(\mathbb{R}^d\)), RoPE divides the vector into \(d/2\) pairs (or 2D subspaces). For a token at position \(i\), denote its embedding by:
\[
\mathbf{x}_i \in \mathbb{R}^d.
\]
We partition \(\mathbf{x}_i\) into pairs:
\[
\mathbf{x}_i^{(k)} = 
\begin{pmatrix}
x_{i,2k} \\
x_{i,2k+1}
\end{pmatrix},\quad k = 0, 1, \ldots, \frac{d}{2}-1.
\]
Subsequently, for each 2D subspace indexed by \(k\), RoPE defines a rotation angle:
\[
\theta_{i, k} = i \cdot \alpha_k,
\]
where \(\alpha_k\) is a scaling factor that typically depends on the dimension \(k\). A popular choice is:
\[
\alpha_k = \frac{1}{10000^{\frac{2k}{d}}}.
\]
This scaling mimics the frequency scaling in sinusoidal embeddings (\ie positional embedding), ensuring that different subspaces capture positional information at different granularities.

In two-dimensional geometry, any rotation by an angle \(\theta\) can be represented by a \textit{rotation matrix} \(R(\theta)\). This matrix is a standard tool in linear algebra and has the form:
\[
R(\theta) = 
\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix}.
\]
\begin{itemize}
	\item Geometric Interpretation: The rotation matrix \(R(\theta)\) rotates any 2D vector by the angle \(\theta\) (in the counterclockwise direction) while preserving its magnitude. This property makes it ideal for encoding positional shifts—rotating a vector does not change its ``content'' (its norm) but changes its direction, thereby encoding positional information.
	\item Inherent Relative Encoding: When different positions correspond to different rotation angles, the relationship between any two positions can be captured by the difference in their rotation angles. This leads to a natural encoding of relative position without having to explicitly compute or store separate relative position vectors.
\end{itemize}
\textbf{However, Transformer embeddings are typically high-dimensional}. Thus, RoPE adopts a simple trick. They split the high dimensional vector into two halves so that each pair forms a 2D subspace. Thus, we can apply the rotation matrix.

For each 2D subspace, the rotary transformation is applied as follows. Given the subvector:
\[
\mathbf{x}_i^{(k)} = 
\begin{pmatrix}
x_{i,2k} \\
x_{i,2k+1}
\end{pmatrix},
\]
we compute its rotated version:
\[
\text{RoPE}_i \bigl(\mathbf{x}_i^{(k)}\bigr)
= R(\theta_{i, k})\, \mathbf{x}_i^{(k)}
= \begin{pmatrix}
\cos(\theta_{i, k}) & -\sin(\theta_{i, k}) \\
\sin(\theta_{i, k}) & \cos(\theta_{i, k})
\end{pmatrix}
\begin{pmatrix}
x_{i,2k} \\
x_{i,2k+1}
\end{pmatrix}.
\]
This yields the updated coordinates:
\[
\begin{aligned}
\tilde{x}_{i, 2k} &= x_{i,2k}\cos(\theta_{i,k}) - x_{i,2k+1}\sin(\theta_{i,k}),\\[6pt]
\tilde{x}_{i, 2k+1} &= x_{i,2k+1}\cos(\theta_{i,k}) + x_{i,2k}\sin(\theta_{i,k}).
\end{aligned}
\]
After processing all \(d/2\) subspaces, we concatenate the results back into a full \(d\)-dimensional vector \(\tilde{\mathbf{x}}_i\).

In transformer architectures, RoPE is applied to both the query and the key vectors except the value vecotrs:
\[
\begin{aligned}
\tilde{\mathbf{q}}_i &= \text{RoPE}_i(\mathbf{q}_i), \\
\tilde{\mathbf{k}}_j &= \text{RoPE}_j(\mathbf{k}_j).
\end{aligned}
\]

Then, the attention score between positions \(i\) and \(j\) is calculated as:
\[
\text{Attention}(i, j) = \frac{\tilde{\mathbf{q}}_i \cdot \tilde{\mathbf{k}}_j}{\sqrt{d}}.
\]

A key property of RoPE is that the dot product between the rotated vectors can be reinterpreted to show how relative positions are encoded. In particular, one can derive that:
\[
\tilde{\mathbf{q}}_i^\top \tilde{\mathbf{k}}_j = \mathbf{q}_i^\top\, \mathbf{M}(i,j)\, \mathbf{k}_j,
\]
where \(\mathbf{M}(i,j)\) is a block diagonal matrix that encapsulates the effect of the relative positional difference \(j-i\).

Focus on one 2D subspace (indexed by \(k\)):
\begin{itemize}
	\item The query subvector at position \(i\) is rotated by \(\theta_{i,k} = i\alpha_k\).
	\item The key subvector at position \(j\) is rotated by \(\theta_{j,k} = j\alpha_k\).
\end{itemize}

The dot product in this subspace is:
\[
\tilde{\mathbf{q}}_i^{(k)\top} \tilde{\mathbf{k}}_j^{(k)} 
= \left(\mathbf{q}_i^{(k)}\right)^\top R(\theta_{i,k})^\top R(\theta_{j,k}) \, \mathbf{k}_j^{(k)}.
\]
Since the transpose of a rotation matrix is its inverse (i.e., \(R(\theta)^\top = R(-\theta)\)), we have:
\[
R(\theta_{i,k})^\top R(\theta_{j,k}) = R(-\theta_{i,k})R(\theta_{j,k}) = R\bigl(\theta_{j,k} - \theta_{i,k}\bigr).
\]
Because \(\theta_{j,k} - \theta_{i,k} = (j-i)\alpha_k\), the transformation becomes:
\[
R\bigl((j-i)\alpha_k\bigr).
\]

Repeating this for each 2D subspace results in a block diagonal matrix:
\[
\mathbf{M}(i,j) = \text{diag}\Bigl(
R\bigl((j-i)\alpha_0\bigr),\,
R\bigl((j-i)\alpha_1\bigr),\,
\ldots,\,
R\bigl((j-i)\alpha_{\frac{d}{2}-1}\bigr)
\Bigr).
\]
Each block is the 2×2 rotation matrix:
\[
R\bigl((j-i)\alpha_k\bigr)
= \begin{pmatrix}
\cos\bigl((j-i)\alpha_k\bigr) & -\sin\bigl((j-i)\alpha_k\bigr) \\
\sin\bigl((j-i)\alpha_k\bigr) & \cos\bigl((j-i)\alpha_k\bigr)
\end{pmatrix}.
\]

\begin{itemize}
	\item Relative Positional Bias:  
  The matrix \(\mathbf{M}(i,j)\) adjusts the dot product between queries and keys based on their relative positions \((j-i)\). Thus, the value vectors are not modified. Instead of explicitly adding a relative position embedding, the rotation inherently modulates the interaction between tokens.
	\item Unified Encoding: Since \(\mathbf{M}(i,j)\) is built from standard rotation matrices \(R(\theta)\), it seamlessly encodes the relative positional difference across all 2D subspaces. This results in a unified treatment where both absolute and relative positional cues are embedded into the attention calculation.
	\item Elegant Mathematical Foundation: The use of \(R(\theta)\) comes directly from classical geometry and linear algebra. It leverages the well-known properties of rotations in 2D—specifically, that rotations preserve vector norms and that the composition of rotations is itself a rotation (with the angle being the sum or difference of the individual angles). This mathematical elegance translates into an efficient and effective mechanism for positional encoding.
\end{itemize}

% \paragraph{Summary}
% \begin{itemize}
% 	\item Stability of Vectors: Adding tokens at the end of a sentence doesn't affect the vectors for words at the beginning, facilitating efficient caching.
% 	\item Preservation of Relative Positions: If two words, say ``pig'' and ``dog,'' maintain the same relative distance in different contexts, their vectors are rotated by the same amount. This ensures that the angle, and consequently the dot product between these vectors, remains constant
% \end{itemize}

% \begin{itemize}
% 	\item Rotary Positional Encoding (RoPE) rotates the token embeddings by a position-dependent angle rather than simply adding a positional vector.
% 	\item Decomposition into 2D Subspaces: The \(d\)-dimensional embedding is split into \(d/2\) pairs. For each pair (treated as a 2D vector), a rotation is applied.
% 		\tie
% 3. **Rotation Matrix \(R(\theta)\):**  
%    The standard 2D rotation matrix,
%    \[
%    R(\theta) = 
%    \begin{pmatrix}
%    \cos(\theta) & -\sin(\theta) \\
%    \sin(\theta) & \cos(\theta)
%    \end{pmatrix},
%    \]
%    rotates vectors by \(\theta\) and comes from classical geometry. It is used here because it naturally encodes a continuous positional shift.
% 4. **Position-Specific Rotation:**  
%    For token at position \(i\) in subspace \(k\), the rotation angle is \(\theta_{i, k} = i \cdot \alpha_k\), where \(\alpha_k\) scales the rotation differently for each subspace.
% 5. **Integration with Attention:**  
%    RoPE is applied to both query and key vectors. The dot product between rotated vectors can be re-expressed as:
%    \[
%    \tilde{\mathbf{q}}_i^\top \tilde{\mathbf{k}}_j = \mathbf{q}_i^\top\, \mathbf{M}(i,j)\, \mathbf{k}_j,
%    \]
%    where \(\mathbf{M}(i,j)\) is a block diagonal matrix with blocks \(R((j-i)\alpha_k)\) that capture the relative position between tokens.

% By rotating the embeddings with \(R(\theta)\), RoPE introduces a sophisticated yet efficient mechanism to encode both absolute and relative positional information, enhancing the transformer’s ability to capture the structure and nuances inherent in sequential data.
% \end{itemize}






\subsection{Encoder}

\subsection{Decoder}
The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.

\begin{lstlisting}[language=Python]
def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:
	seq_len, dimension = tgt.size(1), tgt.size(2)
	tgt += position_encoding(seq_len, dimension)
	for layer in self.layers:
		tgt = layer(tgt, memory)
	return torch.softmax(self.linear(tgt), dim=-1)
\end{lstlisting}


\section{Inference of Autoregressive Model}

In an autoregressive transformer (such as GPT), tokens are generated one at a time. During inference, the model must compute attention for the next token based on all previously generated tokens. However, because recomputing the entire attention from scratch at each time step would be inefficient, these models use \textit{caching} to store intermediate results (the keys and values) from previous time steps. Below is an explanation with equations and clear notations.

\paragraph{Inference Overview in Autoregressive Models}
\begin{itemize}
	\item Training: The model can process the entire sequence in parallel using masked self-attention. A mask (typically a triangular matrix) prevents tokens from ``seeing'' future tokens.
	\item Inference: The model generates one token at a time. At time step \( t+1 \), it uses the already generated tokens \( [x_1, x_2, \dots, x_t] \) to compute the probability distribution for the next token.
\end{itemize}

To avoid recomputing keys and values for tokens \( x_1, \dots, x_t \) at every step, the model stores them (usually for each layer). When a new token is generated, only its query needs to be computed, and then the cached keys and values are used to compute the attention.

The technique of storing and reusing the computed keys and values from previous tokens during autoregressive generation is commonly called \textit{KV caching} (short for Key-Value Caching).

\paragraph{Step 1: Previous Tokens and Cached Representations}

Assume that by time step \( t \) the model has generated tokens:
\[
x_1,\, x_2,\, \dots,\, x_t.
\]

For a given transformer layer, let the cached key and value matrices be:
\[
\begin{aligned}
K_{\leq t} &\in \mathbb{R}^{t \times d_k}, \\
V_{\leq t} &\in \mathbb{R}^{t \times d_v},
\end{aligned}
\]
where \( d_k \) is the key (and query) dimension and \( d_v \) is the value dimension.

\paragraph{Step 2: Compute the Query for the New Token}

When generating the next token \( x_{t+1} \), its input (often the embedding of the previously generated token or a special “start” symbol) is used to compute a query vector for each layer:
\[
q_{t+1} \in \mathbb{R}^{d_k}.
\]

This is computed by a linear projection:
\[
q_{t+1} = x_{t+1} \, W^Q,
\]
where \( W^Q \in \mathbb{R}^{d_{\text{model}} \times d_k} \) is the learned projection matrix.

\paragraph{Step 3: Compute the Attention Scores}

The new query \( q_{t+1} \) is compared with all the cached keys. In a single attention head, the (scaled) dot-product attention scores are computed as:
\[
s_{t+1,j} = \frac{q_{t+1} \cdot k_j}{\sqrt{d_k}} \quad \text{for } j=1,2,\dots,t,
\]
and often, for implementation convenience, the new token's own key \( k_{t+1} \) is also computed and appended to the cache. In that case, you would have:
\[
s_{t+1,j} = \frac{q_{t+1} \cdot k_j}{\sqrt{d_k}} \quad \text{for } j=1,2,\dots,t+1.
\]

Since the model is autoregressive, the mask is implicit. There are no ``future'' tokens beyond \( t+1 \) at inference time. (If you do compute for all \( t+1 \) positions, a mask would ensure that token \( t+1 \) only attends to tokens \( 1 \) through \( t+1 \).)

\paragraph{Step 4: Apply the Softmax to Get Attention Weights}

The scores are then normalized with the softmax function to obtain the attention weights:
\[
\alpha_{t+1,j} = \frac{\exp(s_{t+1,j})}{\sum_{j'=1}^{t+1} \exp(s_{t+1,j'})}, \quad j = 1,\dots,t+1.
\]

These weights determine how much the new token attends to each of the previous tokens (and its own representation, if included).

\paragraph{Step 5: Compute the Weighted Sum of Values}

The output of the attention layer for the new token is then computed as:
\[
z_{t+1} = \sum_{j=1}^{t+1} \alpha_{t+1,j}\, v_j,
\]
where each \( v_j \) is the value vector from the cache (or computed for the new token in the case of \( j=t+1 \)):
\[
v_j = x_j\, W^V, \quad j = 1,\dots,t+1.
\]

This \( z_{t+1} \) is then passed on through the rest of the transformer layer (including feed-forward sub-layers, layer normalization, etc.) to eventually produce logits over the vocabulary.

\paragraph{Step 6: Generate the Next Token and Update the Cache}
\begin{itemize}
	\item The model uses the final output (after all transformer layers) to compute a probability distribution over the vocabulary.
	\item A token is chosen (\eg via sampling or greedy decoding) and appended to the sequence.
	\item The new token's key and value vectors (from each layer) are computed and added to the cache so that future tokens can attend to it.
\end{itemize}

\subsection{Inference without Caching}

Let's take a look at the following example:
\begin{itemize}
	\item Number of tokens so far: \(t=3\). We have already generated tokens \(\{x_1, x_2, x_3\}\). We now want to generate token \(x_4\).  
	\item Model dimensionality: To keep it simple, let's say each token embedding is 2-dimensional (\(d_{\text{model}} = 2\)) and the attention uses a single head with key/query dimension \(d_k = 2\) and value dimension \(d_v = 2\). 
	\item Token embeddings (just made-up numbers):
	  \[
		x_1 = \begin{bmatrix}1 \\ 2\end{bmatrix}, \quad
		x_2 = \begin{bmatrix}3 \\ 4\end{bmatrix}, \quad
		x_3 = \begin{bmatrix}5 \\ 6\end{bmatrix}, \quad
		x_4 = \begin{bmatrix}? \\ ?\end{bmatrix} \; 
	  \]
	  The forth one is the one we want to generate. We will compute attention for tokens \(x_1, x_2, x_3, x_4\) all at once.  
  \item Projection matrices (\(W^Q, W^K, W^V\)) are each \(2 \times 2\) for this example. For instance, we have the following matrices:
  \[
    W^Q = \begin{bmatrix}1 & 0 \\[6pt] 0 & 1\end{bmatrix}, \quad
    W^K = \begin{bmatrix}1 & 2 \\[6pt] 0 & 1\end{bmatrix}, \quad
    W^V = \begin{bmatrix}0.5 & -0.5 \\[6pt] 1.0 & 0.5\end{bmatrix}.
  \]
\end{itemize}

\paragraph{Step A: Compute Queries, Keys, and Values}

\begin{itemize}
	\item Queries:  
   \[
     q_i = x_i \, W^Q 
   \]
   For \(i=1,2,3,4\):
   \begin{itemize}
	   \item \(q_1 = [1\;\;2] \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix} = \begin{bmatrix}1 & 2\end{bmatrix}\)
	   \item \(q_2 = [3\;\;4] \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix} = \begin{bmatrix}3 & 4\end{bmatrix}\)
	   \item \(q_3 = [5\;\;6] \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix} = \begin{bmatrix}5 & 6\end{bmatrix}\)
	   \item \(q_4 = [?,\; ?] \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix} = \begin{bmatrix}? & ?\end{bmatrix}\)
   \end{itemize}
	\item Keys:
	   \[
		 k_i = x_i \, W^K
	   \]
	   For \(i=1,2,3,4\):
	   \begin{itemize}
		   \item \(k_1 = [1\;\;2] \begin{bmatrix}1 & 2 \\ 0 & 1\end{bmatrix}
			 = \begin{bmatrix}1 & 4\end{bmatrix}\)
		   \item \(k_2 = [3\;\;4] \begin{bmatrix}1 & 2 \\ 0 & 1\end{bmatrix} 
			 = \begin{bmatrix}3 & 10\end{bmatrix}\)
		   \item \(k_3 = [5\;\;6] \begin{bmatrix}1 & 2 \\ 0 & 1\end{bmatrix} 
			 = \begin{bmatrix}5 & 16\end{bmatrix}\)
		   \item \(k_4 = [?,\; ?] \begin{bmatrix}1 & 2 \\ 0 & 1\end{bmatrix} 
			 = \begin{bmatrix}? & ?\end{bmatrix}\)
	   \end{itemize}
	\item Values:
	   \[
		 v_i = x_i \, W^V
	   \]
	   For \(i=1,2,3,4\):
	   \begin{itemize}
		   \item \(v_1 = [1\;\;2] \begin{bmatrix}0.5 & -0.5 \\ 1.0 & 0.5\end{bmatrix} 
			 = \begin{bmatrix}2.5 & 0.5\end{bmatrix}\)
		   \item \(v_2 = [3\;\;4] \begin{bmatrix}0.5 & -0.5 \\ 1.0 & 0.5\end{bmatrix} 
			 = \begin{bmatrix}5.5 & 0.5\end{bmatrix}\)
		   \item \(v_3 = [5\;\;6] \begin{bmatrix}0.5 & -0.5 \\ 1.0 & 0.5\end{bmatrix} 
			 = \begin{bmatrix}8.5 & 0.5\end{bmatrix}\)
		   \item \(v_4 = [?,\; ?] \begin{bmatrix}0.5 & -0.5 \\ 1.0 & 0.5\end{bmatrix} 
			 = \begin{bmatrix}? & ?\end{bmatrix}\)
	   \end{itemize}
\end{itemize}

All of the above must be computed at the current time step if we do not use caching.

With KV caching, you would \emph{not} recalculate \(k_1, k_2, k_3\) and \(v_1, v_2, v_3\). Instead:

\begin{itemize}
	\item You already have \(\{k_1, k_2, k_3\}\) and \(\{v_1, v_2, v_3\}\) stored from the previous steps.  
	\item You only compute:
		\begin{itemize}
			\item \(q_4\) (the query for the new token),
			\item \(k_4, v_4\) (the new key and value to add to the cache).  
		\end{itemize}
\end{itemize}
In other words, you skip re-projecting and re-computing every key and value from tokens \(\{1,2,3\}\). This saves a substantial amount of computation when generating long sequences, especially in large transformers (like GPT).

\textbf{Note that storing data in the cache uses up memory space.} Systems with limited memory resources may struggle to accommodate this additional memory overhead, potentially resulting in out-of-memory errors. This is especially the case when long inputs need to be processed, as the memory required for the cache grows linearly with the input length and the batch size.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.8]{./images/transformer/kv_caching.pdf}
	\caption{An example of KV caching}
\end{figure}

\paragraph{Computational Cost}

In sum, the vanilla self-attention on $n$ tokens,
\begin{itemize}
	\item each token needs to look at all $n$ tokens to get attention scores
    \item Thus, the cost is $O(n^2)$ 
\end{itemize}

For instance, if we given a sentence with three tokens,
\begin{enumerate}
	\item First token: Look at 1 token (cost: $O(1^2)$)
	\item Second token: Look at 2 tokens (cost: $O(2^2)$)
	\item Third token: Look at 3 tokens (cost: $O(3^2)$)
\end{enumerate}

If we add up all these costs for generating a sequence of length $n$, we get:
\begin{align*}
	O(1^2+2^2+3^2+\dots+n^2)\approx O(n^3)
\end{align*}

With caching,
\begin{enumerate}
	\item Process 1 token cost $O(1)$
	\item Process 1 new token + look at 1 cached token cost $O(2)$
	\item Process 1 new token + look at 2 cached tokens cost $O(3)$ 
\end{enumerate}

Adding these up:
\begin{align*}
	O(1+2+3+\dots+n)=O(n^2)
\end{align*}


