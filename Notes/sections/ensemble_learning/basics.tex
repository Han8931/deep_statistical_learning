\chapter{Ensemble Learning}

\section{Bagging}

\textit{Bagging} (bootstrap aggregating) is a simple way to make a noisy model more stable. Imagine you have a wobbly predictor-like a deep decision tree-that can change its mind a lot if the training data changes a little. Instead of trusting a single tree, bagging builds many versions of that model on slightly different views of the data, and then averages their answers (or takes a majority vote for classification). Averaging smooths out the noise, so the final prediction is steadier and usually more accurate.


How do we get those slightly different views? That's where bootstrapping comes in. From the original training set, we repeatedly draw new datasets of the same size with replacement (so the same example can appear multiple times, and some examples are left out). Each bootstrapped dataset trains its own model. Because each model sees a different sample, their mistakes won't line up perfectly—averaging then cancels out a lot of the randomness.

Why does this help? Many models, especially high-variance ones like decision trees, can overfit the quirks of a single dataset. Bagging reduces variance by blending many perspectives, without increasing bias too much. In practice, this often yields a solid accuracy boost and better reliability. A nice side effect is you get out-of-bag estimates: the examples left out of each bootstrap can be used as a built-in validation set to measure performance without a separate hold-out.

Bagging isn't magic—if the base model is already very stable (low variance) or consistently biased in the wrong direction, averaging won't fix the core problem. It also costs more compute and can feel less interpretable since you're dealing with many models instead of one. But as a foundational ensemble idea, bagging is both elegant and powerful—and it's the backbone of methods like Random Forests, which add an extra twist by randomly selecting features, too.

\subsection{Intuition}


We start with a single training set ($D={(x_i,y_i)}_{i=1}^n$). Instead of hoping for many independent datasets, we \textbf{simulate} them by drawing \textbf{bootstrap samples} ($D_1,\dots,D_M$): each ($D_m$) is formed by sampling ($n$) examples \textbf{with replacement} from ($D$). For each ($D_m$), we fit a base learner ($h_m$) (\eg a decision tree, ridge regression, logistic regression). We then \textbf{aggregate} the models:
\begin{align*}
	\bar{h}(x)=\frac{1}{M}\sum_{m=1}^{M}h_m(x).
\end{align*}

If we could draw truly independent datasets ($D^{(1)},\dots,D^{(M)}$) from the population and train ($h^{D^{(m)}}$) on each, then the \textbf{empirical average hypothesis} would converge to the \textbf{theoretical average hypothesis} ($\mathbb{E}_D[h^D(x)]$) as ($M\to\infty$). With bagging, our bootstraps come from the \textbf{empirical distribution} of ($D$), not the population, so they're not independent. 

\begin{itemize}
	\item Each bootstrap sample is just a reshuffled, reweighted version of the same original points.
	\item So two bootstrap samples will overlap a lot (they reuse the same observations), unlike two brand-new datasets collected from the real world.
	\item In short: they're new mixes of the same ingredients, not new ingredients.
\end{itemize}

Still, two facts make bagging effective:
\begin{enumerate}
	\item \textbf{Variance reduction by averaging}: Training many models on these different mixes and averaging their predictions smooths out noise (reduces variance), especially for models that change a lot when the data changes (like deep trees).
	\item \textbf{Bootstrap as a population proxy}: Drawing with replacement from ($D$) mimics sampling from the underlying data-generating process via the empirical distribution. This is not perfect independence, but for many learners it's good enough to bring ($\bar h$) close to ($\mathbb{E}_D[h^D(x)]$) in practice, especially for unstable learners (small data perturbations $\to$ large model changes), like deep decision trees.
\end{enumerate}

\subsection{Bias–variance intuition}

\begin{itemize}
	\item Variance: Bagging typically decreases variance because averaging stabilizes predictions.
	\item Bias: Bagging usually does not increase bias, and may slightly reduce it for some learners. But its main win is variance.
	\item Who benefits most? Unstable learners (\eg trees, k-NN with small ($k$)) benefit a lot. Stable learners (\eg linear/ridge regression) benefit less, though averaging can still help in noisy regimes.
\end{itemize}

Their are some costs: 
\begin{itemize}
	\item Interpretability
	\item Computational costs
\end{itemize}

## When and why it works (and when it doesn't)

\begin{itemize}
	\item Correlation matters. If base learners are too correlated, variance reduction is limited. This is why Random Forests add feature subsampling at each split to further decorrelate trees, improving over plain bagging of trees.
	\item Computational cost. Training ($M$) models costs ($M\times$) as much compute (though it parallelizes nicely).
	\item Interpretability. An averaged ensemble is harder to interpret than a single model (especially vs. a small tree or linear model). You can mitigate with feature importance, partial dependence, SHAP, and so on., but it's still less transparent.
	\item Diminishing returns in ($M$). Gains flatten as (M) grows because you approach the correlation floor. In practice, tens to a few hundreds of models are often sufficient.
\end{itemize}

\section{Random Forest}

Now, what exactly makes a random forest different from a bagged forest? The answer to this question has to do with the way in which single trees are grown in a random forests. We are going to introduce a twist in the tree-building algorithm. More specifically, we are going to modify the node splitting procedure. Assuming we have p features, we predetermine a number $k\ll p$ such that at each node, $k$ features are randomly selected (from the set of $p$ predictors), to find the best split.

The fundamental reason to introduce another random sampling operation, which is repeated for every node partition, is to attempt reducing the number of correlated predictors that could potentially be present in the construction of a single tree. This seemingly awkward operation turns out to return high dividends when a forest is deployed in practice.

\section{Boosting}

Boosting builds a strong model by adding many small, simple models (weak learners) sequentially. Each new model focuses on the mistakes of the models before it. In the end, we add their predictions to get one strong predictor.

\begin{itemize}
	\item Bagging = parallel averaging to reduce \textbf{variance}.
	\item Boosting = sequential fixing to reduce \textbf{bias} (and often variance too).
\end{itemize}

Types of Boosting Algorithms:
\begin{itemize}
	\item AdaBoost (Adaptive Boosting)
	\item Gradient Boosting
	\item XGBoost
\end{itemize}

\subsection{AdaBoost}

Boosting is a general strategy for learning classifiers by combining simpler ones. The idea of boosting is to take a ``weak classifier'' - that is, any classifier that will do at least slightly better than chance — and use it to build a much better classifier, thereby boosting the performance of the weak classification algorithm. This boosting is done by averaging the outputs of a collection of weak classifiers. 

The most popular boosting algorithm is \textit{AdaBoost}, so-called because it is ``adaptive.'' AdaBoost is extremely simple to use and implement (far simpler than SVMs), and often gives very effective results. There is tremendous flexibility in the choice of weak classifier as well. Boosting is a specific example of a general class of learning algorithms called ensemble methods, which attempt to build better learning algorithms by combining multiple simpler algorithms.

Suppose we are given training data $\{(\rvx_i, y_i)\}^N_{i=1}$, where $\rvx_i \in \mathbb{R}^K$ and $y_i \in \left\{-1, 1\right\}$. And suppose we are given a (potentially large) number of weak classifiers, denoted $f_m(\rvx) \in \left\{-1, 1\right\}$, and a \textbf{0-1} loss function $I$, defined as
\begin{align*}
	I(f_m(\rvx), y) = \begin{cases}
		0&\text{ if } f_m(\rvx_i)=y_i\\
		1&\text{ if } f_m(\rvx_i)\neq y_i
	\end{cases}
\end{align*}
\begin{itemize}
	\item $I=0$ if prediction equals the truth ( $\hat{y}=y$)
	\item $I=1$ if prediction is wrong ( $\hat{y}\neq y$)
\end{itemize}

AdaBoost aims to build a strong classifier by combining many weak ones. It does this in rounds $m=1,2,\dots,M$ :

\begin{enumerate}
	\item Give every training example the same weight. Every sample is equally important at the beginning.
	\item Fit a \textit{weak learner} $f_m$ on the weighted data
	\item Compute how good it is: measure its \textit{weighted error} of weak learner, $e_m$. 
		\begin{align*}
			e_m = \sum_{i=1}^{N}w_i^m \cdot I(f_m(\rvx_i), y_i),
		\end{align*}
		where $w_i^m$ is the current example weight. 
		\begin{itemize}
			\item This measures how many (weighted) mistakes that stump/tree makes this round.
		\end{itemize}
	\item Compute $\alpha_m$, which is the \textit{vote weight} (strength) assigned to that weak learner when forming the final ensemble: 
		\begin{align*}
			\alpha_m = \frac{1}{2}\ln \left(\frac{1-e_m}{e_m}\right).
		\end{align*}
		\begin{itemize}
			\item minimizing the weighted exponential loss for fixed $f_m$.
			\item bigger when the error $e_m$ is smaller
			\item As $e_m\to 0$, $\alpha_m\to \infty$.
			\item As $e_m\to 1$, $\alpha_m\to -\infty$.
		\end{itemize}
	\item Repeat: fit the next weak learner on the reweighted data, and so on.
		\begin{align*}
			w_i^{(m+1)}\propto w_i^{(m)}\cdot \exp(-\alpha_m y_i f_m(\rvx_i)).
		\end{align*}
\end{enumerate}

After learning, the final classifier is based on a linear combination of the weak classifiers:
\begin{align*}
	F(\rvx) = \sum_{m=1}^M \alpha_mf_m(\rvx).
\end{align*}
If more of the strong (high-$\alpha$) weak learners say $+1$ than $-1$, the final answer is $+1$, etc.

This is why it's called \textbf{adaptive:} each new weak learner adapts to the mistakes of the previous ones by focusing more weight on hard examples.

\subsection{Gradient Boosting}

Similar to AdaBoost, \textit{gradient boosting} is an ensemble method that combines multiple weak learners (such as decision trees) into a strong learner. Also, similar to AdaBoost (and in contrast to Bagging), gradient boosting is a sequential (rather than parallel) algorithm – it is powerful but rather expensive to train.

In contrast to AdaBoost, we do not adjust the weights for the training examples that have been either misclassified or correctly classified. Also, we do not compute a weight for each model in the sequential gradient boosting ensemble. Instead, in gradient boosting, we \textit{optimize a differentiable loss function} (\eg mean-squared-error for regression or negative log-likelihood for classification) of a weak learner via consecutive rounds of boosting. The output of gradient boosting is an additive models of multiple weak learners (we do not apply majority voting to the ensemble of models like AdaBoost).

Note that in gradient boosting, we may use decision trees (the most common base learning algorithm for gradient boosting is in fact a decision tree algorithm), but we do not necessarily use decision tree stumps. The first model in gradient boosting is basically just a decision tree's root node (on which we do majority voting in classification or averaging in regression). The consequent models in gradient boosting are deeper decision trees. The depth is usually determined by the practitioner. Values like 8 or 32 are common, depending on the complexity of the dataset/difficulty of the task.

Conceptually, the main idea behind gradient boosting can be summarized via the following three steps:
\begin{enumerate}
	\item Construct a base tree (just the root node)
	\item Build next tree based on errors of the previous tree
	\item Combine tree from step 1 with trees from step 2. Go to step 2.
\end{enumerate}


Build a strong predictor by **stacking many tiny corrections**: start with a simple guess, then repeatedly add small models that fix what’s still wrong.

\begin{itemize}
	\item Start simple: Make an initial prediction for every data point (\eg the average target value).
	\item Look at what you got wrong: Compute the residuals = (true − predicted).
	\item For classification, this error is the negative gradient of the loss (that’s the “gradient” in gradient boosting).
	\item Learn to fix the errors: Train a small, weak model (usually a shallow decision tree) to predict those residuals.
	\item Add a small step: Add a fraction (the learning rate) of that weak model to your running prediction.
	\item Repeat: Recompute the residuals with the improved predictions, fit another small tree to those, add it in, and so on.
\end{itemize}

After ($M$) rounds you have:
\begin{align*}
	\hat{y}(x) = f_0(x) + \nu \cdot h_1(x) + \nu \cdot h_2(x) + \cdots + \nu \cdot h_M(x)
\end{align*}
where ($f_0$) is the initial guess, each ($h_m$) is a small tree trained to fix current mistakes, and ($\nu$) is the learning rate (\eg 0.1).

\begin{itemize}
	\item Targets: $[3, 5, 7]$.
	\item Initial guess: mean = 5 → predictions $[5, 5, 5]$.
	\item Residuals: $[-2, 0, +2]$.
	\item Fit a tiny tree to map features → residuals (learns left points are low, right points are high).
	\item Add a small step (say 0.5$\times$ tree output) to your predictions.
	\item Recompute residuals and repeat. Each round chips away at what's still wrong.
\end{itemize}

