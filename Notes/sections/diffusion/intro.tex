\chapter{Diffusion Model}
\section{Introduction}

	(Denoising) Diffusion models have emerged as the new SOTA family of deep generative models. 
	\begin{itemize}
		\item First proposed in 2015.
		\item Outperform GANs on image synthesis (DDPM) $\sim$ 2020.
		\item Stable training dynamics.
		\item Image synthesis, super resolution, text-to-image, and so on.
%		\item DMs are inspired by non-equilibrium thermodynamics. 
%			\begin{itemize}
%				\item Random motion of molecules from a region of high concentration to a region of low concentration.
%			\end{itemize}
	\end{itemize}
%	\begin{figure}[h]
%		\centering
%		\includegraphics[scale=0.15]{./images/diffusion.png}
%		\caption{Disffusion.}
%		\label{fig:diffusion}
%	\end{figure}
% \vskip0pt plus 1filll
% \href{https://arxiv.org/abs/1503.03585}{\tiny \blue{ICML 2015 Deep Unsupervised Learning using Nonequilibrium Thermodynamics}}

The overall idea is to construct a Markov chain of progressively less noisy samples. Each transition denoises a noisy sample. Diffusion models consist of two Markov chains:
\begin{enumerate}
	\item Forward: A Markov chain of diffusion steps to \textbf{slowly add random noise} to data. 
		$$\underbrace{\rvx_0}_{\text{data}}\to \rvx_1\cdots\to \underbrace{\rvx_T}_{\text{noise}}$$
		\begin{itemize}
			\item $\mathcal{D}_{data}\to \mathcal{N}$.
		\end{itemize}
	\item Backward (Reverse): Learn to \textbf{reverse the diffusion process} to construct desired data samples from the noise. 
		$$\underbrace{\rvx_T}_{\text{noise}}\to \rvx_{T-1}\cdots\to \underbrace{\rvx_0}_{\text{data}}$$

		\begin{itemize}
			\item $\mathcal{N}\to \mathcal{D}_{data}$.
		\end{itemize}
\end{enumerate}
Breaking the overall process into smaller steps allows us to use simple distributions at each step. As will be discussed in the following subsections, we can use Gaussian distributions for the transitions. Thanks to the properties of a Gaussian, the posterior will remain a Gaussian if the likelihood and the prior are both Gaussians. Therefore, if each transitional distribution above is a Gaussian, the joint distribution is also a Gaussian. Since a Gaussian is fully characterized by the first two moments (mean and variance), the computation is highly tractable. 

This particular structure is called the \textit{variational diffusion model}.
\begin{figure}[t]
	\centering
	\includegraphics[scale=0.28]{./images/diffusion/diffusion_model.png}
\end{figure}
	% \href{https://cvpr2022-tutorial-diffusion-models.github.io/}{\tiny \blue{CVRP 2022 Tutorial: Denoising Diffusion-based Generative Modeling: Foundations and Applications}}

\textbf{Some properties of diffusion models}:
\begin{enumerate}
	%$$q(\rvx_t|\rvx_{t-1}) = \mathcal{T}(\rvx_t|\rvx_{t-1})$$
	\item Diffusion model has a pre-defined sampling equation.
		\begin{itemize}
			\item The equation relies on a random noise.
			\item Noise is all we need $\to$ Predict noise at a time step $t$.
		\end{itemize}
	\item Fit a model via forward and backward processes.
	\item \textbf{Iterative transform} of one distribution into another via \textbf{Makov Chain}.
		\begin{itemize}
			\item Diffusion model$\approx$Generative Markov Chain.
		\end{itemize}
	\item We want to learn a transition model:
		$$p_\theta(\mathbf{x}_{t-1}|\rvx_t) = \mathcal{N}(\mathbf{x}_{t-1}|\boldsymbol{\mu}_{\theta}(\mathbf{x}_{t}, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t,t)).$$
	\item Base case: $p(\mathbf{x}_T) = \mathcal{N}(0,I)$ 
	\item Marginal distribution over $\mathbf{x}_0$ can be expressed as follows:
		$$p_\theta(\mathbf{x}_0) = \int p_\theta(\rvx_0,\dots,\mathbf{x}_T)d\rvx_1,\dots,\rvx_T$$
		\begin{itemize}
			\item Similar to the VAE, the $\rvx_1,\dots, \rvx_T$ can be considered as latent variables, since we don't know. 
		\end{itemize}
	\item The goal is to learn the parameters so that
		$$p(\rvx_0)\approx p_\theta(\rvx_0)$$
\end{enumerate}

\section{Forward Diffusion}
Let's first model a forward trajectory by using the \textit{Markov property}: 
$$q(\mathbf{x}_{0:T}) = q(\rvx_0)\prod^T_{t=1} \underbrace{q(\mathbf{x}_t \vert \mathbf{x}_{t-1})}_{\text{Transition kernel}} $$
\begin{itemize}
	\item \textit{Slow transform with a large} $T$: $\rvx_0\to \rvx_1\cdots\to \rvx_T$
		\begin{itemize}
			\item To track the transitions of data, we will try to model as many steps as possible.
		\end{itemize}
	\item The question is how to model $q(\mathbf{x}_t \vert \mathbf{x}_{t-1})$?
\end{itemize}

In a continuous case (\eg image), forward diffusion $q(\rvx_t \vert \rvx_{t-1})$ can be parameterized as follows:
\begin{align}
	q(\rvx_t \vert \rvx_{t-1}) &= \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \rvx_{t-1}, \beta_t\mathbf{I})
	\label{eq:forward_diffusion}
\end{align}
\begin{itemize}
		\item $\beta_t\in (0,1)$ is a variance at time $t$.
		\item $\sqrt{1 - \beta_t}$ downscales $\rvx_{t-1}$ to be 0, $\beta_1<\cdots<\beta_t$. Thus, $\rvx_t$ will become more noisier than $\rvx_{t-1}$. Then, $\rvx_t$ can be sampled as:
			$$\mathbf{x}_t= \sqrt{1 - \beta_t} \mathbf{x}_{t-1}+ \sqrt{\beta_t} \odot \epsilon$$
			\begin{enumerate}
				\item Sample $\rvx_{t}\sim q(\rvx_t)$ and scale it by $\sqrt{1 - \beta_t}$
				\item Adds noise $\epsilon\sim \mathcal{N}(0,I)$ with variance $\beta_t$.
			\end{enumerate}
		\item The above process is autoregressive (\ie ancestral sampling), but we can sample $\mathbf{x}_t$ directly from $q(\mathbf{x}_t|\rvx_0)$ in an analytic form:
\begin{align}
	\rvx_t &= \sqrt{1 - \beta_t} \mathbf{x}_{t-1}+ \sqrt{\beta_t} \odot \epsilon_{t-1}\\
						   &= \sqrt{\alpha_t} \mathbf{x}_{t-1}+ \sqrt{1-\alpha_t} \odot \epsilon_{t-1}\\
						   &= \sqrt{\alpha_t} \bigg(\sqrt{\alpha_{t-1}} \mathbf{x}_{t-2}+ \sqrt{1-\alpha_{t-1}} \odot \epsilon_{t-2}\bigg) + \sqrt{1-\alpha_t} \odot \epsilon_{t-1}\\
						   &= \sqrt{\alpha_t\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{\alpha_t-\alpha_t\alpha_{t-1}} \odot \epsilon_{t-2} + \sqrt{1-\alpha_t} \odot \epsilon_{t-1}\\
						   &= \sqrt{\alpha_t\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{\alpha_t-\alpha_t\alpha_{t-1}+1-\alpha_t} \odot \epsilon_{t-2} \\
						   &= \sqrt{\alpha_t\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}} \odot \epsilon_{t-2} \\
						   &= \dots\\
						   &= \sqrt{\prod_{t}\alpha_t} \mathbf{x}_{0} + \sqrt{1-\prod_t \alpha_t} \odot \epsilon_{t_0} \\
						   &= \sqrt{\bar{\alpha}_t} \mathbf{x}_{0}+ \sqrt{1-\bar{\alpha}_t} \odot \epsilon\\
	&\sim \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \rvx_{0}, (1-\bar{\alpha}_t)\mathbf{I}),
	\label{eq:diffusion_forward_sampling}
\end{align}
			where $\alpha_t = 1-\beta_t$ and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$. Thus, $\mathbf{x}_t= \sqrt{\bar{\alpha}_t} \mathbf{x}_{0}+ \sqrt{1-\bar{\alpha}_t} \odot \epsilon$. Note that the fifth step is done by using the property of sum of two Gaussian distributions (\eg $\mathcal{N}(0, \sigma_1^2I)+\mathcal{N}(0, \sigma_2^2I) = \mathcal{N}(0, (\sigma_1^2+\sigma_2^2)I)$ ). 
\end{itemize}

We can get some intuitions:
\begin{itemize}
	\item The original input $\rvx_0$ \textbf{gradually loses all info} during the forward diffusion process.
	\item This Markov chain has a \textbf{stationary distribution} \ie as $t\to \infty$, $q(\rvx_t) \approx \mathcal{N}(0,I)$.
		\begin{itemize}
			\item In practice, $T$ is set to be a very high number \eg 1,000.
			\item The high $T$ minimizes the information loss at each step, which allows a smooth training.
		\end{itemize}
\end{itemize}

\section{Backward Process}

In the forward process, we model the diffusion process, which turns the input data into the noisy data. However, a noisy data is not what we want to obtain. What we want is to generate a new data with high quality.

To generate data, we will reverse the forward process. Then, we can reconstruct the true data. We call this process \textit{Backward process}! The backward process works as follows:

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.30]{./images/diffusion/reverse.png}
\end{figure}
% \href{https://cvpr2022-tutorial-diffusion-models.github.io/}{\tiny \blue{CVPR 2022 Tutorial: Denoising Diffusion-based Generative Modeling: Foundations and Applications}}

\begin{itemize}
	\item Start from the noise distribution $q(\rvx_T)\approx \mathcal{N}(0,I)$.
	\item Then, sample a noise $\rvx_T\sim \mathcal{N}(\rvx_T|0,I)$
	\item Iteratively sample $\rvx_{t-1}\sim q(\rvx_{t-1}|\rvx_t).$ 
		\begin{itemize}
			\item Note that $q(\rvx_{t-1}|\rvx_t)$ is the \textbf{true denoising distribution} that we have no access.
			\item In general, $q(\rvx_{t-1}|\rvx_t)$ is intractable.
		\end{itemize}
	%\item In general, $q(x_{t-1}|x_t)\propto q(x_{t-1})q(x_t|x_{t-1})$ is intractable.
	\item We can \textbf{approximate} $q(\rvx_{t-1}|\rvx_t)$ as \textbf{Normal distribution} if $\beta_t$ is small enough in each forward diffusion step.
	\item We will approximate $q(\rvx_{t-1}|\rvx_t)$ using a neural network, $p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$.
	\item $p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod^T_{t=1} p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$.
		\begin{itemize}
			\item $p(\rvx_T) = \mathcal{N}(\rvx_T;0,I)$.
			\item $p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))$.  

			\item We can model the denoising distribution as Normal distribution like above (\cf \Cref{eq:diffusion_kl_true_denoising}).
			\item Note that the reverse conditional probability $q(\rvx_{t-1}|\rvx_t)$ is tractable when it is conditioned on $\rvx_0$ as shown in \Cref{eq:diffusion_kl_true_denoising}. This allows us to train a neural network to model this denoising distribution. 
		\end{itemize}
		% \begin{itemize}
		% 	\item Network outputs mean and variance ($\boldsymbol{\mu}_{\theta}$ and $\boldsymbol{\Sigma}_{\theta}$). 
		% 	\item We are not going to make a model to predict them directly.
		% 	\item \textbf{Train a noise prediction network.}
		% \end{itemize}
	\item Key to the success of this sampling process is training the reverse Markov chain to match the actual time reversal of the forward Markov chain. 
	\item After optimizing the backward process, the sampling procedure is that just sample Gaussian noise from $p(\rvx_T)$ and then iteratively running the denoising transitions (backward process) for $T$ steps to generate a novel $\rvx_0$.
\end{itemize}

