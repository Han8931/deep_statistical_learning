\section{Distribution Modeling}
What we want to learn (or model) is $p_\theta(\rvx_0)\approx p(\rvx_0)$ (approximate data distribution).
\begin{itemize}
	\item $p_\theta(\rvx_0) = \int p_\theta(\rvx_{0:T})d\rvx_{1:T}$
	\item It is intractable to compute all trajectories (\ie integral over all $\rvx_{1:T}$). 
		% $$\argmax_\theta\mathbb{E}_{\rvx_0\sim p}[\log p_\theta(\rvx_0)] = \mathbb{E}_{\rvx_0\sim p}\bigg[\log\int p_\theta(\rvx_{0:T})d\rvx_{1:T}\bigg].$$
	\item Thus, we will deviate the issue by leveraging a variational lower bound with KL-Divergence as follows:
\end{itemize}

The log-likelihood of $p_\theta$ is given by
\begin{align}
\log p_\theta(\rvx_0) &=  \log\int p(\rvx_{0:T})d\rvx_{1:T}\\
	&= \log\int p(\rvx_{0:T})\frac{q(\rvx_{1:T}|\rvx_0)}{q(\rvx_{1:T}|\rvx_0)}d\rvx_{1:T}\\
	&= \log\int q(\rvx_{1:T}|\rvx_0)\frac{p(\rvx_{0:T})}{q(\rvx_{1:T}|\rvx_0)}d\rvx_{1:T}\\
	&\geq \int q(\rvx_{1:T}|\rvx_0)\log\frac{p(\rvx_{0:T})}{q(\rvx_{1:T}|\rvx_0)}d\rvx_{1:T}\quad \text{by Jensen's Inequality}\\
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\bigg[\log\frac{p(\rvx_{0:T})}{q(\rvx_{1:T}|\rvx_0)}\bigg] \quad (\textbf{ELBO})\\
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\bigg[\log p(\rvx_T)\prod_{t=1}^T\frac{p(\rvx_{t-1}|\rvx_t)}{q(\rvx_{t}|\rvx_{t-1})}\bigg]\quad \text{by Markov Property}\\
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\bigg[\log \frac{p(\rvx_T)p(\rvx_0|\rvx_{1})\blue{\prod_{t=2}^{T} p(\rvx_{t-1}|\rvx_{t})}}{q(\rvx_T|\rvx_{T-1})\prod_{t=1}^{T-1}  q(\rvx_{t}|\rvx_{t-1})}\bigg]\\
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\bigg[\log \frac{p(\rvx_T)p(\rvx_0|\rvx_{1})}{q(\rvx_T|\rvx_{T-1})}\bigg] + \underbrace{\mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\bigg[\log \blue{\prod_{t=1}^{T-1}}\frac{\blue{p(\rvx_{t}|\rvx_{t+1})}}{q(\rvx_{t}|\rvx_{t-1})}\bigg]}_{\text{Consistency term}}
\end{align}
The first term of the last line can be further decomposed:
\begin{align*}
	\underbrace{\mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}[\log p(\rvx_0|\rvx_{1})]}_{\text{Reconstruction}}+\underbrace{\mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\bigg[\log \frac{p(\rvx_T)}{q(\rvx_T|\rvx_{T-1})}\bigg]}_{\text{Prior matching}}+\mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\bigg[\sum_{t=1}^{T-1}\log \frac{ p(\rvx_{t}|\rvx_{t+1})}{q(\rvx_{t}|\rvx_{t-1})}\bigg]
\end{align*}
The \textit{reconstruction term} can be simplified as follows:
\begin{align*}
	\mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}[\log p(\rvx_0|\rvx_{1})] = \mathbb{E}_{q(\rvx_{1}|\rvx_0)}[\log p(\rvx_0|\rvx_{1})]. 
\end{align*}
This is because we can ignore the steps $[2,\dots,T]$, which are unrelated to the calculation of the expectation. 

The \textit{prior matching term} ensures that the latent distribution at the end of the diffusion step is similar to a prior distribution, which is a Gaussian distribution. Note that the \textit{prior matching term} requires no optimization, as it has no trainable parameters. Recall that we have assumed a large enough $T$ such that the final distribution is Gaussian, this term effectively becomes zero.

Now, let's simplify the prior matching term. First, we will only consider the $\rvx_T, \rvx_{T-1}$ since the conditional probability depends only on them. Then, the expectation term $q(\rvx_{1:T}|\rvx_0)$ becomes:
\begin{align*}
	q(\rvx_{1:T}|\rvx_0) &= q(\rvx_{T},\rvx_{T-1}|\rvx_0)\\
						 &= q(\rvx_{T},\rvx_{T-1},\rvx_0)/q(\rvx_0)\\
						 &= q(\rvx_{T-1}|\rvx_0)q(\rvx_T|\rvx_{T-1},\rvx_0) \quad \text{by Chain Rule}\\
						 &= q(\rvx_{T-1}|\rvx_0)q(\rvx_T|\rvx_{T-1}) \quad \text{by Markov Property}
\end{align*}
By using the above simplification,
\begin{align*}
	\mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\bigg[\log \frac{p(\rvx_T)}{q(\rvx_T|\rvx_{T-1})}\bigg] &= \mathbb{E}_{q(\rvx_{T-1}|\rvx_0)}\bigg[\mathbb{E}_{q(\rvx_{T}|\rvx_{T-1})} \log \frac{p(\rvx_T)}{q(\rvx_T|\rvx_{T-1})}\bigg]\\ 
																							 &= -\mathbb{E}_{q(\rvx_{T-1}|\rvx_0)}\left[D_{KL}[ q(\rvx_{T}|\rvx_{T-1}) \| p(\rvx_T) ]\right].
\end{align*}

Finally, the \textit{consistency term} can be expressed as follows:
\begin{align*}
	\mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\bigg[\sum_{t=1}^{T-1}\log \frac{ p(\rvx_{t}|\rvx_{t+1})}{q(\rvx_{t}|\rvx_{t-1})}\bigg] &= \sum_{t=1}^{T-1}\mathbb{E}_{q(\rvx_{t-1},\rvx_t,\rvx_{t+1}|\rvx_0)}\bigg[\log \frac{ p(\rvx_{t}|\rvx_{t+1})}{q(\rvx_{t}|\rvx_{t-1})}\bigg]\\
																															 &= - \sum_{t=1}^{T-1} \mathbb{E}_{q(\rvx_{t-1},\rvx_{t+1}|\rvx_0)}\left[D_{KL}[q(\rvx_{t}|\rvx_{t-1})\|p(\rvx_{t}|\rvx_{t+1})]\right]
\end{align*}
Note that the last step is done by the chain rule:
\begin{align*}
	q(\rvx_{t}, \rvx_{t-1}|\rvx_0) = \frac{q(\rvx_{t}, \rvx_{t-1},\rvx_0)}{q(\rvx_0)} = \frac{q(\rvx_{t}|\rvx_{t-1}, \rvx_0)q(\rvx_{t-1}, \rvx_0)}{q(\rvx_0)} = q(\rvx_{t}|\rvx_{t-1})q(\rvx_{t-1}|\rvx_0)
\end{align*}

In sum, we derive the ELBO and its factorized form is given by
\begin{align*}
\log p_\theta(\rvx_0) &\geq  \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\bigg[\log\frac{p(\rvx_{0:T})}{q(\rvx_{1:T}|\rvx_0)}\bigg]\\
					  &= \mathbb{E}_{q(\rvx_{1}|\rvx_0)}[\log p(\rvx_0|\rvx_{1})] -\mathbb{E}_{q(\rvx_{T-1}|\rvx_0)}\left[D_{KL}[ q(\rvx_{T}|\rvx_{T-1}) \| p(\rvx_T) ]\right]\\
					 &\quad - \sum_{t=1}^{T-1} \mathbb{E}_{q(\rvx_{t-1},\rvx_{t+1}|\rvx_0)}\left[D_{KL}[q(\rvx_{t}|\rvx_{t-1})\|p(\rvx_{t}|\rvx_{t+1})]\right].
\end{align*}
Let's closely look at the consistency term. 
\begin{itemize}
	\item It endeavors to make the distribution at $x_t$ consistent, from both forward and backward processes. That is, a denoising step from a noisier image should match the corresponding noising step from a cleaner image, for every intermediate timestep.  This term is minimized when we train $p_\theta(\rvx_t|\rvx_{t+1})$ to match the Gaussian distribution $q(\rvx_t|\rvx_{t-1})$, which is defined in \Cref{eq:forward_diffusion}. 
\end{itemize}

% Finally, we can express the ELBO as follows:
% \begin{align*}
% 	\text{ELBO}_{\phi,\theta}(\rvx) &= 
% \end{align*}

\begin{itemize}
	\item Under this derivation, all terms of the ELBO are computed as expectations, and can therefore be approximated using Monte Carlo estimates. 
\item The challenge of the above variational diffusion model is that we need to draw samples $(x_{t-1} , x_{t+1} )$ from a joint distribution $q(x_{t-1}, x_{t+1}|x_0)$ for every time step $t$. Thus, the variance of its Monte Carlo estimate could potentially be higher than a term that is estimated using only one random variable per time step. As it is computed by summing up $T-1$ consistency terms, the final estimated value of the ELBO may have high variance for large $T$ values.
\end{itemize}

Let us instead try to derive a new ELBO, where each term is computed as an expectation over \textbfonly one random variable at a time. The key insight is that we can rewrite encoder transitions as $q(x_t|x_{t-1}) = q(x_t|x_{t-1}, x_0)$, where the extra conditioning term is superfluous due to the Markov property. Then, according to Bayes rule, we can rewrite each transition as:
\begin{align*}
	q(\rvx_t|\rvx_{t-1}, \rvx_0)= \frac{q(\rvx_{t-1}|\rvx_t, \rvx_0)q(\rvx_t| \rvx_0)}{q(\rvx_{t-1}| \rvx_0)}.
\end{align*}
Armed with this equation, we can factorize the ELBO again as follows:
\begin{align}
	L 
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\left[\log\frac{p(\rvx_{0:T})}{q(\rvx_{1:T}|\rvx_0)}\right] \quad \to \textrm{ELBO}\\		
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\left[\log p(\rvx_T)\prod_{t=1}^T\frac{p(\rvx_{t-1}|\rvx_t)}{q(\rvx_{t}|\rvx_{t-1})}\right]\\
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\left[\log \frac{p(\rvx_T)p(\rvx_0|\rvx_{1})\prod_{t=2}^{T} p(\rvx_{t-1}|\rvx_{t})}{q(\rvx_1|\rvx_{0})\prod_{t=2}^{T}  q(\rvx_{t}|\rvx_{t-1})}\right]\\
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\left[\log \frac{p(\rvx_T)p(\rvx_0|\rvx_{1})\prod_{t=2}^{T} p(\rvx_{t-1}|\rvx_{t})}{q(\rvx_1|\rvx_{0})\prod_{t=2}^{T}  q(\rvx_{t}|\rvx_{t-1}, \blue{\rvx_0})}\right] \quad \textrm{ by Markov Property}\\
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\left[\log \frac{p(\rvx_T)p(\rvx_0|\rvx_{1})}{q(\rvx_1|\rvx_{0})}+\log \prod_{t=2}^{T}\frac{ p(\rvx_{t-1}|\rvx_{t})}{q(\rvx_{t}|\rvx_{t-1}, \rvx_0)}\right]\\
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\left[\log \frac{p(\rvx_T)p(\rvx_0|\rvx_{1})}{q(\rvx_1|\rvx_{0})}+\log \prod_{t=2}^{T}\frac{ p(\rvx_{t-1}|\rvx_{t})}{\frac{q(\rvx_{t-1}|\rvx_t, \rvx_0)q(\rvx_t| \rvx_0)}{q(\rvx_{t-1}| \rvx_0)}}\right]\\
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\left[\log \frac{p(\rvx_T)p(\rvx_0|\rvx_{1})}{q(\rvx_1|\rvx_{0})}+\log \frac{q(\rvx_1|\rvx_0)}{q(\rvx_T|\rvx_0)}+\log \prod_{t=2}^{T}\frac{ p(\rvx_{t-1}|\rvx_{t})}{q(\rvx_{t-1}|\rvx_t, \rvx_0)}\right]\\
	&= \mathbb{E}_{q(\rvx_{1:T}|\rvx_0)}\left[\log \frac{p(\rvx_T)p(\rvx_0|\rvx_{1})}{q(\rvx_T|\rvx_{0})}+\log \prod_{t=2}^{T}\frac{ p(\rvx_{t-1}|\rvx_{t})}{q(\rvx_{t-1}|\rvx_t, \rvx_0)}\right]\\
	&= \mathbb{E}_{q(\rvx_{1}|\rvx_0)}[\log p(\rvx_0|\rvx_{1})]+\mathbb{E}_{q(\rvx_{T}|\rvx_0)}\left[\log \frac{p(\rvx_T)}{q(\rvx_T|\rvx_{0})}\right]\\ 
	&\quad+\sum_{t=2}^{T}\mathbb{E}_{q(\rvx_{t},\rvx_{t-1}|\rvx_0)}\left[\log\frac{ p(\rvx_{t-1}|\rvx_{t})}{q(\rvx_{t-1}|\rvx_t, \rvx_0)}\right]\\
	&= \mathbb{E}_{q(\rvx_{1}|\rvx_0)}[\log p(\rvx_0|\rvx_{1})]-D_{KL}(q(\rvx_{T}|\rvx_0)||p(\rvx_T))\\
	&\quad -\sum_{t=2}^{T}\mathbb{E}_{q(\rvx_{t}|\rvx_0)}[D_{KL}(q(\rvx_{t-1}|\rvx_t, \rvx_0)\|p(\rvx_{t-1}|\rvx_{t}))]
%	\label{eq:diffusion_objective}
\end{align}
Let's delve into the last three terms:
\begin{itemize}
	\item $\mathbb{E}_{q(\rvx_{1}|\rvx_0)}[\log p(\rvx_0|\rvx_{1})]$: Reconstruction term. 
	\item $D_{KL} [ q(\rvx_{T}|\rvx_0)||p(\rvx_T) ]$: Prior matching term.
		\begin{itemize}
			\item No trainable parameters, since it can be fully characterized by $\rvx_t$ and $\rvx_0$, so we don't need a neural network to estimate it. 
		\end{itemize}
	\item $\sum_{t=2}^{T}\mathbb{E}_{q(\rvx_{t}|\rvx_0)}[D_{KL}(q(\rvx_{t-1}|\rvx_t, \rvx_0)\|p(\rvx_{t-1}|\rvx_{t}))]$: Consistency term (or denoising matching term).
		% \begin{itemize}
		% 	\item $p_\theta(\rvx_{t-1}|\rvx_t)$ as an approximation to tractable, ground-truth denoising transition step. The $q(\rvx_{t-1}|\rvx_t, \rvx_0)$ transition step can act as a ground-truth signal, since it defines how to denoise a noisy image with access to what the final, completely denoised image $\rvx_0$ should be. This term is therefore minimized when the two denoising steps match as closely as possible, as measured by their KL Divergence.
		% \end{itemize}
\end{itemize}
We can notice that the consistency term requires to estimate three different probabilities:
\begin{enumerate}
	\item $q(\rvx_{t}|\rvx_0)$: we can sample $\rvx_t$ by only using $\rvx_0$ and $\epsilon$ as we derived before: 
		\begin{align*}
			\rvx_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_{0}+ \sqrt{1-\bar{\alpha}_t} \odot \epsilon
		\end{align*}
	\item $q(\rvx_{t-1}|\rvx_t, \rvx_0)$: This distribution says we want to sample a less noisy sample $\rvx_{t-1}$ using the clean sample $\rvx_0$ and its noisy version $\rvx_t$. In other words, we want to denoise the sample by using the two samples. 
	\item $p(\rvx_{t-1}|\rvx_{t})$: We are going to use a distribution parameterized by $\theta$, which is a neural network. We can rewrite this as follows:
		\begin{align*}
			p_\theta(\rvx_{t-1}|\rvx_{t})
		\end{align*}
\end{enumerate}
In sum, the consistency term or denoising matching term is the term for training a neural network to make it learn how to denoise samples. Let's compute the KL divergence:

By using the chain rule, $q(\rvx_{t-1}|\rvx_t, \rvx_0)$ can be factorized as follows:
\begin{align*}
	q(\rvx_{t-1}|\rvx_t, \rvx_0) = \frac{q(\rvx_{t}|\rvx_{t-1}, \rvx_0)q(\rvx_{t-1}|\rvx_0)}{q(\rvx_{t}|\rvx_0)}=\frac{q(\rvx_{t}|\rvx_{t-1})q(\rvx_{t-1}|\rvx_0)}{q(\rvx_{t}|\rvx_0)}.
\end{align*}
Then, $q(\rvx_{t}|\rvx_{t-1}, \rvx_0) = q(\rvx_{t}|\rvx_{t-1})$ by Markov property. Then we get
\begin{align}
	q(\rvx_{t-1}|\rvx_t, \rvx_0) &=\frac{q(\rvx_{t}|\rvx_{t-1})q(\rvx_{t-1}|\rvx_0)}{q(\rvx_{t}|\rvx_0)}\\
								 &= \frac{\mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \rvx_{t-1}, (1-\bar{\alpha}_t)\mathbf{I})\mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}} \rvx_{0}, (1-\bar{\alpha}_{t-1})\mathbf{I})}{\mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \rvx_{0}, (1-\bar{\alpha}_t)\mathbf{I})}\\
								 & \vdots\\
								 &\propto \mathcal{N}\bigg(\rvx_{t-1};\underbrace{\frac{\sqrt{{\alpha}_{t}}(1-\bar{\alpha}_{t-1})\rvx_t+\sqrt{\bar{\alpha}_{t-1}}(1-{\alpha}_{t})\rvx_0}{1-\bar{\alpha}_{t}}}_{\mu_q(\rvx_t,\rvx_0)}, \underbrace{\frac{(1-{\alpha}_{t})(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}\mathbf{I}}_{\Sigma_{q}(t)}\bigg),
	\label{eq:diffusion_kl_true_denoising}
\end{align}
where $\alpha_t = 1-\beta_t$ and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$. We have shown that at each step, $\rvx_{t-1} \sim q(\rvx_{t-1}|\rvx_t, \rvx_0)$ is still normally distributed, with mean $\mu_q(\rvx_t,\rvx_0)$ that is a function of $\rvx_t$ and $\rvx_0$, and variance $\Sigma_q(t)$ as a function of $\alpha$ coefficients. We can rewrite the variance as $\Sigma_q(t) = \sigma^2_q(t)\mathbf{I}$.

We can also notice that the mean $\boldsymbol{\mu}_q(\rvx_t,\rvx_0)$ is a linear combination of $\rvx_t$ and $\rvx_0$. Geometrically, it lives on the straight line connecting $\rvx_t$ and $\rvx_0$. In other words, the denoised sample $\rvx_{t-1}$ stays somewhere between the $\rvx_t$ and $\rvx_0$.

Another observation is that the coefficient of $\rvx_t$ increases and that of $\rvx_0$ decreases. Conversely, $\rvx_t$ decreases and that of $\rvx_0$ increases as $t$ goes to 0 from $T$.

In order to match approximate denoising transition step $p_\theta(\rvx_{t-1}|\rvx_{t})$ to the denoising transition step $q(\rvx_{t-1}|\rvx_t, \rvx_0)$ as closely as possible, we can also model it as a Gaussian. Let's simplify the KL divergence:
\begin{align*}
	&D_{KL}(q(\rvx_{t-1}|\rvx_t, \rvx_0)\|p_\theta(\rvx_{t-1}|\rvx_{t}))\\ 
	&= D_{KL}(\mathcal{N}(\rvx_{t-1};\mu_q,\Sigma_q(t))\|\mathcal{N}(\rvx_{t-1};\mu_\theta,\Sigma_q(t)))\\ 
	& \vdots\\ 
	&= \frac{1}{2\sigma^2_q(t)}\|\boldsymbol{\mu}_\theta(\rvx_t)- \boldsymbol{\mu}_q(\rvx_t, \rvx_0)\|^2_2.
\end{align*}
By using this, we can rewrite the ELBO that we want to maximize:
\begin{align*}
	\text{ELBO}_\theta(\rvx) = \mathbb{E}_{q(\rvx_{1}|\rvx_0)}[\log p(\rvx_0|\rvx_{1})] -\sum_{t=2}^{T}\mathbb{E}_{q(\rvx_{t}|\rvx_0)}\left[\frac{1}{2\sigma^2_q(t)}\|\boldsymbol{\mu}_\theta(\rvx_t)- \boldsymbol{\mu}_q(\rvx_t, \rvx_0)\|^2_2\right]
\end{align*}
Note that the reconstruction term can be analytically computed, so we can make simplify further. 

% Next, we can use the \Cref{eq:diffusion_kl_true_denoising} to compute the optimal $\theta$ by solving $\min_\theta D_{KL}(q(\rvx_{t-1}|\rvx_t, \rvx_0)\|p_\theta(\rvx_{t-1}|\rvx_{t}))$, but we can simplify the optimization problem by using \Cref{eq:diffusion_forward_sampling}:
% \begin{align*}
% 	\rvx_t &= \sqrt{\bar{\alpha}_t} \mathbf{x}_{0}+ \sqrt{1-\bar{\alpha}_t} \odot \epsilon\\
% 	\mathbf{x}_{0} &= \frac{\rvx_t - \sqrt{1-\bar{\alpha}_t}\odot \epsilon}{\sqrt{\bar{\alpha}_t}}.
% \end{align*}

% By plugging the above equation into $\boldsymbol{\mu}_q(\rvx_t,\rvx_0)$, we can exclude $\rvx_0$ term as follows:
% \begin{align}
% 	\boldsymbol{\mu}_q(\rvx_t,\rvx_0) &= \frac{\sqrt{{\alpha}_{t}}(1-\bar{\alpha}_{t-1})\rvx_t+\sqrt{\bar{\alpha}_{t-1}}(1-{\alpha}_{t})\rvx_0}{1-\bar{\alpha}_{t}}\\
% 									  &= \frac{\sqrt{{\alpha}_{t}}(1-\bar{\alpha}_{t-1})\rvx_t+\sqrt{\bar{\alpha}_{t-1}}(1-{\alpha}_{t})\frac{\rvx_t - \sqrt{1-\bar{\alpha}_t}\odot \epsilon}{\sqrt{\bar{\alpha}_t} }}{1-\bar{\alpha}_{t}}\\
% 									  &\vdots\\
% 									  &= \frac{1}{\sqrt{\alpha_t}}\rvx_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha_t}}\sqrt{\alpha_t}}\boldsymbol{\epsilon}_0
% 	\label{eq:diffusion_mean_simple}
% \end{align}
% Thus, we can force $\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)$, which has no dependency on $\rvx_0$ term, to match the $\boldsymbol{\mu}_q$. Since the $\rvx_t$ is given at training time, we just need to predict the $\boldsymbol{\epsilon}_t$. Then, we can express $\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)$ as follows:
% \begin{align*}
% \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) &= \frac{1}{\sqrt{\alpha_t}} \bigg( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \bigg) \\
% \end{align*}
% Thus, $\rvx_{t-1}\sim p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$ can be expressed as follows:
% \begin{align*}
% 	\mathcal{N}\Bigg(\mathbf{x}_{t-1}; \frac{1}{\sqrt{\alpha_t}} \bigg( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \bigg), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t)\Bigg)
% \end{align*}
% Note that we can use the variance derived in \Cref{eq:diffusion_kl_true_denoising} instead of estimating it from a network for simplicity. Finally, we can solve the KL-divergence term:

% \begin{align}
% 	\mathcal{L} &= \min_\theta D_{KL}(q(\rvx_{t-1}|\rvx_t, \rvx_0)\|p_\theta(\rvx_{t-1}|\rvx_{t}))\\
% 				& \vdots\\
% 				&= \min_\theta \frac{1}{2\sigma^2_q(t)}\frac{(1-\alpha_t)^2}{(1-\bar{\alpha}_t)\alpha_t}\big[\|\boldsymbol{\epsilon}_0- \hat{\boldsymbol{\epsilon}}_\theta(\rvx_t,t)\|^2_2\big].
% 	\label{eq:diffusion_kl_mean_simple}
% \end{align}
% Here $\hat{\boldsymbol{\epsilon}}_\theta(\rvx_t,t)$ is a is a neural network that learns to predict the source noise $\boldsymbol{\epsilon}_0 \sim \mathcal{N}(\boldsymbol{\epsilon} \mathbf{0}, \mathbf{I})$ that determines $\rvx_t$ from $\rvx_0$. We have therefore shown that the overall learning objective is equivalent to learning to predict the noise.


