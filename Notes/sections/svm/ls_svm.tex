\section{Least Square SVM}
\label{sec:ls_svm}
Least Squares Support Vector Machine (LS-SVM) is a modified version of the traditional Support Vector Machine (SVM) that simplifies the optimization process by using a least squares cost function. LS-SVM transforms the quadratic programming problem in classical SVM into a set of linear equations, which are easier and faster to solve. Hereâ€™s a breakdown of the key concepts and steps involved in LS-SVM:

The LS-SVM optimization problem is formulated as:

\subsection{Primal Problem}

   \[
   \min_{w, b, e} \frac{1}{2} \|w\|^2 + \frac{\gamma}{2} \sum_{i=1}^N e_i^2
   \]
   subject to:
   \[
   y_i (w^T \phi(x_i) + b) = 1 - e_i, \quad \forall i \]
   where:
   \begin{itemize}
	   \item \( w \) is the weight vector.
   	   \item \( b \) is the bias term.
   	   \item \( e_i \) are the error variables. In LS-SVM, the error terms $e_i$ serve a similar purpose to the slack variables ($\xi$) in traditional SVMs. However, instead of using hinge loss as in traditional SVM, LS-SVM uses a squared error loss function. 
   	   \item \( \gamma \) is a regularization parameter.
   	   \item \( \phi(x_i) \) is the feature mapping function.
   \end{itemize}

\subsection{Lagrangian Function}
   The Lagrangian for the above primal problem is:
   \[
   L(w, b, e, \alpha) = \min_{w, b, e} \frac{1}{2} \|w\|^2 + \frac{\gamma}{2} \sum_{i=1}^N e_i^2 - \sum_{i=1}^n \alpha_i \left[ y_i (w^T \phi(x_i) + b) - 1 + e_i \right]
   \]
   where \( \alpha_i \) are the Lagrange multipliers. Then, by setting the partial derivatives of the Lagrangian with respect to \( w \), \( b \), \( e \), and \( \alpha \) to zero, we get the KKT conditions.

To find the stationary points of the Lagrangian, we take the partial derivatives with respect to \( w \), \( b \), \( e_i \), and \( \alpha_i \), and set them to zero. First, take partial derivative with respect to 

\begin{itemize}
	\item \( w \): \[
   \frac{\partial L}{\partial w} = w - \sum_{i=1}^n \alpha_i y_i \phi(x_i) = 0 \implies w = \sum_{i=1}^n \alpha_i y_i \phi(x_i)
   \]
	\item \( b \):
   \[
   \frac{\partial L}{\partial b} = -\sum_{i=1}^n \alpha_i y_i = 0
   \]
\item \( e_i \):
   \[
   \frac{\partial L}{\partial e_i} = \gamma e_i - \alpha_i = 0 \implies \alpha_i = \gamma e_i
   \]
\item \( \alpha_i \):
   \[
   \frac{\partial L}{\partial \alpha_i} = - \left[ y_i (w^T \phi(x_i) + b) - 1 + e_i \right] = 0 \implies y_i (w^T \phi(x_i) + b) = 1 - e_i
   \]
\end{itemize}

   % Solving the KKT conditions leads to a set of linear equations:
   % \[
   % \begin{bmatrix}
   % 0 & y^T \\
   % y & \Omega + \frac{1}{\gamma} I
   % \end{bmatrix}
   % \begin{bmatrix}
   % b \\
   % \alpha
   % \end{bmatrix}
   % =
   % \begin{bmatrix}
   % 0 \\
   % 1
   % \end{bmatrix}
   % \]
   % where \( \Omega \) is the kernel matrix, \( I \) is the identity matrix, and \( \alpha \) is the vector of Lagrange multipliers.

From the KKT conditions, we substitute \( w \) and \( \alpha_i = \gamma e_i \) back into the constraints:

\[
y_i \left( \sum_{j=1}^n \alpha_j y_j \phi(x_j)^T \phi(x_i) + b \right) = 1 - \frac{\alpha_i}{\gamma}
\]

Let \( K(x_i, x_j) = \phi(x_i)^T \phi(x_j) \) be the kernel function, then we can rewrite the above equation as:

\[
y_i \left( \sum_{j=1}^n \alpha_j y_j K(x_i, x_j) + b \right) = 1 - \frac{\alpha_i}{\gamma}
\]

We can express this in matrix form. Define:
\begin{itemize}
	\item \( \alpha = [\alpha_1, \alpha_2, \ldots, \alpha_n]^T \)
	\item \( y = [y_1, y_2, \ldots, y_n]^T \)
	\item \( 1_n \) as a column vector of ones of length \( n \)
	\item \( K \) as the kernel matrix, where \( K_{ij} = K(x_i, x_j) \)
\end{itemize}


Then the linear system becomes:

\[
\begin{bmatrix}
0 & y^T \\
y & K + \frac{1}{\gamma} I
\end{bmatrix}
\begin{bmatrix}
b \\
\alpha
\end{bmatrix}
=
\begin{bmatrix}
0 \\
1_n
\end{bmatrix}
\]
Note that the dimension of the matrix on the left-hand side is $(N+1)\times (N+1)$

Once we have \( b \) and \( \alpha \), the decision function for a new input \( x \) is given by:
\[
f(x) = \sum_{i=1}^n \alpha_i y_i K(x_i, x) + b.
\]

\paragraph{Example: }Suppose we have a small dataset with \( n = 3 \) training samples. Then the dimensions of the matrix and vectors would be as follows:

- \( y \) is a \( 3 \times 1 \) vector (e.g., \( y = \begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix} \)).
- \( y^T \) is a \( 1 \times 3 \) row vector (e.g., \( y^T = \begin{bmatrix} y_1 & y_2 & y_3 \end{bmatrix} \)).
- \( K \) is a \( 3 \times 3 \) kernel matrix.
- \( I \) is a \( 3 \times 3 \) identity matrix.
- \( \frac{1}{\gamma} I \) is a \( 3 \times 3 \) scaled identity matrix.
- \( K + \frac{1}{\gamma} I \) is a \( 3 \times 3 \) matrix.
- \( \alpha \) is a \( 3 \times 1 \) vector.
- \( 1_3 \) is a \( 3 \times 1 \) vector of ones.

The full matrix becomes:

\[
\begin{bmatrix}
0 & y_1 & y_2 & y_3 \\
y_1 & K_{11} + \frac{1}{\gamma} & K_{12} & K_{13} \\
y_2 & K_{21} & K_{22} + \frac{1}{\gamma} & K_{23} \\
y_3 & K_{31} & K_{32} & K_{33} + \frac{1}{\gamma}
\end{bmatrix}
\]

This matrix has dimensions \( 4 \times 4 \).

So, the dimension of the matrix on the left-hand side in the system of linear equations for LS-SVM is \( (n + 1) \times (n + 1) \), where \( n \) is the number of training samples.
