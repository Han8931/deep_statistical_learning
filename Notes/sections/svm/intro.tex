\chapter{Support Vector Machine}
Support Vector Machines (SVMs) are among the most effective and versatile tools in machine learning, widely used for various tasks. SVMs work by finding the optimal boundary, or hyperplane, that separates different classes of data with the maximum margin, making them highly reliable for classification, especially with complex datasets.

What truly sets SVMs apart is their ability to handle both linear and non-linear data through the \textit{kernel trick}, allowing them to adapt to a wide range of problems with impressive accuracy. In this blog post, we'll delve into how SVMs work and gently explore the mathematical foundations behind their powerful performance.

% \section{Orthogonal Projection}
% When working with vectors $x$ and $y$, finding the orthogonal projection of $x$ onto $y$ is a common task in linear algebra. The projection is a way to express how much of $x$ lies in the direction of $y$.

% By definition, the magnitude of the projection $z$ of $x$ onto $y$ is given by::
% $$\lVert z\rVert  = \lVert x\rVert cos(\theta).$$
% Here, $\theta$ is the angle between $x$ and $y$. To connect this with the dot product, recall that:
% $$x\cdot y = \lVert x\rVert \ \lVert y\rVert  cos(\theta).$$
% This formula allows us to replace the cosine term:
% $$\lVert z\rVert  =  \frac{x\cdot y}{\lVert y\rVert }$$
% Simplifying further, we express the magnitude of $z$ as:
% $$\lVert z\rVert = u\cdot x,$$
% where $u$ is an unit vector of $y$. Since $z$ is in the direction of $y$, we can write:
% $$z = \lVert z\rVert \cdot u,$$
% Then, 
% \begin{align*}
% 	z &= (u\cdot x)\cdot u.
% \end{align*}
% This gives us the final expression for the orthogonal projection of $x$ onto $y$:
% \begin{align*}
% 	\textrm{Proj}_yx &= (u\cdot x)\cdot u\\ 
% 					 &= \Bigg(\frac{y\cdot x}{\lVert y\rVert }\Bigg)\frac{y}{\lVert y\rVert }
% \end{align*}
% In this formula, the projection $\textrm{Proj}_yx$ represents the component of $x$ that lies along the direction of $y$.

\section{Decision Boundary with Margin}
A hyperplane(or decision surface) is used to separate data points belonging to different classes. The goal of SVM is to find the optimal separating hyperplane. However, what is the optimal separating hyperplanes? \textbf{The optimal hyperplane is the one which maximizes the distance from the hyperplane to the nearest data point of any class. Support vectors are the data points that lie closest to the hyperplane}. The distance is referred to as the \textit{margin}. SVMs maximize the margin around the separating hyperplane.

The equation of a hyperplane in $\mathbb{R}^p$ can be expressed as:
$$\mathbf{w}\cdot \mathbf{x}+b=0.$$
Here, $\mathbf{w}$ is the normal vector to the hyperplane. It is clear when we set $b = \mathbf{w}\cdot\mathbf{x}_0$
\begin{align*}
	\mathbf{w}(\mathbf{x}-\mathbf{x}_0)=0.
\end{align*}
Let's consider a simple scenario, where training data is linearly separable: 
$$\mathcal{D} = \left\{ (\mathbf{x}_i, y_i)\mid\mathbf{x}_i \in \mathbb{R}^p,\, y_i \in \{-1,1\}\right\}_{i=1}^N.$$
Then, we can build two supporting hyperplanes, which separate the data with no points between them:
\begin{itemize}
	\item $H_1:\mathbf{w}\cdot \mathbf{x}+b=1$
	\item $H_2:\mathbf{w}\cdot \mathbf{x}+b=-1$
\end{itemize}
All samples have to satisfy one of two constraints:
\begin{enumerate}
	\item $\mathbf{w}\cdot \mathbf{x}+b\geq1$
	\item $\mathbf{w}\cdot \mathbf{x}+b\leq-1$
\end{enumerate}
For instance, if we set $\rvw = (0.5, 0.5)$, $b=-3$, then $\rvx = (4,5)$ will be at $H_1$ and $\rvx=(2,1)$ at $H_2$. Collectively, these constraints can be combined into a single expression:
\begin{align*}
	y\cdot (\mathbf{w}\cdot \mathbf{x}+b)\geq 1.
\end{align*}
To maximize the margin, we can consider a unit vector $\mathbf{u} = \frac{\mathbf{w}}{\lVert\mathbf{w}\rVert}$, which is perpendicular to the hyperplanes and a point $\rvx_0$ on $H_2$. If we scale $\rvu$ from $\rvx_0$, we get $\rvz = \rvx_0+\gamma\rvu$ and we assume that $\rvz$ reaches at $H_1$. Then, $\mathbf{w}\cdot \rvz +b=1$. This is equivalent to 
\begin{align*}
	\mathbf{w}\cdot (\rvx_0+\gamma\rvu)+b=1\\
	\mathbf{w}\rvx_0+\mathbf{w}\gamma\frac{\mathbf{w}}{\lVert\mathbf{w}\rVert}+b=1\\
	\mathbf{w}\rvx_0+\gamma\lVert \mathbf{w}\rVert +b=1\\
	\mathbf{w}\rvx_0+b=1-\gamma\lVert \mathbf{w}\rVert 
\end{align*}
As $\rvx_0$ is on $H_2$, we get $\mathbf{w}\rvx_0+b=-1$. Finally, we obtain
\begin{align*}
	-1=1-\gamma\lVert \mathbf{w}\rVert \\
	\gamma=\frac{2}{\lVert \mathbf{w}\rVert }.
\end{align*}
Note that the scaled unit vector $\gamma\rvu$'s magnitude is $\gamma$. Thus, the maximization of margin is equivalent to maximize $\gamma$. To maximize $\gamma$ (\ie margin), we have to minimize $\lVert \mathbf{w} \rVert$. Thus, finding the optimal hyperplane reduces to solving the following optimization problem:
\begin{align*}
	&\min \lVert \mathbf{w}\rVert ,\, \textrm{ subject to} \\
	&y_i(\mathbf{w}\cdot \mathbf{x}_i+b)\geq 1 \quad\forall i.
\end{align*}
Equivalently, 
\begin{align*}
	&\min \frac{1}{2}\lVert \rvw\rVert ^2,\, \textrm{ subject to} \\
	&y_i(\mathbf{w}\cdot \mathbf{x}_i+b)\geq 1 \quad\forall i.
\end{align*}
Now, we have \textit{convex quadratic optimization problem}. The solution of this problem gives us the optimal hyperplane that maximizes the margin (\cf \Cref{sec:svm_optimization}). However, in practice, the data may not be perfectly separable. To address this, we introduce a \textit{soft margin} that allows for some misclassification. This is done by admitting small errors in classification and potentially using a more complex, \textit{nonlinear decision boundary}, improving the generalization of the model.

\subsection{Alternative Derivation}
Consider a point $\rvx$. Let $\rvd$ be the vector from a hyperplane (\ie $\rvw\rvx+b=0$) to $\rvx$ of minimum length. Let $\rvx_0$ be the projection of $\rvx$ onto the hyperplane. Then, 
\begin{align*}
	\rvx_0 = \rvx - \rvd.
\end{align*}
As $\rvd$ is parallel to $\rvw$, so $\rvd = \alpha \rvw$ for some $\alpha\in \mathb{R}$. Since $\rvx_0$ is on the hyperplane, $\rvw\rvx_0 + b = 0$. Thus, 
\begin{align*}
	\rvw\rvx_0 + b = \rvw(\rvx-\rvd) + b = \rvw(\rvx-\alpha\rvw) + b = 0.
\end{align*}
Then, we get
\begin{align*}
	\alpha = \frac{\rvw\rvx+b}{\rvw^T\rvw}.
\end{align*}
The length of $\rvd$ is given by  
\begin{align*}
	\|\rvd\|_2 = \sqrt{\alpha^2\rvw^T\rvw} = \frac{|\rvw\rvx+b|}{\sqrt{\rvw^T\rvw}} = \frac{|\rvw\rvx+b|}{\|\rvw\|_2}.
\end{align*}
We can obtain the margin by choosing the support vector, which is the closest point to the hyperplane by
\begin{align*}
	\gamma(\rvw, b) = \min_{\rvx\in \mathcal{D}}\frac{|\rvw\rvx+b|}{\|\rvw\|_2}.
\end{align*}
Note that the margin and hyperplane are scale invarianct:
\begin{align*}
	\gamma(\beta\rvw, \beta b) = \gamma(\rvw, b).
\end{align*}


\section{Error Handling in SVM}
In practice, it's unrealistic to expect a perfect separation of data, especially when the data is noisy or not linearly separable. To address this, we can \textbf{allow for some prediction errors while still striving to find an optimal decision boundary}.

One approach is to minimize the norm of the weight vector, while penalizing the number of errors $N_e$. The optimization problem can be formulated as follows:
\begin{align*}
	&\min \frac{1}{2}\lVert \mathbf{w}\rVert^2 +C\cdot N_{e},\, \text{ subject to}\\
	&y_i(\mathbf{w}\cdot \mathbf{x}_i+b)\geq 1 \quad \forall i.
\end{align*}
Here, $C$ is a regularization parameter that controls the trade-off between minimizing the weight vector and the number of errors. The penalty approach described here is known as \textit{0-1 loss}, where all errors are treated equally. However, this approach is not commonly used. Instead, a more practical approach introduces a \textit{slack variable} with \textit{hinge loss}. The slack variable ($\xi_j$) measures the degree of misclassification or how much a point is violating the margin. This leads to the following problem:
\begin{align*}
	&\min \frac{1}{2}\lVert \mathbf{w}\rVert^2 +C\sum_j\xi_j ,\quad \textrm{subject to }\\
		 &\begin{cases}
		&y_i(\mathbf{w}\cdot \mathbf{x}_i+b)\geq 1-\xi_j \ \forall i,\quad \\
		&\xi_j\geq 0,\ \forall j.
	\end{cases}
\end{align*}
Note that $\xi_j>1$, when SVMs make prediction errors:
\begin{align*}
	\xi_j = \left(1-(\mathbf{w}\mathbf{x}_j+b)\cdot y_j\right)
\end{align*}
Let's look at the new constraint. If some data points are misclassified, then $\xi_j>1$ and $y_i(\mathbf{w}\cdot \mathbf{x}_i+b)\leq 0$. This approach is called \textbf{soft-margin SVM}. Lastly, how do we set $C$? A large value of $C$ places a higher penalty on errors, leading to a narrower margin but fewer misclassifications (\ie  the SVM will try to classify all data points correctly), whereas a smaller value of $C$ allows for a wider margin but potentially more misclassifications. The optimal value of $C$ is typically chosen through cross-validation.

% \section{Kernel Trick}
% \label{sec:kernel_trick}
% Applying the kernel trick simply means replacing the dot product of two examples in a dual form by a kernel function. 

% \begin{align}
% 	 \max_\alpha \sum_i \alpha_i -\frac{1}{2}\sum_i\sum_j \alpha_i\alpha_j y_iy_j \psi(\mathbf{x}_i)\cdot \psi(\mathbf{x}_j)
% 	 \label{eq:kernel_dual_form}
% \end{align}
% Equivalently, 
% \begin{align}
% 	 \max_\alpha \sum_i \alpha_i -\frac{1}{2}\sum_i\sum_j \alpha_i\alpha_j y_iy_j K(\mathbf{x}_i,\mathbf{x}_j).
% \end{align}












