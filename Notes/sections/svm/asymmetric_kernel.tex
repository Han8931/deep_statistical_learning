\section{LS-SVM with Asymmetric Kernels}
\label{sec:asymmetric_kernels}

Recall that the dual form of LS-SVM is given by
\begin{align*}
	\begin{bmatrix}
	0 & y^T \\
	y & \Omega + \frac{1}{\gamma} I
	\end{bmatrix}
	\begin{bmatrix}
	b \\
	\alpha
	\end{bmatrix}
	=
	\begin{bmatrix}
	0 \\
	e \end{bmatrix}
\end{align*}
An interesting point here is that using an asymmetric kernel in LS-SVM will not reduce to its symmetrization and asymmetric information can be learned. Then we can develop asymmetric kernels in the LS-SVM framework in a straightforward way.

Asymmetric kernels are particularly useful in capturing directional relationships in data that symmetric kernels cannot. For instance, in scenarios involving directed graphs or conditional probabilities, the relationship from $x$ to $y$ is inherently different from the relationship from $y$ to $x$.

\subsection{AsK-LS Primal Problem Formulation}
We first define a generalized kernel trick for an inner product of two mappings $\phi_s$ and $\phi_t$.
\begin{align*}
	K(\rvu, \rvv) = \langle \phi_s(\rvu), \phi_t(\rvv)\rangle, \forall \rvu \in \mathbb{R}^{d_s}, \rvv \in \mathbb{R}^{d_t},
\end{align*}
where $\phi_s: \mathbb{R}^{d_s}\to \mathbb{R}^{p}$, $\phi_t: \mathbb{R}^{d_t}\to \mathbb{R}^{p}$, and $\mathbb{R}^p$ is a high-dimensional or even an infinite-dimensional space. Note that $d_s$ and $d_t$ can be different. 

The primal problem for AsK-LS is formulated to handle these asymmetric relationships. The goal is to find the weight vectors \( \omega \) and \( \nu \), and bias terms \( b_1 \) and \( b_2 \), that minimize the following objective function:
\begin{align*}
	\min_{\omega, \nu, b_1, b_2, e, h} \frac{1}{2} \omega^T \nu + \frac{\gamma}{2} \sum_{i=1}^m e_i^2 + \frac{\gamma}{2} \sum_{i=1}^m h_i^2, 
\end{align*}
subject to the constraints:
\begin{align*}
	& y_i (\omega^T \phi_s(x_i) + b_1) = 1 - e_i\\
	& y_i (\nu^T \phi_t(x_i) + b_2) = 1 - h_i
\end{align*}
Here:
\begin{itemize}
	\item \( \omega \) and \( \nu \) are weight vectors for the source and target features.
	\item \( \phi_s(x) \) and \( \phi_t(x) \) are the source and target feature mappings.
	\item \( e_i \) and \( h_i \) are error terms for the source and target constraints.
	\item \( \gamma \) is a regularization parameter.
\end{itemize}
Note that this formulation is almost the same as the LS-SVM except that this considers both the source and target feature spaces simultaneously.

\subsection{Dual Form}
Let's transform it into a \textit{dual} form. The dual problem involves solving a system of linear equations derived from the primal problem's Lagrangian. The Lagrangian function for the primal problem is:
\begin{align*}
	&\mathcal{L}( \omega, \nu, b_1, b_2, e, h, \alpha, \beta) = \\
	&\frac{1}{2} \omega^T \nu + \frac{\gamma}{2} \sum_{i=1}^m e_i^2 + \frac{\gamma}{2} \sum_{i=1}^m h_i^2+ \\& \sum_{i=1}^m \alpha_i (1 - e_i - y_i (\omega^T \phi_s(x_i) + b_1)) + \sum_{i=1}^m \beta_i (1 - h_i - y_i (\nu^T \phi_t(x_i) + b_2))
\end{align*}
The KKT conditions are derived by setting the partial derivatives of the Lagrangian with respect to \( \omega, \nu, b_1, b_2, e, \) and \( h \) to zero. The dual problem leads to the following linear system:

\[
\begin{bmatrix}
0 & 0 & Y^T & 0 \\
0 & 0 & 0 & Y^T \\
Y & 0 & \frac{I}{\gamma} & H \\
0 & Y & H^T & \frac{I}{\gamma}
\end{bmatrix}
\begin{bmatrix}
b_1 \\
b_2 \\
\alpha \\
\beta
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
1 \\
1
\end{bmatrix}
\]
where:
\begin{itemize}
	\item \( Y \) is a vector of class labels.
	\item \( H \) is the kernel matrix with elements \( H_{ij} = y_i K(x_i, x_j) y_j \), where \( K(x_i, x_j) = \langle \phi_s(x_i), \phi_t(x_j) \rangle \) is the asymmetric kernel function.
		\begin{itemize}
			\item For an asymmetric kernel \( K \), the kernel function \( K(x_i, x_j) \neq K(x_j, x_i) \). This asymmetry is directly incorporated into the matrix \( H \), where:
				\begin{align*}
					H_{ij} &= y_i K(x_i, x_j) y_j \\
					H_{ji} &= y_j K(x_j, x_i) y_i
				\end{align*}
		\end{itemize}
\end{itemize}

% ### 5. Feature Interpretation and Asymmetric Information
AsK-LS uses two different feature mappings \( \phi_s \) and \( \phi_t \) for the source and target features. This approach allows capturing more information compared to symmetric kernels. The dual solution provides weight vectors \( \omega \) and \( \nu \), which span the target and source feature spaces, respectively.

The decision functions for classification from the source and target perspectives are given by
\begin{align*}
	f_t(x) &= \sum_{i=1}^m \alpha_i y_i K(x_i, x) + b_1\\
	f_s(x) &= \sum_{i=1}^m \beta_i y_i K(x, x_i) + b_2
\end{align*}
These functions utilize the learned asymmetric relationships in the data.


\subsection{Asymmetric Kernels}
We can consider asymmetric kernels like these:
\begin{align*}
	K_{SNE}(\rvx, \rvy) &= \frac{\exp(-\|\rvx-\rvy\|_2^2)/\sigma^2}{\sum_{\rvz\in\mathbf{X}_{train}}\exp(-\|\rvx-\rvz\|_2^2)/\sigma^2}\\
	K_{T}(\rvx, \rvy) &= \frac{(1+\|\rvx-\rvy\|_2^2)^{-1}}{\sum_{\rvz\in\mathbf{X}_{train}}(1+\|\rvx-\rvz\|_2^2)^{-1}}
\end{align*}


